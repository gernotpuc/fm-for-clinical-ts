{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "from skopt import BayesSearchCV, space\n",
    "from sklearn.metrics import average_precision_score\n",
    "# use feature importance for feature selection\n",
    "from numpy import sort\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "### Parameters\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from autogluon.core.metrics import make_scorer\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from autogluon.common import space\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from sklearn.model_selection import KFold\n",
    "#import ts_fc_functions as ff\n",
    "import import_ipynb\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel, XGBModel, TiDEModel#, LightGBMModel\n",
    "from darts.models import NHiTSModel,LightGBMModel,RNNModel\n",
    "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
    "from darts.metrics import mape, r2_score, mae, rmse\n",
    "import torch\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_training_scale(data,value_to_predict,resample_rate,min_ts_length,imputation,max_train_length,prediction_length,padding,scale_series):\n",
    "\n",
    "    resampled_df = resample_ts(data,resample_rate,min_ts_length,imputation,max_train_length,value_to_predict)\n",
    "\n",
    "    resampled_df, scaler = scale_ts(resampled_df,value_to_predict)\n",
    "    valid_series_dict_full, test_series_dict_full, train_series_dict_full = ts_dictionary(resampled_df,prediction_length,value_to_predict,resample_rate)\n",
    "\n",
    "        # Pad the series\n",
    "    if padding == True:\n",
    "        train_series_dict_full = pad_series_dict(train_series_dict_full, max_train_length, pad_value=0)\n",
    "        valid_series_dict_full = pad_series_dict(valid_series_dict_full, max_train_length, pad_value=0)   \n",
    "\n",
    "    return train_series_dict_full,valid_series_dict_full,resampled_df, scaler\n",
    "\n",
    "def data_processing_training(data,value_to_predict,resample_rate,min_ts_length,imputation,max_train_length,prediction_length,padding):\n",
    "\n",
    "    resampled_df = resample_ts(data,resample_rate,min_ts_length,imputation,max_train_length,value_to_predict)\n",
    "    valid_series_dict_full, test_series_dict_full, train_series_dict_full = ts_dictionary(resampled_df,prediction_length,value_to_predict,resample_rate)\n",
    "    if padding == True:\n",
    "        train_series_dict_full = pad_series_dict(train_series_dict_full, max_train_length, pad_value=0)\n",
    "        valid_series_dict_full = pad_series_dict(valid_series_dict_full, max_train_length, pad_value=0)   \n",
    "\n",
    "    return train_series_dict_full,valid_series_dict_full,resampled_df\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_use_case_3_data(data_use_case_3, start_date, end_date):\n",
    "    df_sorted = data_use_case_3.sort_values(by='performedDateTime').reset_index(drop=True)\n",
    "    df_no_duplicates = df_sorted.drop_duplicates(subset=['performedDateTime', 'subject_reference', 'encounter_reference']).reset_index(drop=True)\n",
    "\n",
    "    # Generate a date range with unique dates\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "    # Duplicate each date to have two entries per day\n",
    "    date_range = date_range.repeat(2)\n",
    "    df_no_duplicates[\"date\"] = pd.to_datetime(df_no_duplicates[\"date\"])\n",
    "\n",
    "\n",
    "    ctscans = pd.DataFrame(date_range, columns=[\"date\"])\n",
    "    ctscans[\"time_of_day\"] = ctscans.groupby(\"date\").cumcount().map({0: \"a_morning\", 1: \"b_evening\"})\n",
    "\n",
    "    count_df = df_no_duplicates.groupby([\"date\", \"time_of_day\"]).size().reset_index(name=\"count_scans\")\n",
    "    ctscans = pd.merge(ctscans, count_df, on=[\"date\", \"time_of_day\"], how=\"left\")\n",
    "    ctscans[\"count_scans\"] = ctscans[\"count_scans\"].fillna(0).astype(int)\n",
    "    ctscans = ctscans.drop(index=0)\n",
    "\n",
    "    # Reset the index if desired, to avoid a missing index number\n",
    "    ctscans = ctscans.reset_index(drop=True)\n",
    "\n",
    "    # Create a new column 'shift_count' initialized to NaN\n",
    "    ctscans[\"shift_count\"] = np.nan\n",
    "\n",
    "    # Iterate over the rows and sum count_scans for each consecutive \"b_evening\" and \"a_morning\" pair\n",
    "    for i in range(len(ctscans) - 1):\n",
    "        if ctscans.loc[i, \"time_of_day\"] == \"b_evening\" and ctscans.loc[i + 1, \"time_of_day\"] == \"a_morning\":\n",
    "            # Sum count_scans for the consecutive \"b_evening\" and \"a_morning\"\n",
    "            shift_sum = ctscans.loc[i, \"count_scans\"] + ctscans.loc[i + 1, \"count_scans\"]\n",
    "            \n",
    "            # Assign the result to the first row of the pair (\"b_evening\" row)\n",
    "            ctscans.loc[i, \"shift_count\"] = shift_sum\n",
    "\n",
    "    # Optionally, forward-fill or set NaN values to 0 in 'shift_count'\n",
    "    ctscans[\"shift_count\"] = ctscans[\"shift_count\"].fillna(0).astype(int)\n",
    "    ctscans = ctscans[ctscans[\"time_of_day\"] != \"a_morning\"].reset_index(drop=True)\n",
    "    ctscans[\"day_of_week\"] = ctscans[\"date\"].dt.day_name()\n",
    "    last_friday = ctscans[ctscans[\"day_of_week\"] == \"Friday\"][\"date\"].max()\n",
    "\n",
    "    # Step 2: Initialize ts_id counter and end_date for the first 4-week period\n",
    "    current_id = 1\n",
    "    end_date = last_friday\n",
    "\n",
    "    # Step 3: Create an empty 'ts_id' column\n",
    "    ctscans[\"ts_id\"] = 0\n",
    "\n",
    "    # Step 4: Assign ts_id to each 4-week (28-day) period, going backwards from the last Friday\n",
    "    while end_date >= ctscans[\"date\"].min():\n",
    "        start_date = end_date - pd.Timedelta(weeks=4)\n",
    "\n",
    "        # Assign the current ts_id to dates within the 4-week range\n",
    "        ctscans.loc[(ctscans[\"date\"] > start_date) & (ctscans[\"date\"] <= end_date), \"ts_id\"] = current_id\n",
    "\n",
    "        # Move to the next 4-week period back in time\n",
    "        end_date = start_date\n",
    "        current_id += 1\n",
    "    ctscans\n",
    "    return ctscans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def bootstrap_metrics_roc(actual_values, predicted_values, group_ids, n_bootstrap=1000, ci_percentile=95, threshold=8):\n",
    "    \"\"\"\n",
    "    Calculate confusion matrix components, classification metrics, regression metrics, MCC, AUC, and ROC curve with confidence intervals.\n",
    "    \"\"\"\n",
    "    # Group values by group_ids\n",
    "    data = pd.DataFrame({\n",
    "        \"actual\": actual_values,\n",
    "        \"predicted\": predicted_values,\n",
    "        \"group_id\": group_ids\n",
    "    })\n",
    "    \n",
    "    # Aggregate per group (binary classification)\n",
    "    grouped = data.groupby(\"group_id\").agg(\n",
    "        actual_fever=(\"actual\", lambda x: (x <= threshold).any()),\n",
    "        predicted_fever=(\"predicted\", lambda x: (x <= threshold).any())\n",
    "    )\n",
    "       # Calculate exact confusion matrix components\n",
    "    tp_exact = ((grouped[\"actual_fever\"] == True) & (grouped[\"predicted_fever\"] == True)).sum()\n",
    "    fn_exact = ((grouped[\"actual_fever\"] == True) & (grouped[\"predicted_fever\"] == False)).sum()\n",
    "    fp_exact = ((grouped[\"actual_fever\"] == False) & (grouped[\"predicted_fever\"] == True)).sum()\n",
    "    tn_exact = ((grouped[\"actual_fever\"] == False) & (grouped[\"predicted_fever\"] == False)).sum()\n",
    "\n",
    "    # Calculate exact classification metrics\n",
    "    sensitivity_exact = tp_exact / (tp_exact + fn_exact) if (tp_exact + fn_exact) > 0 else 0\n",
    "    specificity_exact = tn_exact / (tn_exact + fp_exact) if (tn_exact + fp_exact) > 0 else 0\n",
    "    ppv_exact = tp_exact / (tp_exact + fp_exact) if (tp_exact + fp_exact) > 0 else 0\n",
    "    npv_exact = tn_exact / (tn_exact + fn_exact) if (tn_exact + fn_exact) > 0 else 0\n",
    "    accuracy_exact = (tp_exact + tn_exact) / (tp_exact + fn_exact + fp_exact + tn_exact) if (tp_exact + fn_exact + fp_exact + tn_exact) > 0 else 0\n",
    "    f1_exact = (2 * tp_exact) / (2 * tp_exact + fp_exact + fn_exact) if (2 * tp_exact + fp_exact + fn_exact) > 0 else 0\n",
    "    mcc_exact = ((tp_exact * tn_exact) - (fp_exact * fn_exact)) / (\n",
    "        np.sqrt((tp_exact + fp_exact) * (tp_exact + fn_exact) * (tn_exact + fp_exact) * (tn_exact + fn_exact))\n",
    "        if (tp_exact + fp_exact) > 0 and (tp_exact + fn_exact) > 0 and (tn_exact + fp_exact) > 0 and (tn_exact + fn_exact) > 0\n",
    "        else 1\n",
    "    )\n",
    "\n",
    "    # Calculate exact regression metrics\n",
    "    mae_exact = np.mean(np.abs(actual_values - predicted_values))\n",
    "    mse_exact = np.mean((actual_values - predicted_values) ** 2)\n",
    "    rmse_exact = np.sqrt(mse_exact)\n",
    "    mape_exact = np.mean(np.abs((actual_values - predicted_values) / actual_values)) * 100\n",
    "    smape_exact = np.mean(np.abs(actual_values - predicted_values) / (np.abs(actual_values) + np.abs(predicted_values))) * 100\n",
    "\n",
    "    # Calculate AUC and ROC curve for exact values\n",
    "    auc_exact = roc_auc_score(grouped[\"actual_fever\"], grouped[\"predicted_fever\"])\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(grouped[\"actual_fever\"], grouped[\"predicted_fever\"])\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_exact:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    roc_curve_path = \"roc_curve.png\"  # Save path for ROC curve\n",
    "    plt.savefig(roc_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Initialize arrays for bootstrapping\n",
    "    auc_bootstrap = []\n",
    "    tp_bootstrap, fn_bootstrap, fp_bootstrap, tn_bootstrap = [], [], [], []\n",
    "    sensitivity_bootstrap, specificity_bootstrap, ppv_bootstrap = [], [], []\n",
    "    npv_bootstrap, accuracy_bootstrap, f1_bootstrap, mcc_bootstrap = [], [], [], []\n",
    "    mae_bootstrap, mse_bootstrap, rmse_bootstrap, mape_bootstrap, smape_bootstrap = [], [], [], [], []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample groups with replacement\n",
    "        resampled_groups = grouped.sample(n=len(grouped), replace=True)\n",
    "        resampled_indices = data[data[\"group_id\"].isin(resampled_groups.index)].index\n",
    "        actual_resample = actual_values[resampled_indices]\n",
    "        predicted_resample = predicted_values[resampled_indices]\n",
    "\n",
    "        # Calculate confusion matrix for bootstrap sample\n",
    "        tp = ((resampled_groups[\"actual_fever\"] == True) & (resampled_groups[\"predicted_fever\"] == True)).sum()\n",
    "        fn = ((resampled_groups[\"actual_fever\"] == True) & (resampled_groups[\"predicted_fever\"] == False)).sum()\n",
    "        fp = ((resampled_groups[\"actual_fever\"] == False) & (resampled_groups[\"predicted_fever\"] == True)).sum()\n",
    "        tn = ((resampled_groups[\"actual_fever\"] == False) & (resampled_groups[\"predicted_fever\"] == False)).sum()\n",
    "        \n",
    "        tp_bootstrap.append(tp)\n",
    "        fn_bootstrap.append(fn)\n",
    "        fp_bootstrap.append(fp)\n",
    "        tn_bootstrap.append(tn)\n",
    "\n",
    "        # Classification metrics\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "        f1 = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "        mcc = ((tp * tn) - (fp * fn)) / (\n",
    "            np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "            if (tp + fp) > 0 and (tp + fn) > 0 and (tn + fp) > 0 and (tn + fn) > 0\n",
    "            else 1\n",
    "        )\n",
    "\n",
    "        sensitivity_bootstrap.append(sensitivity)\n",
    "        specificity_bootstrap.append(specificity)\n",
    "        ppv_bootstrap.append(ppv)\n",
    "        npv_bootstrap.append(npv)\n",
    "        accuracy_bootstrap.append(accuracy)\n",
    "        f1_bootstrap.append(f1)\n",
    "        mcc_bootstrap.append(mcc)\n",
    "\n",
    "        # AUC for bootstrap sample\n",
    "        try:\n",
    "            auc = roc_auc_score(resampled_groups[\"actual_fever\"], resampled_groups[\"predicted_fever\"])\n",
    "        except ValueError:\n",
    "            auc = np.nan  # Handle cases where AUC cannot be computed (only one class present)\n",
    "        auc_bootstrap.append(auc)\n",
    "\n",
    "        # Regression metrics\n",
    "        mae_bootstrap.append(np.mean(np.abs(actual_resample - predicted_resample)))\n",
    "        mse_bootstrap.append(np.mean((actual_resample - predicted_resample) ** 2))\n",
    "        rmse_bootstrap.append(np.sqrt(mse_bootstrap[-1]))\n",
    "        mape_bootstrap.append(np.mean(np.abs((actual_resample - predicted_resample) / actual_resample)) * 100)\n",
    "        smape_bootstrap.append(np.mean(np.abs(actual_resample - predicted_resample) / (np.abs(actual_resample) + np.abs(predicted_resample))) * 100)\n",
    "\n",
    "    # Prepare classification metrics\n",
    "    classification_metrics = pd.DataFrame({\n",
    "        \"Metric\": [\"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\", \"Accuracy\", \"F1\", \"MCC\", \"AUC\", \"TP\", \"FN\", \"FP\", \"TN\"],\n",
    "        \"Point Estimate\": [\n",
    "            sensitivity_exact, specificity_exact, ppv_exact, npv_exact, accuracy_exact, f1_exact, mcc_exact, auc_exact,\n",
    "            tp_exact, fn_exact, fp_exact, tn_exact\n",
    "        ],\n",
    "        \"CI Lower\": [\n",
    "            np.percentile(sensitivity_bootstrap, 2.5), np.percentile(specificity_bootstrap, 2.5),\n",
    "            np.percentile(ppv_bootstrap, 2.5), np.percentile(npv_bootstrap, 2.5),\n",
    "            np.percentile(accuracy_bootstrap, 2.5), np.percentile(f1_bootstrap, 2.5), np.percentile(mcc_bootstrap, 2.5),\n",
    "            np.percentile(auc_bootstrap, 2.5), np.percentile(tp_bootstrap, 2.5), np.percentile(fn_bootstrap, 2.5),\n",
    "            np.percentile(fp_bootstrap, 2.5), np.percentile(tn_bootstrap, 2.5)\n",
    "        ],\n",
    "        \"CI Upper\": [\n",
    "            np.percentile(sensitivity_bootstrap, 97.5), np.percentile(specificity_bootstrap, 97.5),\n",
    "            np.percentile(ppv_bootstrap, 97.5), np.percentile(npv_bootstrap, 97.5),\n",
    "            np.percentile(accuracy_bootstrap, 97.5), np.percentile(f1_bootstrap, 97.5), np.percentile(mcc_bootstrap, 97.5),\n",
    "            np.percentile(auc_bootstrap, 97.5), np.percentile(tp_bootstrap, 97.5), np.percentile(fn_bootstrap, 97.5),\n",
    "            np.percentile(fp_bootstrap, 97.5), np.percentile(tn_bootstrap, 97.5)\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Prepare regression metrics\n",
    "    regression_metrics = pd.DataFrame({\n",
    "        \"Metric\": [\"MAE\", \"MSE\", \"RMSE\", \"MAPE\", \"SMAPE\"],\n",
    "        \"Point Estimate\": [mae_exact, mse_exact, rmse_exact, mape_exact, smape_exact],\n",
    "        \"CI Lower\": [\n",
    "            np.percentile(mae_bootstrap, 2.5), np.percentile(mse_bootstrap, 2.5),\n",
    "            np.percentile(rmse_bootstrap, 2.5), np.percentile(mape_bootstrap, 2.5),\n",
    "            np.percentile(smape_bootstrap, 2.5)\n",
    "        ],\n",
    "        \"CI Upper\": [\n",
    "            np.percentile(mae_bootstrap, 97.5), np.percentile(mse_bootstrap, 97.5),\n",
    "            np.percentile(rmse_bootstrap, 97.5), np.percentile(mape_bootstrap, 97.5),\n",
    "            np.percentile(smape_bootstrap, 97.5)\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    return classification_metrics, regression_metrics,roc_curve_path\n",
    "\n",
    "\n",
    "def predict_proba_ci_roc(output_dir_classification, classification_filename, output_dir_forecasting, forecasting_filename, model, train_series_dict, valid_series_dict, prediction_length, scaler, n_bootstrap=1000):\n",
    "    # Generate predictions\n",
    "    pred = model.predict(n=prediction_length, series=list(train_series_dict.values()), num_samples=500)\n",
    "\n",
    "    # Extract actual and predicted values\n",
    "    last_values = {\n",
    "        key: ts.values()[-prediction_length:].flatten()\n",
    "        for key, ts in valid_series_dict.items()\n",
    "    }\n",
    "    pred_arrays = np.array([ts.all_values() for ts in pred]).squeeze(axis=2)\n",
    "    pred_medians = np.median(pred_arrays, axis=2)\n",
    "\n",
    "    actual_values = []\n",
    "    predicted_values = []\n",
    "    keys = []\n",
    "\n",
    "    for (key, last_value), pred_value in zip(last_values.items(), pred_medians):\n",
    "        actual_values.append(last_value)\n",
    "        predicted_values.append(pred_value)\n",
    "        keys.append(key)\n",
    "\n",
    "    # Rescale values\n",
    "    actual_values_np = scaler.inverse_transform(np.array(actual_values))\n",
    "    predicted_values_np = scaler.inverse_transform(np.array(predicted_values))\n",
    "\n",
    "    # Generate group IDs\n",
    "    group_ids = np.repeat(range(len(keys)), prediction_length)\n",
    "\n",
    "    # Calculate metrics\n",
    "    classification_metrics, regression_metrics, roc_curve_path = bootstrap_metrics_roc(\n",
    "        actual_values_np.flatten(), predicted_values_np.flatten(), group_ids, n_bootstrap=n_bootstrap\n",
    "    )\n",
    "\n",
    "    # Save classification results\n",
    "    os.makedirs(output_dir_classification, exist_ok=True)\n",
    "    classification_metrics.to_csv(os.path.join(output_dir_classification, classification_filename), index=False)\n",
    "\n",
    "    # Save regression results\n",
    "    os.makedirs(output_dir_forecasting, exist_ok=True)\n",
    "    regression_metrics.to_csv(os.path.join(output_dir_forecasting, forecasting_filename), index=False)\n",
    "\n",
    "    print(f\"ROC curve saved to {roc_curve_path}\")\n",
    "    \n",
    "    return predicted_values, actual_values, keys, pred, actual_values_np, predicted_values_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timesfm_data_ci_roc_(model_name, output_dir_forecasting, output_dir_classification, forecasting_filename, classification_filename, resampled_df,tfm, scaler, prediction_length, value_to_predict, n_bootstrap=1000):\n",
    "    \n",
    "    full_data_test = resampled_df.groupby(\"encounter_id\").tail(prediction_length)\n",
    "    full_data_train = resampled_df.drop(full_data_test.index)\n",
    "    full_data_train_ = full_data_train.rename(columns={'recorded_time': 'ds', 'encounter_id': 'unique_id','value':'y'})\n",
    "    full_data_test_ = full_data_test.rename(columns={'recorded_time': 'ds', 'encounter_id': 'unique_id','value':'y'})\n",
    "    #full_data_train_ = full_data_train_.drop(columns=['subject_reference','code_coding_0_display'])\n",
    "    #full_data_test_ = full_data_test_.drop(columns=['subject_reference','code_coding_0_display'])\n",
    "    forecasts = tfm.forecast_on_df(\n",
    "    inputs=full_data_train_,\n",
    "    freq=\"D\",  # monthly\n",
    "    value_name=\"y\",\n",
    "    num_jobs=3,\n",
    ")\n",
    "    def rescale_to_original(scaled_df, scaler, columns_to_rescale):\n",
    "        scaled_df[columns_to_rescale] = scaler.inverse_transform(scaled_df[columns_to_rescale])\n",
    "        return scaled_df\n",
    "    columns_to_rescale_actual = [\"y\"]\n",
    "    columns_to_rescale_predict = [\"timesfm\"]\n",
    "    actuals = rescale_to_original(full_data_test_, scaler,columns_to_rescale_actual)\n",
    "    predictions = rescale_to_original(forecasts, scaler,columns_to_rescale_predict)\n",
    "\n",
    "    def bootstrap_timellm_metrics_length(forecasts_df, actual_values_df, prediction_length, n_bootstrap, ci_percentile=95):\n",
    "        mae_bootstrap = []\n",
    "        mse_bootstrap = []\n",
    "        rmse_bootstrap = []\n",
    "        smape_bootstrap = []\n",
    "        mape_bootstrap = []\n",
    "\n",
    "        # Merge forecasts and actual values on 'unique_id' and 'ds'\n",
    "        merged_df = pd.merge(forecasts_df.reset_index(), actual_values_df, on=['unique_id', 'ds'])\n",
    "\n",
    "        unique_ids = merged_df['unique_id'].unique()\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            # Sample with replacement\n",
    "            sampled_ids = np.random.choice(unique_ids, size=len(unique_ids), replace=True)\n",
    "            sampled_data = merged_df[merged_df['unique_id'].isin(sampled_ids)]\n",
    "            \n",
    "            if len(sampled_ids) == 0:  # Skip this iteration if no valid samples\n",
    "                continue\n",
    "\n",
    "            total_mae = 0\n",
    "            total_mse = 0\n",
    "            total_smape = 0\n",
    "            total_mape = 0\n",
    "\n",
    "            for unique_id in sampled_ids:\n",
    "                # Select the forecast and true values for this unique_id\n",
    "                data = sampled_data[sampled_data['unique_id'] == unique_id].sort_values(by='ds')\n",
    "                \n",
    "                if data.empty:\n",
    "                    continue  # Skip if no data for this ID\n",
    "\n",
    "                forecast_values = data['timesfm'].values[-prediction_length:]\n",
    "                true_values = data['y'].values[-prediction_length:]\n",
    "\n",
    "                if len(forecast_values) == 0 or len(true_values) == 0:\n",
    "                    continue  # Avoid processing empty arrays\n",
    "\n",
    "                # Compute errors\n",
    "                absolute_errors = np.abs(forecast_values - true_values)\n",
    "                \n",
    "                # MAE\n",
    "                mae = absolute_errors.mean()\n",
    "                total_mae += mae\n",
    "\n",
    "                # MSE\n",
    "                mse = (absolute_errors ** 2).mean()\n",
    "                total_mse += mse\n",
    "\n",
    "                # SMAPE\n",
    "                denominator = (np.abs(true_values) + np.abs(forecast_values)) / 2\n",
    "                smape = (absolute_errors / denominator).mean() * 100\n",
    "                total_smape += smape\n",
    "\n",
    "                # MAPE\n",
    "                mape = (np.abs((true_values - forecast_values) / true_values).mean()) * 100\n",
    "                total_mape += mape\n",
    "\n",
    "            if len(sampled_ids) == 0:\n",
    "                continue  # Ensure we don't divide by zero\n",
    "\n",
    "            # Compute overall metrics for this bootstrap sample\n",
    "            mae_bootstrap.append(total_mae / len(sampled_ids))\n",
    "            mse_bootstrap.append(total_mse / len(sampled_ids))\n",
    "            rmse_bootstrap.append(np.sqrt(total_mse / len(sampled_ids)))\n",
    "            smape_bootstrap.append(total_smape / len(sampled_ids))\n",
    "            mape_bootstrap.append(total_mape / len(sampled_ids))\n",
    "\n",
    "\n",
    "        # Calculate mean and confidence intervals\n",
    "        metrics = {\n",
    "            \"mae\": (\n",
    "                np.mean(mae_bootstrap),\n",
    "                np.percentile(mae_bootstrap, (100 - ci_percentile) / 2),\n",
    "                np.percentile(mae_bootstrap, 100 - (100 - ci_percentile) / 2),\n",
    "            ),\n",
    "            \"mse\": (\n",
    "                np.mean(mse_bootstrap),\n",
    "                np.percentile(mse_bootstrap, (100 - ci_percentile) / 2),\n",
    "                np.percentile(mse_bootstrap, 100 - (100 - ci_percentile) / 2),\n",
    "            ),\n",
    "            \"rmse\": (\n",
    "                np.mean(rmse_bootstrap),\n",
    "                np.percentile(rmse_bootstrap, (100 - ci_percentile) / 2),\n",
    "                np.percentile(rmse_bootstrap, 100 - (100 - ci_percentile) / 2),\n",
    "            ),\n",
    "            \"smape\": (\n",
    "                np.mean(smape_bootstrap),\n",
    "                np.percentile(smape_bootstrap, (100 - ci_percentile) / 2),\n",
    "                np.percentile(smape_bootstrap, 100 - (100 - ci_percentile) / 2),\n",
    "            ),\n",
    "            \"mape\": (\n",
    "                np.mean(mape_bootstrap),\n",
    "                np.percentile(mape_bootstrap, (100 - ci_percentile) / 2),\n",
    "                np.percentile(mape_bootstrap, 100 - (100 - ci_percentile) / 2),\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        return metrics, merged_df\n",
    "\n",
    "    # Example usage\n",
    "    metrics, merged_data = bootstrap_timellm_metrics_length(\n",
    "        forecasts_df=predictions,\n",
    "        actual_values_df=actuals,\n",
    "        prediction_length=prediction_length,  # Use the desired prediction length\n",
    "        n_bootstrap=1000,     # Number of bootstrap samples\n",
    "        ci_percentile=95      # Confidence interval\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(merged_data.info())\n",
    "\n",
    "\n",
    "    # Create DataFrame for forecasting metrics\n",
    "    forecasting_metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "    # Save forecasting metrics\n",
    "    os.makedirs(output_dir_forecasting, exist_ok=True)\n",
    "    forecasting_csv_path = os.path.join(output_dir_forecasting, forecasting_filename)\n",
    "    forecasting_metrics_df.to_csv(forecasting_csv_path, index=False)\n",
    "    print(f\"Forecasting metrics saved to {forecasting_csv_path}\")\n",
    "    merged_data = merged_data.rename(columns={'y': 'actual', 'timesfm': 'predictions'})\n",
    "\n",
    "    #merged_data['actual'] = actuals.flatten()\n",
    "    #merged_data['predictions'] = predictions.flatten()\n",
    "    fever_flags_actual = merged_data.groupby(level=0)['actual'].transform(lambda x: (x < 8).any())\n",
    "    fever_flags_predicted = merged_data.groupby(level=0)['predictions'].transform(lambda x: (x < 8).any())\n",
    "    merged_data['actual_fever'] = fever_flags_actual\n",
    "    merged_data['predictions_fever'] = fever_flags_predicted\n",
    "    merged_data = merged_data.reset_index(level=0)\n",
    "    unique_encounters = merged_data.drop_duplicates(subset=['unique_id'])\n",
    "    print(unique_encounters.info())\n",
    "    # Confusion matrix computation\n",
    "    confusion_conditions = [\n",
    "        (unique_encounters['actual_fever'] == True) & (unique_encounters['predictions_fever'] == True),  # TP\n",
    "        (unique_encounters['actual_fever'] == True) & (unique_encounters['predictions_fever'] == False), # FN\n",
    "        (unique_encounters['actual_fever'] == False) & (unique_encounters['predictions_fever'] == True), # FP\n",
    "        (unique_encounters['actual_fever'] == False) & (unique_encounters['predictions_fever'] == False) # TN\n",
    "    ]\n",
    "    confusion_choices = ['TP', 'FN', 'FP', 'TN']\n",
    "    unique_encounters['confusion'] = np.select(confusion_conditions, confusion_choices)\n",
    "\n",
    "    confusion_matrix_bootstrap = {'TP': [], 'FN': [], 'FP': [], 'TN': []}\n",
    "    metrics_bootstrap = {'sensitivity': [], 'specificity': [], 'ppv': [], 'npv': [], 'accuracy': [], 'f1': [], 'mcc': []}\n",
    "    auc_scores = []  # For storing AUC scores\n",
    "\n",
    "    # Bootstrapping for confusion matrix, classification metrics, and AUC\n",
    "    for _ in range(n_bootstrap):\n",
    "        bootstrap_sample = unique_encounters.sample(frac=1, replace=True)\n",
    "        tp = ((bootstrap_sample['actual_fever'] == True) & (bootstrap_sample['predictions_fever'] == True)).sum()\n",
    "        fn = ((bootstrap_sample['actual_fever'] == True) & (bootstrap_sample['predictions_fever'] == False)).sum()\n",
    "        fp = ((bootstrap_sample['actual_fever'] == False) & (bootstrap_sample['predictions_fever'] == True)).sum()\n",
    "        tn = ((bootstrap_sample['actual_fever'] == False) & (bootstrap_sample['predictions_fever'] == False)).sum()\n",
    "\n",
    "        confusion_matrix_bootstrap['TP'].append(tp)\n",
    "        confusion_matrix_bootstrap['FN'].append(fn)\n",
    "        confusion_matrix_bootstrap['FP'].append(fp)\n",
    "        confusion_matrix_bootstrap['TN'].append(tn)\n",
    "\n",
    "        metrics_bootstrap['sensitivity'].append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['specificity'].append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        metrics_bootstrap['ppv'].append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "        metrics_bootstrap['npv'].append(tn / (tn + fn) if (tn + fn) > 0 else 0)\n",
    "        metrics_bootstrap['accuracy'].append((tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['f1'].append((2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['mcc'].append(((tp * tn) - (fp * fn)) / (\n",
    "            ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "        ) if ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) > 0 else 0)\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(bootstrap_sample['actual_fever'], bootstrap_sample['predictions'])\n",
    "        except ValueError:\n",
    "            auc = np.nan  # Handle case where AUC cannot be computed (only one class present)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    # Confidence intervals for confusion matrix\n",
    "    confusion_matrix_ci = {}\n",
    "    for key, values in confusion_matrix_bootstrap.items():\n",
    "        lower = np.percentile(values, 2.5)\n",
    "        upper = np.percentile(values, 97.5)\n",
    "        mean_value = np.mean(values)\n",
    "        confusion_matrix_ci[key] = {\"Mean\": mean_value, \"CI Lower\": lower, \"CI Upper\": upper}\n",
    "\n",
    "    # Confidence intervals for classification metrics\n",
    "    metrics_ci = {}\n",
    "    for metric, values in metrics_bootstrap.items():\n",
    "        lower = np.percentile(values, 2.5)\n",
    "        upper = np.percentile(values, 97.5)\n",
    "        mean_value = np.mean(values)\n",
    "        metrics_ci[metric] = {\"Mean\": mean_value, \"CI Lower\": lower, \"CI Upper\": upper}\n",
    "\n",
    "    # Confidence intervals for AUC\n",
    "    auc_mean = np.nanmean(auc_scores)\n",
    "    auc_lower = np.nanpercentile(auc_scores, 2.5)\n",
    "    auc_upper = np.nanpercentile(auc_scores, 97.5)\n",
    "\n",
    "    # Prepare data for classification metrics CSV\n",
    "    classification_metrics_data = []\n",
    "    for metric, stats in metrics_ci.items():\n",
    "        classification_metrics_data.append({\n",
    "            \"Metric\": metric.capitalize(),\n",
    "            \"Point Estimate\": stats[\"Mean\"],\n",
    "            \"95% CI Lower\": stats[\"CI Lower\"],\n",
    "            \"95% CI Upper\": stats[\"CI Upper\"]\n",
    "        })\n",
    "\n",
    "    # Add confusion matrix stats\n",
    "    for key, stats in confusion_matrix_ci.items():\n",
    "        classification_metrics_data.append({\n",
    "            \"Metric\": f\"Avg {key}\",\n",
    "            \"Point Estimate\": stats[\"Mean\"],\n",
    "            \"95% CI Lower\": stats[\"CI Lower\"],\n",
    "            \"95% CI Upper\": stats[\"CI Upper\"]\n",
    "        })\n",
    "\n",
    "    # Add AUC metric\n",
    "    classification_metrics_data.append({\n",
    "        \"Metric\": \"AUC\",\n",
    "        \"Point Estimate\": auc_mean,\n",
    "        \"95% CI Lower\": auc_lower,\n",
    "        \"95% CI Upper\": auc_upper\n",
    "    })\n",
    "\n",
    "    classification_metrics_df = pd.DataFrame(classification_metrics_data)\n",
    "\n",
    "    # Save classification metrics\n",
    "    os.makedirs(output_dir_classification, exist_ok=True)\n",
    "    classification_csv_path = os.path.join(output_dir_classification, classification_filename)\n",
    "    classification_metrics_df.to_csv(classification_csv_path, index=False)\n",
    "    print(f\"Classification metrics saved to {classification_csv_path}\")\n",
    "\n",
    "    # Plot ROC curve and save it\n",
    "    def plot_roc_curve(actuals, predictions, output_path):\n",
    "        fpr, tpr, _ = roc_curve(actuals, predictions)\n",
    "        auc = roc_auc_score(actuals, predictions)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        print(f\"ROC curve saved to {output_path}\")\n",
    "\n",
    "    # Save the ROC curve plot\n",
    "    roc_curve_path = os.path.join(output_dir_classification, 'roc_curve.png')\n",
    "    plot_roc_curve(unique_encounters['actual_fever'], unique_encounters['predictions'], roc_curve_path)\n",
    "\n",
    "    return predictions, actuals, merged_data,unique_encounters['actual_fever'],unique_encounters['predictions']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_use_case_3_data_with_scaling(data_use_case_3,start_date,end_date):\n",
    "    df_sorted = data_use_case_3.sort_values(by='performedDateTime').reset_index(drop=True)\n",
    "    df_no_duplicates = df_sorted.drop_duplicates(subset=['performedDateTime', 'subject_reference', 'encounter_reference']).reset_index(drop=True)\n",
    "\n",
    "    # Generate a date range with unique dates\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "    # Duplicate each date to have two entries per day\n",
    "    date_range = date_range.repeat(2)\n",
    "    df_no_duplicates[\"date\"] = pd.to_datetime(df_no_duplicates[\"date\"])\n",
    "\n",
    "    ctscans = pd.DataFrame(date_range, columns=[\"date\"])\n",
    "    ctscans[\"time_of_day\"] = ctscans.groupby(\"date\").cumcount().map({0: \"a_morning\", 1: \"b_evening\"})\n",
    "\n",
    "    count_df = df_no_duplicates.groupby([\"date\", \"time_of_day\"]).size().reset_index(name=\"count_scans\")\n",
    "    ctscans = pd.merge(ctscans, count_df, on=[\"date\", \"time_of_day\"], how=\"left\")\n",
    "    ctscans[\"count_scans\"] = ctscans[\"count_scans\"].fillna(0).astype(int)\n",
    "    ctscans = ctscans.drop(index=0)\n",
    "\n",
    "    # Reset the index if desired, to avoid a missing index number\n",
    "    ctscans = ctscans.reset_index(drop=True)\n",
    "\n",
    "    # Create a new column 'shift_count' initialized to NaN\n",
    "    ctscans[\"shift_count\"] = np.nan\n",
    "\n",
    "    # Iterate over the rows and sum count_scans for each consecutive \"b_evening\" and \"a_morning\" pair\n",
    "    for i in range(len(ctscans) - 1):\n",
    "        if ctscans.loc[i, \"time_of_day\"] == \"b_evening\" and ctscans.loc[i + 1, \"time_of_day\"] == \"a_morning\":\n",
    "            # Sum count_scans for the consecutive \"b_evening\" and \"a_morning\"\n",
    "            shift_sum = ctscans.loc[i, \"count_scans\"] + ctscans.loc[i + 1, \"count_scans\"]\n",
    "            \n",
    "            # Assign the result to the first row of the pair (\"b_evening\" row)\n",
    "            ctscans.loc[i, \"shift_count\"] = shift_sum\n",
    "\n",
    "    # Optionally, forward-fill or set NaN values to 0 in 'shift_count'\n",
    "    ctscans[\"shift_count\"] = ctscans[\"shift_count\"].fillna(0).astype(int)\n",
    "    ctscans = ctscans[ctscans[\"time_of_day\"] != \"a_morning\"].reset_index(drop=True)\n",
    "    ctscans[\"day_of_week\"] = ctscans[\"date\"].dt.day_name()\n",
    "    last_friday = ctscans[ctscans[\"day_of_week\"] == \"Friday\"][\"date\"].max()\n",
    "\n",
    "    # Step 2: Initialize ts_id counter and end_date for the first 4-week period\n",
    "    current_id = 1\n",
    "    end_date = last_friday\n",
    "\n",
    "    # Step 3: Create an empty 'ts_id' column\n",
    "    ctscans[\"ts_id\"] = 0\n",
    "\n",
    "    # Step 4: Assign ts_id to each 4-week (28-day) period, going backwards from the last Friday\n",
    "    while end_date >= ctscans[\"date\"].min():\n",
    "        start_date = end_date - pd.Timedelta(weeks=4)\n",
    "\n",
    "        # Assign the current ts_id to dates within the 4-week range\n",
    "        ctscans.loc[(ctscans[\"date\"] > start_date) & (ctscans[\"date\"] <= end_date), \"ts_id\"] = current_id\n",
    "\n",
    "        # Move to the next 4-week period back in time\n",
    "        end_date = start_date\n",
    "        current_id += 1\n",
    "\n",
    "    # Min-Max scaling for 'count_scans' and 'shift_count'\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_columns = [\"shift_count\"]\n",
    "\n",
    "    # Apply Min-Max Scaling\n",
    "    ctscans[scaled_columns] = scaler.fit_transform(ctscans[scaled_columns])\n",
    "\n",
    "    return ctscans, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_windows_with_weekday_prediction(data, target_col, lookback, horizon, prediction_days):\n",
    "    \"\"\"\n",
    "    Creates rolling windows for a time series while preserving dates for each window.\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame containing the time series.\n",
    "        target_col (str): Column name of the target variable.\n",
    "        lookback (int): Number of steps to look back.\n",
    "        horizon (int): Number of steps to forecast.\n",
    "        prediction_days (list): List of valid day names for the prediction window (e.g., ['Monday', ..., 'Friday']).\n",
    "    Returns:\n",
    "        X (list): List of dictionaries containing input sequences and their corresponding dates.\n",
    "        y (list): List of dictionaries containing target sequences and their corresponding dates.\n",
    "        dates (list): Start dates of each rolling window.\n",
    "    \"\"\"\n",
    "    X, y, dates = [], [], []\n",
    "\n",
    "    for i in range(len(data) - lookback - horizon + 1):\n",
    "        # Define the input (lookback) window\n",
    "        window = data.iloc[i:i + lookback]\n",
    "        input_values = window[target_col].values\n",
    "        input_dates = window['date'].values\n",
    "        \n",
    "        # Define the prediction (horizon) window\n",
    "        prediction_window = data.iloc[i + lookback:i + lookback + horizon]\n",
    "        target_values = prediction_window[prediction_window['day_of_week'].isin(prediction_days)][target_col].values\n",
    "        target_dates = prediction_window[prediction_window['day_of_week'].isin(prediction_days)]['date'].values\n",
    "\n",
    "        # Ensure the prediction window matches the expected length\n",
    "        if len(target_values) == len(prediction_days):\n",
    "            X.append({\"values\": input_values, \"dates\": input_dates})\n",
    "            y.append({\"values\": target_values, \"dates\": target_dates})\n",
    "            dates.append(data.iloc[i + lookback]['date'])\n",
    "\n",
    "    def convert_to_dataframe_with_id(data, id_name):\n",
    "        \"\"\"\n",
    "        Converts a list of dictionaries with 'values' and 'dates' into a DataFrame, \n",
    "        assigning a unique id to each sequence.\n",
    "        Args:\n",
    "            data (list): List of dictionaries with keys 'values' and 'dates'.\n",
    "            id_name (str): Column name for the unique identifier.\n",
    "        Returns:\n",
    "            DataFrame: Combined DataFrame of all sequences with dates, values, and ids.\n",
    "        \"\"\"\n",
    "        df_list = []\n",
    "        for idx, entry in enumerate(data, start=1):\n",
    "            temp_df = pd.DataFrame({\n",
    "                \"date\": entry[\"dates\"],\n",
    "                \"value\": entry[\"values\"],\n",
    "                id_name: idx  # Assign a unique ID for each sequence\n",
    "            })\n",
    "            df_list.append(temp_df)\n",
    "        return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Example X and y conversion with IDs\n",
    "\n",
    "    X_df = convert_to_dataframe_with_id(X, id_name=\"X_id\")\n",
    "    y_df = convert_to_dataframe_with_id(y, id_name=\"y_id\")\n",
    "    def merge_and_order_dataframes(X_df, y_df, X_id_col=\"X_id\", y_id_col=\"y_id\"):\n",
    "        \"\"\"\n",
    "        Merges X_df and y_df, ordering by id and date, and distinguishing X and y records.\n",
    "        Args:\n",
    "            X_df (DataFrame): DataFrame for X with date, value, and ID.\n",
    "            y_df (DataFrame): DataFrame for y with date, value, and ID.\n",
    "            X_id_col (str): Column name for X IDs.\n",
    "            y_id_col (str): Column name for y IDs.\n",
    "        Returns:\n",
    "            DataFrame: Merged and ordered DataFrame.\n",
    "        \"\"\"\n",
    "        # Add a label column to distinguish X and y\n",
    "        X_df[\"type\"] = \"X\"\n",
    "        y_df[\"type\"] = \"y\"\n",
    "        \n",
    "        # Rename ID columns to align for merging\n",
    "        X_df = X_df.rename(columns={X_id_col: \"id\"})\n",
    "        y_df = y_df.rename(columns={y_id_col: \"id\"})\n",
    "\n",
    "        # Concatenate X and y DataFrames\n",
    "        merged_df = pd.concat([X_df, y_df], ignore_index=True)\n",
    "        \n",
    "        # Order by id and date\n",
    "        merged_df = merged_df.sort_values(by=[\"id\", \"date\"]).reset_index(drop=True)\n",
    "        \n",
    "        return merged_df,X_df,y_df\n",
    "    merged_df,X_df,y_df = merge_and_order_dataframes(X_df, y_df)\n",
    "\n",
    "    return X, y, dates,X_df,y_df,merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_dictionary(full_data, X_df, y_df):\n",
    "    value_to_predict = 'value'\n",
    "    \n",
    "    def create_timeseries_dict(grouped_data):\n",
    "        series_dict = {}\n",
    "        for encounter_id, group in grouped_data:\n",
    "            group['date'] = pd.to_datetime(group['date'])  # Ensure 'date' is datetime\n",
    "            group = group.sort_values(by='date')\n",
    "            group = group.set_index('date')  # Set 'date' as index\n",
    "            ts_series = TimeSeries.from_dataframe(group, value_cols=value_to_predict, freq='D')\n",
    "            series_dict[encounter_id] = ts_series\n",
    "        return series_dict\n",
    "\n",
    "    grouped_train = X_df.groupby('id')\n",
    "    grouped_test = y_df.groupby('id')\n",
    "    grouped_validation = full_data.groupby('id')\n",
    "\n",
    "    train_series_dict = create_timeseries_dict(grouped_train)\n",
    "    test_series_dict = create_timeseries_dict(grouped_test)\n",
    "    valid_series_dict = create_timeseries_dict(grouped_validation)\n",
    "\n",
    "    return valid_series_dict, test_series_dict, train_series_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_validation_split(train_series_dict, valid_series_dict, input_chunk_length, output_chunk_length):\n",
    "    \"\"\"\n",
    "    Adjusts train_series_dict to ensure no overlap with targets in valid_series_dict.\n",
    "    Ensures strict separation to prevent information leakage.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_series_dict: Dictionary of training series (time series without targets).\n",
    "    - valid_series_dict: Dictionary of validation series (includes prediction targets).\n",
    "    - input_chunk_length: Length of the input window for the model.\n",
    "    - output_chunk_length: Length of the prediction horizon.\n",
    "    \n",
    "    Returns:\n",
    "    - adjusted_train_series_dict: Training series dictionary without overlapping target data.\n",
    "    - valid_series_dict: Validation series dictionary remains unchanged.\n",
    "    \"\"\"\n",
    "    # Initialize the adjusted training series dictionary\n",
    "    adjusted_train_series_dict = {}\n",
    "    for key, train_series in train_series_dict.items():\n",
    "        #print('TRAIN:', key)\n",
    "        # Check if the key exists in both dictionaries\n",
    "        if key not in valid_series_dict:\n",
    "            continue\n",
    "\n",
    "        # Extract the corresponding validation series\n",
    "        valid_series = valid_series_dict[key]\n",
    "\n",
    "        # Identify the last index of the training data\n",
    "        train_end_idx = len(train_series) - output_chunk_length  # Leave gap for prediction horizon\n",
    "        #print(\"train_end_idx\", train_end_idx)\n",
    "        # Truncate training series to avoid overlap with validation targets\n",
    "        adjusted_train_series = train_series[:train_end_idx]\n",
    "        adjusted_train_series_dict[key] = adjusted_train_series\n",
    "    #print('adjusted_train_series_dict:', train_series_dict)\n",
    "    return train_series_dict, valid_series_dict\n",
    "\n",
    "    #return train_series_dict, valid_series_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_time_series_split(train_series_dict, valid_series_dict, n_splits):\n",
    "    \"\"\"\n",
    "    Custom time-based splitting function for dictionaries of time series.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_series_dict: Dictionary of training time series.\n",
    "    - valid_series_dict: Dictionary of validation time series.\n",
    "    - n_splits: Number of splits for cross-validation.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of (train_keys, valid_keys) pairs for each fold.\n",
    "    \"\"\"\n",
    "    keys = list(train_series_dict.keys())\n",
    "    #print(keys)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    splits = []\n",
    "    for train_idx, valid_idx in kf.split(keys):\n",
    "        train_keys = [keys[i] for i in train_idx]\n",
    "        valid_keys = [keys[i] for i in valid_idx]\n",
    "        splits.append((train_keys, valid_keys))\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_kfold_cv_no_leakage_fixed(model_class, train_series_dict, valid_series_dict, scaler, \n",
    "                                          prediction_length, input_chunk_length, output_chunk_length, \n",
    "                                          epochs=50, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for time series with no information leakage.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_class: The class of the model (e.g., NBEATSModel).\n",
    "    - train_series_dict: Dictionary of training time series.\n",
    "    - valid_series_dict: Dictionary of validation time series.\n",
    "    - scaler: Scaler used for rescaling the data.\n",
    "    - prediction_length: Prediction horizon.\n",
    "    - input_chunk_length: Input chunk length for the model.\n",
    "    - output_chunk_length: Output chunk length for the model.\n",
    "    - epochs: Number of epochs to train the model.\n",
    "    - n_splits: Number of folds.\n",
    "    - random_state: Random state for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - Fold-wise metrics and predictions.\n",
    "    \"\"\"\n",
    "    splits = custom_time_series_split(train_series_dict, valid_series_dict, n_splits)\n",
    "    fold_results = []\n",
    "    #print(splits)\n",
    "    for fold, (train_keys, valid_keys) in enumerate(splits):\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "        train_fold_series = {k: train_series_dict[k] for k in train_keys}\n",
    "        train_fold_series_validation = {k: valid_series_dict[k] for k in train_keys}\n",
    "\n",
    "        valid_fold_series = {k: valid_series_dict[k] for k in valid_keys}\n",
    "        #print(train_fold_series)\n",
    "        # Ensure no overlap between train and validation\n",
    "        train_fold_series, valid_fold_series = prepare_train_validation_split(\n",
    "            train_fold_series, valid_fold_series, input_chunk_length, output_chunk_length\n",
    "        )\n",
    "\n",
    "        print(f\"Fold {fold + 1}: {len(train_fold_series)} training series, {len(valid_fold_series)} validation series.\")\n",
    "\n",
    "        # Skip if no training series are left\n",
    "        if len(train_fold_series) == 0:\n",
    "            print(f\"Skipping Fold {fold + 1} due to empty training set.\")\n",
    "            continue\n",
    "\n",
    "        # Initialize and train the model\n",
    "        # for nbeats\n",
    "        #model = model_class(input_chunk_length=input_chunk_length, output_chunk_length=prediction_length, \n",
    "        #                    random_state=random_state, likelihood=QuantileRegression())\n",
    "        #for lightgbm\n",
    "        model = model_class(lags=input_chunk_length, output_chunk_length=prediction_length, \n",
    "                            random_state=random_state, likelihood=\"quantile\",quantiles=[0.05, 0.1, 0.5, 0.9, 0.95], verbose=-1)\n",
    "        try:\n",
    "            #nbeats\n",
    "            #model.fit(series=list(train_fold_series_validation.values()), epochs=epochs, verbose=True)\n",
    "            #lightgbm\n",
    "            model.fit(series=list(train_fold_series_validation.values()))\n",
    "\n",
    "            #model.fit(series=list(train_fold_series.values()),val_series=list(train_fold_series_validation.values()), epochs=epochs, verbose=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting model in Fold {fold + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Make predictions\n",
    "        try:\n",
    "                    # Truncate validation series to exclude the last `prediction_length` entries\n",
    "            truncated_valid_fold_series = {\n",
    "            key: series[:-prediction_length] for key, series in valid_fold_series.items()\n",
    "            }\n",
    "            #print('train',truncated_valid_fold_series.values())\n",
    "            pred = model.predict(n=prediction_length, series=list(truncated_valid_fold_series.values()), num_samples=500)\n",
    "            pred_arrays = np.array([ts.all_values() for ts in pred]).squeeze(axis=2)\n",
    "            pred_medians = np.median(pred_arrays, axis=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting in Fold {fold + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Prepare actual and predicted values\n",
    "\n",
    "        # Align actual values with predictions\n",
    "        actual_values = []\n",
    "        predicted_values = []\n",
    "\n",
    "        # Use the same keys to ensure alignment\n",
    "        for key, pred_series in zip(valid_keys, pred_medians):\n",
    "            #print('valid',valid_series_dict[key].values())\n",
    "            actual_values.append(valid_series_dict[key].values()[-prediction_length:].flatten())\n",
    "            predicted_values.append(pred_series)\n",
    "            \n",
    "        #for pred_series in pred_medians:\n",
    "        #    predicted_values.append(pred_series)\n",
    "\n",
    "        actual_values_np = np.array(actual_values)\n",
    "        predicted_values_np = np.array(predicted_values)\n",
    "        #print('train',train_fold_series.values())\n",
    "        #print('predicted', predicted_values_np)\n",
    "        #print('valid',valid_series_dict.values())\n",
    "        #print('actual', actual_values_np)\n",
    "\n",
    "        # Rescale predictions\n",
    "        actual_values_rescaled = scaler.inverse_transform(actual_values_np)\n",
    "        predicted_values_rescaled = scaler.inverse_transform(predicted_values_np)\n",
    "\n",
    "        # Evaluate metrics\n",
    "        metrics = bootstrap_metrics(actual_values_rescaled.flatten(), predicted_values_rescaled.flatten())\n",
    "\n",
    "        # Store metrics for this fold\n",
    "        fold_results.append({\n",
    "            \"fold\": fold + 1,\n",
    "            \"metrics\": metrics,\n",
    "            \"predictions\": predicted_values_rescaled,\n",
    "            \"actuals\": actual_values_rescaled,\n",
    "        })\n",
    "        print(f\"Fold {fold + 1} results: {metrics}\")\n",
    "\n",
    "    # Aggregate results\n",
    "    if fold_results:\n",
    "        aggregated_metrics = {metric: np.mean([fold[\"metrics\"][metric][0] for fold in fold_results]) \n",
    "                              for metric in [\"mae\", \"mse\", \"rmse\", \"mape\", \"smape\"]}\n",
    "        print(\"Aggregated cross-validation metrics MEAN:\")\n",
    "        print(aggregated_metrics)\n",
    "        aggregated_metrics_05 = {metric: np.mean([fold[\"metrics\"][metric][1] for fold in fold_results]) \n",
    "                              for metric in [\"mae\", \"mse\", \"rmse\", \"mape\", \"smape\"]}\n",
    "        print(\"Aggregated cross-validation metrics CI-LOW:\")\n",
    "        print(aggregated_metrics_05)\n",
    "        aggregated_metrics_95 = {metric: np.mean([fold[\"metrics\"][metric][2] for fold in fold_results]) \n",
    "                              for metric in [\"mae\", \"mse\", \"rmse\", \"mape\", \"smape\"]}\n",
    "        print(\"Aggregated cross-validation metrics CI-UP:\")\n",
    "        print(aggregated_metrics_95)\n",
    "    else:\n",
    "        print(\"No valid folds to aggregate metrics.\")\n",
    "        aggregated_metrics = {}\n",
    "\n",
    "    return fold_results, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_metrics_auto(actual_values, predicted_values, n_bootstrap=1000, ci_percentile=95):\n",
    "    # Initialize arrays to store metrics across bootstraps\n",
    "    mae_bootstrap = []\n",
    "    mse_bootstrap = []\n",
    "    rmse_bootstrap = []\n",
    "    mape_bootstrap = []\n",
    "    smape_bootstrap = []\n",
    "\n",
    "    n_samples = len(actual_values)\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample with replacement\n",
    "        resample_indices = np.random.choice(np.arange(n_samples), size=n_samples, replace=True)\n",
    "        actual_resample = np.array([actual_values[i] for i in resample_indices])\n",
    "        predicted_resample = np.array([predicted_values[i] for i in resample_indices])\n",
    "\n",
    "        # Calculate metrics for this bootstrap sample\n",
    "        mae = np.mean(np.abs(actual_resample - predicted_resample))\n",
    "        mse = np.mean((actual_resample - predicted_resample) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mape = np.mean(np.abs((actual_resample - predicted_resample) / actual_resample)) * 100\n",
    "        smape = np.mean(np.abs(actual_resample - predicted_resample) / (np.abs(actual_resample) + np.abs(predicted_resample))) * 100\n",
    "\n",
    "        # Append the results\n",
    "        mae_bootstrap.append(mae)\n",
    "        mse_bootstrap.append(mse)\n",
    "        rmse_bootstrap.append(rmse)\n",
    "        mape_bootstrap.append(mape)\n",
    "        smape_bootstrap.append(smape)\n",
    "\n",
    "    # Calculate mean and confidence intervals\n",
    "    metrics = {\n",
    "        \"mae\": (np.mean(mae_bootstrap), np.percentile(mae_bootstrap, (100 - ci_percentile) / 2), np.percentile(mae_bootstrap, 100 - (100 - ci_percentile) / 2)),\n",
    "        \"mse\": (np.mean(mse_bootstrap), np.percentile(mse_bootstrap, (100 - ci_percentile) / 2), np.percentile(mse_bootstrap, 100 - (100 - ci_percentile) / 2)),\n",
    "        \"rmse\": (np.mean(rmse_bootstrap), np.percentile(rmse_bootstrap, (100 - ci_percentile) / 2), np.percentile(rmse_bootstrap, 100 - (100 - ci_percentile) / 2)),\n",
    "        \"mape\": (np.mean(mape_bootstrap), np.percentile(mape_bootstrap, (100 - ci_percentile) / 2), np.percentile(mape_bootstrap, 100 - (100 - ci_percentile) / 2)),\n",
    "        \"smape\": (np.mean(smape_bootstrap), np.percentile(smape_bootstrap, (100 - ci_percentile) / 2), np.percentile(smape_bootstrap, 100 - (100 - ci_percentile) / 2)),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from autogluon.timeseries import TimeSeriesDataFrame\n",
    "\n",
    "def autogluon_data_ci_roc(model_name, output_dir_forecasting, output_dir_classification, forecasting_filename, classification_filename, resampled_df, predictor, scaler, prediction_length, value_to_predict, n_bootstrap=1000):\n",
    "    resampled_df = resampled_df.rename(columns={'id': 'encounter_id', 'date': 'recorded_time'})\n",
    "    # Split the data into train and test sets\n",
    "    def split_train_test(group):\n",
    "        test_rows = group.nlargest(prediction_length, 'recorded_time')\n",
    "        train_rows = group.drop(test_rows.index)\n",
    "        return train_rows, test_rows\n",
    "\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "\n",
    "    # Group by encounter_id and split each group\n",
    "    for name, group in resampled_df.groupby('encounter_id'):\n",
    "        train_rows, test_rows = split_train_test(group)\n",
    "        train_list.append(train_rows)\n",
    "        test_list.append(test_rows)\n",
    "\n",
    "    # Concatenate the lists into DataFrames\n",
    "    train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "    test_data = pd.concat(test_list).reset_index(drop=True)\n",
    "    train_data = train_data.sort_values(by=['encounter_id', 'recorded_time']).reset_index(drop=True)\n",
    "    test_data = test_data.sort_values(by=['encounter_id', 'recorded_time']).reset_index(drop=True)\n",
    "\n",
    "    train_data_ = train_data[['encounter_id', 'recorded_time', value_to_predict]]\n",
    "    train_data_['recorded_time'] = train_data_['recorded_time'].dt.tz_localize(None)\n",
    "    test_data_ = test_data[['encounter_id', 'recorded_time', value_to_predict]]\n",
    "    test_data_['recorded_time'] = test_data_['recorded_time'].dt.tz_localize(None)\n",
    "\n",
    "    train_data = TimeSeriesDataFrame.from_data_frame(\n",
    "        train_data_,\n",
    "        id_column=\"encounter_id\",\n",
    "        timestamp_column=\"recorded_time\"\n",
    "    )\n",
    "\n",
    "    test_data = TimeSeriesDataFrame.from_data_frame(\n",
    "        test_data_,\n",
    "        id_column=\"encounter_id\",\n",
    "        timestamp_column=\"recorded_time\"\n",
    "    )\n",
    "\n",
    "    # Generate predictions using AutoGluon predictor\n",
    "    predictions = predictor.predict(train_data, model=model_name)\n",
    "    predictions_ = predictions[['mean', '0.05', '0.95']]\n",
    "\n",
    "    # Join actual and predicted data\n",
    "    merged_data = test_data.join(predictions_, how=\"inner\")\n",
    "\n",
    "    actuals = merged_data[value_to_predict].values.reshape(-1, 1)\n",
    "    predictions = merged_data['mean'].values.reshape(-1, 1)\n",
    "\n",
    "    # Rescale the actual and predicted values back to the original scale\n",
    "    def rescale_predictions(scaled_values, scaler):\n",
    "        if len(scaled_values.shape) == 1:\n",
    "            scaled_values = scaled_values.reshape(-1, 1)\n",
    "        return scaler.inverse_transform(scaled_values).flatten()\n",
    "\n",
    "    actuals = rescale_predictions(actuals, scaler)\n",
    "    predictions = rescale_predictions(predictions, scaler)\n",
    "\n",
    "    # Bootstrapping to compute confidence intervals for regression metrics\n",
    "    metrics_forecasting = bootstrap_metrics_auto(actuals, predictions, n_bootstrap=n_bootstrap)\n",
    "\n",
    "    # Prepare data for forecasting metrics CSV\n",
    "    forecasting_metrics_data = []\n",
    "    for metric, values in metrics_forecasting.items():\n",
    "        forecasting_metrics_data.append({\n",
    "            \"Metric\": metric.upper(),\n",
    "            \"Point Estimate\": values[0],\n",
    "            \"95% CI Lower\": values[1],\n",
    "            \"95% CI Upper\": values[2]\n",
    "        })\n",
    "\n",
    "    # Create DataFrame for forecasting metrics\n",
    "    forecasting_metrics_df = pd.DataFrame(forecasting_metrics_data)\n",
    "\n",
    "    # Save forecasting metrics\n",
    "    os.makedirs(output_dir_forecasting, exist_ok=True)\n",
    "    forecasting_csv_path = os.path.join(output_dir_forecasting, forecasting_filename)\n",
    "    forecasting_metrics_df.to_csv(forecasting_csv_path, index=False)\n",
    "    print(f\"Forecasting metrics saved to {forecasting_csv_path}\")\n",
    "\n",
    "    # Clinical outcome estimates for classification\n",
    "    merged_data['actual'] = actuals.flatten()\n",
    "    merged_data['predictions'] = predictions.flatten()\n",
    "    fever_flags_actual = merged_data.groupby(level=0)['actual'].transform(lambda x: (x >= 38).any())\n",
    "    fever_flags_predicted = merged_data.groupby(level=0)['predictions'].transform(lambda x: (x >= 38).any())\n",
    "    merged_data['actual_fever'] = fever_flags_actual\n",
    "    merged_data['predictions_fever'] = fever_flags_predicted\n",
    "    merged_data = merged_data.reset_index(level=0)\n",
    "    unique_encounters = merged_data.drop_duplicates(subset=['item_id'])\n",
    "\n",
    "    # Confusion matrix computation\n",
    "    confusion_conditions = [\n",
    "        (unique_encounters['actual_fever'] == True) & (unique_encounters['predictions_fever'] == True),  # TP\n",
    "        (unique_encounters['actual_fever'] == True) & (unique_encounters['predictions_fever'] == False), # FN\n",
    "        (unique_encounters['actual_fever'] == False) & (unique_encounters['predictions_fever'] == True), # FP\n",
    "        (unique_encounters['actual_fever'] == False) & (unique_encounters['predictions_fever'] == False) # TN\n",
    "    ]\n",
    "    confusion_choices = ['TP', 'FN', 'FP', 'TN']\n",
    "    unique_encounters['confusion'] = np.select(confusion_conditions, confusion_choices)\n",
    "\n",
    "    confusion_matrix_bootstrap = {'TP': [], 'FN': [], 'FP': [], 'TN': []}\n",
    "    metrics_bootstrap = {'sensitivity': [], 'specificity': [], 'ppv': [], 'npv': [], 'accuracy': [], 'f1': [], 'mcc': []}\n",
    "    auc_scores = []  # For storing AUC scores\n",
    "\n",
    "    # Bootstrapping for confusion matrix, classification metrics, and AUC\n",
    "    for _ in range(n_bootstrap):\n",
    "        bootstrap_sample = unique_encounters.sample(frac=1, replace=True)\n",
    "        tp = ((bootstrap_sample['actual_fever'] == True) & (bootstrap_sample['predictions_fever'] == True)).sum()\n",
    "        fn = ((bootstrap_sample['actual_fever'] == True) & (bootstrap_sample['predictions_fever'] == False)).sum()\n",
    "        fp = ((bootstrap_sample['actual_fever'] == False) & (bootstrap_sample['predictions_fever'] == True)).sum()\n",
    "        tn = ((bootstrap_sample['actual_fever'] == False) & (bootstrap_sample['predictions_fever'] == False)).sum()\n",
    "\n",
    "        confusion_matrix_bootstrap['TP'].append(tp)\n",
    "        confusion_matrix_bootstrap['FN'].append(fn)\n",
    "        confusion_matrix_bootstrap['FP'].append(fp)\n",
    "        confusion_matrix_bootstrap['TN'].append(tn)\n",
    "\n",
    "        metrics_bootstrap['sensitivity'].append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['specificity'].append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        metrics_bootstrap['ppv'].append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "        metrics_bootstrap['npv'].append(tn / (tn + fn) if (tn + fn) > 0 else 0)\n",
    "        metrics_bootstrap['accuracy'].append((tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['f1'].append((2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['mcc'].append(((tp * tn) - (fp * fn)) / (\n",
    "            ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "        ) if ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) > 0 else 0)\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(bootstrap_sample['actual_fever'], bootstrap_sample['predictions'])\n",
    "        except ValueError:\n",
    "            auc = np.nan  # Handle case where AUC cannot be computed (only one class present)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    # Confidence intervals for confusion matrix\n",
    "    confusion_matrix_ci = {}\n",
    "    for key, values in confusion_matrix_bootstrap.items():\n",
    "        lower = np.percentile(values, 2.5)\n",
    "        upper = np.percentile(values, 97.5)\n",
    "        mean_value = np.mean(values)\n",
    "        confusion_matrix_ci[key] = {\"Mean\": mean_value, \"CI Lower\": lower, \"CI Upper\": upper}\n",
    "\n",
    "    # Confidence intervals for classification metrics\n",
    "    metrics_ci = {}\n",
    "    for metric, values in metrics_bootstrap.items():\n",
    "        lower = np.percentile(values, 2.5)\n",
    "        upper = np.percentile(values, 97.5)\n",
    "        mean_value = np.mean(values)\n",
    "        metrics_ci[metric] = {\"Mean\": mean_value, \"CI Lower\": lower, \"CI Upper\": upper}\n",
    "\n",
    "    # Confidence intervals for AUC\n",
    "    auc_mean = np.nanmean(auc_scores)\n",
    "    auc_lower = np.nanpercentile(auc_scores, 2.5)\n",
    "    auc_upper = np.nanpercentile(auc_scores, 97.5)\n",
    "\n",
    "    # Prepare data for classification metrics CSV\n",
    "    classification_metrics_data = []\n",
    "    for metric, stats in metrics_ci.items():\n",
    "        classification_metrics_data.append({\n",
    "            \"Metric\": metric.capitalize(),\n",
    "            \"Point Estimate\": stats[\"Mean\"],\n",
    "            \"95% CI Lower\": stats[\"CI Lower\"],\n",
    "            \"95% CI Upper\": stats[\"CI Upper\"]\n",
    "        })\n",
    "\n",
    "    # Add confusion matrix stats\n",
    "    for key, stats in confusion_matrix_ci.items():\n",
    "        classification_metrics_data.append({\n",
    "            \"Metric\": f\"Avg {key}\",\n",
    "            \"Point Estimate\": stats[\"Mean\"],\n",
    "            \"95% CI Lower\": stats[\"CI Lower\"],\n",
    "            \"95% CI Upper\": stats[\"CI Upper\"]\n",
    "        })\n",
    "\n",
    "    # Add AUC metric\n",
    "    classification_metrics_data.append({\n",
    "        \"Metric\": \"AUC\",\n",
    "        \"Point Estimate\": auc_mean,\n",
    "        \"95% CI Lower\": auc_lower,\n",
    "        \"95% CI Upper\": auc_upper\n",
    "    })\n",
    "\n",
    "    classification_metrics_df = pd.DataFrame(classification_metrics_data)\n",
    "\n",
    "    # Save classification metrics\n",
    "    os.makedirs(output_dir_classification, exist_ok=True)\n",
    "    classification_csv_path = os.path.join(output_dir_classification, classification_filename)\n",
    "    classification_metrics_df.to_csv(classification_csv_path, index=False)\n",
    "    print(f\"Classification metrics saved to {classification_csv_path}\")\n",
    "\n",
    "    # Plot ROC curve and save it\n",
    "    def plot_roc_curve(actuals, predictions, output_path):\n",
    "        fpr, tpr, _ = roc_curve(actuals, predictions)\n",
    "        auc = roc_auc_score(actuals, predictions)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        print(f\"ROC curve saved to {output_path}\")\n",
    "\n",
    "    # Save the ROC curve plot\n",
    "    roc_curve_path = os.path.join(output_dir_classification, 'roc_curve.png')\n",
    "    plot_roc_curve(unique_encounters['actual_fever'], unique_encounters['predictions'], roc_curve_path)\n",
    "\n",
    "    return predictions, actuals, train_data, test_data, merged_data,unique_encounters['actual_fever'],unique_encounters['predictions']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timellm_data_ci_roc_(model_name, output_dir_forecasting, output_dir_classification, forecasting_filename, classification_filename, resampled_df,nf, scaler, prediction_length, value_to_predict, n_bootstrap=1000):\n",
    "    \n",
    "    full_data_test = resampled_df.groupby(\"encounter_id\").tail(prediction_length)\n",
    "    full_data_train = resampled_df.drop(full_data_test.index)\n",
    "    full_data_train_ = full_data_train.rename(columns={'recorded_time': 'ds', 'encounter_id': 'unique_id','value':'y'})\n",
    "    full_data_test_ = full_data_test.rename(columns={'recorded_time': 'ds', 'encounter_id': 'unique_id','value':'y'})\n",
    "    #full_data_train_ = full_data_train_.drop(columns=['subject_reference','code_coding_0_display'])\n",
    "    #full_data_test_ = full_data_test_.drop(columns=['subject_reference','code_coding_0_display'])\n",
    "    forecasts = nf.predict(full_data_train_)\n",
    "    def rescale_to_original(scaled_df, scaler, columns_to_rescale):\n",
    "        scaled_df[columns_to_rescale] = scaler.inverse_transform(scaled_df[columns_to_rescale])\n",
    "        return scaled_df\n",
    "    columns_to_rescale_actual = [\"y\"]\n",
    "    columns_to_rescale_predict = [\"TimeLLM\"]\n",
    "    actuals = rescale_to_original(full_data_test_, scaler,columns_to_rescale_actual)\n",
    "    predictions = rescale_to_original(forecasts, scaler,columns_to_rescale_predict)\n",
    "\n",
    "    def bootstrap_timellm_metrics_length(forecasts_df, actual_values_df, prediction_length, n_bootstrap, ci_percentile=95):\n",
    "        mae_bootstrap = []\n",
    "        mse_bootstrap = []\n",
    "        rmse_bootstrap = []\n",
    "        smape_bootstrap = []\n",
    "        mape_bootstrap = []\n",
    "\n",
    "        # Merge forecasts and actual values on 'unique_id' and 'ds'\n",
    "        merged_df = pd.merge(forecasts_df.reset_index(), actual_values_df, on=['unique_id', 'ds'])\n",
    "\n",
    "        unique_ids = merged_df['unique_id'].unique()\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            # Sample with replacement\n",
    "            sampled_ids = np.random.choice(unique_ids, size=len(unique_ids), replace=True)\n",
    "            sampled_data = merged_df[merged_df['unique_id'].isin(sampled_ids)]\n",
    "\n",
    "            total_mae = 0\n",
    "            total_mse = 0\n",
    "            total_smape = 0\n",
    "            total_mape = 0\n",
    "\n",
    "            for unique_id in sampled_ids:\n",
    "                # Select the forecast and true values for this unique_id\n",
    "                data = sampled_data[sampled_data['unique_id'] == unique_id].sort_values(by='ds')\n",
    "                \n",
    "                forecast_values = data['TimeLLM'].values[-prediction_length:]\n",
    "                true_values = data['y'].values[-prediction_length:]\n",
    "\n",
    "                # Compute errors\n",
    "                absolute_errors = np.abs(forecast_values - true_values)\n",
    "                \n",
    "                # MAE\n",
    "                mae = absolute_errors.mean()\n",
    "                total_mae += mae\n",
    "\n",
    "                # MSE\n",
    "                mse = (absolute_errors ** 2).mean()\n",
    "                total_mse += mse\n",
    "\n",
    "                # SMAPE\n",
    "                denominator = (np.abs(true_values) + np.abs(forecast_values)) / 2\n",
    "                smape = (absolute_errors / denominator).mean() * 100\n",
    "                total_smape += smape\n",
    "\n",
    "                # MAPE\n",
    "                mape = (np.abs((true_values - forecast_values) / true_values).mean()) * 100\n",
    "                total_mape += mape\n",
    "\n",
    "            # Compute overall metrics for this bootstrap sample\n",
    "            mae_bootstrap.append(total_mae / len(sampled_ids))\n",
    "            mse_bootstrap.append(total_mse / len(sampled_ids))\n",
    "            rmse_bootstrap.append(np.sqrt(total_mse / len(sampled_ids)))\n",
    "            smape_bootstrap.append(total_smape / len(sampled_ids))\n",
    "            mape_bootstrap.append(total_mape / len(sampled_ids))\n",
    "\n",
    "        # Calculate mean and confidence intervals\n",
    "        metrics = {\n",
    "            \"mae\": (\n",
    "                np.mean(mae_bootstrap),\n",
    "                np.percentile(mae_bootstrap, (100 - ci_percentile) / 2),\n",
    "                np.percentile(mae_bootstrap, 100 - (100 - ci_percentile) / 2),\n",
    "            ),\n",
    "            \"mse\": (\n",
    "                np.mean(mse_bootstrap),\n",
    "                np.percentile(mse_bootstrap, (100 - ci_percentile) / 2),\n",
    "                np.percentile(mse_bootstrap, 100 - (100 - ci_percentile) / 2),\n",
    "            ),\n",
    "            \"rmse\": (\n",
    "                np.mean(rmse_bootstrap),\n",
    "                np.percentile(rmse_bootstrap, (100 - ci_percentile) / 2),\n",
    "                np.percentile(rmse_bootstrap, 100 - (100 - ci_percentile) / 2),\n",
    "            ),\n",
    "            \"smape\": (\n",
    "                np.mean(smape_bootstrap),\n",
    "                np.percentile(smape_bootstrap, (100 - ci_percentile) / 2),\n",
    "                np.percentile(smape_bootstrap, 100 - (100 - ci_percentile) / 2),\n",
    "            ),\n",
    "            \"mape\": (\n",
    "                np.mean(mape_bootstrap),\n",
    "                np.percentile(mape_bootstrap, (100 - ci_percentile) / 2),\n",
    "                np.percentile(mape_bootstrap, 100 - (100 - ci_percentile) / 2),\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        return metrics, merged_df\n",
    "\n",
    "    # Example usage\n",
    "    metrics, merged_data = bootstrap_timellm_metrics_length(\n",
    "        forecasts_df=predictions,\n",
    "        actual_values_df=actuals,\n",
    "        prediction_length=prediction_length,  # Use the desired prediction length\n",
    "        n_bootstrap=1000,     # Number of bootstrap samples\n",
    "        ci_percentile=95      # Confidence interval\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(merged_data.info())\n",
    "\n",
    "\n",
    "    # Create DataFrame for forecasting metrics\n",
    "    forecasting_metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "    # Save forecasting metrics\n",
    "    os.makedirs(output_dir_forecasting, exist_ok=True)\n",
    "    forecasting_csv_path = os.path.join(output_dir_forecasting, forecasting_filename)\n",
    "    forecasting_metrics_df.to_csv(forecasting_csv_path, index=False)\n",
    "    print(f\"Forecasting metrics saved to {forecasting_csv_path}\")\n",
    "    merged_data = merged_data.rename(columns={'y': 'actual', 'TimeLLM': 'predictions'})\n",
    "\n",
    "    #merged_data['actual'] = actuals.flatten()\n",
    "    #merged_data['predictions'] = predictions.flatten()\n",
    "    fever_flags_actual = merged_data.groupby(level=0)['actual'].transform(lambda x: (x < 8).any())\n",
    "    fever_flags_predicted = merged_data.groupby(level=0)['predictions'].transform(lambda x: (x < 8).any())\n",
    "    merged_data['actual_fever'] = fever_flags_actual\n",
    "    merged_data['predictions_fever'] = fever_flags_predicted\n",
    "    merged_data = merged_data.reset_index(level=0)\n",
    "    unique_encounters = merged_data.drop_duplicates(subset=['unique_id'])\n",
    "    print(unique_encounters.info())\n",
    "    # Confusion matrix computation\n",
    "    confusion_conditions = [\n",
    "        (unique_encounters['actual_fever'] == True) & (unique_encounters['predictions_fever'] == True),  # TP\n",
    "        (unique_encounters['actual_fever'] == True) & (unique_encounters['predictions_fever'] == False), # FN\n",
    "        (unique_encounters['actual_fever'] == False) & (unique_encounters['predictions_fever'] == True), # FP\n",
    "        (unique_encounters['actual_fever'] == False) & (unique_encounters['predictions_fever'] == False) # TN\n",
    "    ]\n",
    "    confusion_choices = ['TP', 'FN', 'FP', 'TN']\n",
    "    unique_encounters['confusion'] = np.select(confusion_conditions, confusion_choices)\n",
    "\n",
    "    confusion_matrix_bootstrap = {'TP': [], 'FN': [], 'FP': [], 'TN': []}\n",
    "    metrics_bootstrap = {'sensitivity': [], 'specificity': [], 'ppv': [], 'npv': [], 'accuracy': [], 'f1': [], 'mcc': []}\n",
    "    auc_scores = []  # For storing AUC scores\n",
    "\n",
    "    # Bootstrapping for confusion matrix, classification metrics, and AUC\n",
    "    for _ in range(n_bootstrap):\n",
    "        bootstrap_sample = unique_encounters.sample(frac=1, replace=True)\n",
    "        tp = ((bootstrap_sample['actual_fever'] == True) & (bootstrap_sample['predictions_fever'] == True)).sum()\n",
    "        fn = ((bootstrap_sample['actual_fever'] == True) & (bootstrap_sample['predictions_fever'] == False)).sum()\n",
    "        fp = ((bootstrap_sample['actual_fever'] == False) & (bootstrap_sample['predictions_fever'] == True)).sum()\n",
    "        tn = ((bootstrap_sample['actual_fever'] == False) & (bootstrap_sample['predictions_fever'] == False)).sum()\n",
    "\n",
    "        confusion_matrix_bootstrap['TP'].append(tp)\n",
    "        confusion_matrix_bootstrap['FN'].append(fn)\n",
    "        confusion_matrix_bootstrap['FP'].append(fp)\n",
    "        confusion_matrix_bootstrap['TN'].append(tn)\n",
    "\n",
    "        metrics_bootstrap['sensitivity'].append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['specificity'].append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        metrics_bootstrap['ppv'].append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "        metrics_bootstrap['npv'].append(tn / (tn + fn) if (tn + fn) > 0 else 0)\n",
    "        metrics_bootstrap['accuracy'].append((tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['f1'].append((2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['mcc'].append(((tp * tn) - (fp * fn)) / (\n",
    "            ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "        ) if ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) > 0 else 0)\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(bootstrap_sample['actual_fever'], bootstrap_sample['predictions'])\n",
    "        except ValueError:\n",
    "            auc = np.nan  # Handle case where AUC cannot be computed (only one class present)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    # Confidence intervals for confusion matrix\n",
    "    confusion_matrix_ci = {}\n",
    "    for key, values in confusion_matrix_bootstrap.items():\n",
    "        lower = np.percentile(values, 2.5)\n",
    "        upper = np.percentile(values, 97.5)\n",
    "        mean_value = np.mean(values)\n",
    "        confusion_matrix_ci[key] = {\"Mean\": mean_value, \"CI Lower\": lower, \"CI Upper\": upper}\n",
    "\n",
    "    # Confidence intervals for classification metrics\n",
    "    metrics_ci = {}\n",
    "    for metric, values in metrics_bootstrap.items():\n",
    "        lower = np.percentile(values, 2.5)\n",
    "        upper = np.percentile(values, 97.5)\n",
    "        mean_value = np.mean(values)\n",
    "        metrics_ci[metric] = {\"Mean\": mean_value, \"CI Lower\": lower, \"CI Upper\": upper}\n",
    "\n",
    "    # Confidence intervals for AUC\n",
    "    auc_mean = np.nanmean(auc_scores)\n",
    "    auc_lower = np.nanpercentile(auc_scores, 2.5)\n",
    "    auc_upper = np.nanpercentile(auc_scores, 97.5)\n",
    "\n",
    "    # Prepare data for classification metrics CSV\n",
    "    classification_metrics_data = []\n",
    "    for metric, stats in metrics_ci.items():\n",
    "        classification_metrics_data.append({\n",
    "            \"Metric\": metric.capitalize(),\n",
    "            \"Point Estimate\": stats[\"Mean\"],\n",
    "            \"95% CI Lower\": stats[\"CI Lower\"],\n",
    "            \"95% CI Upper\": stats[\"CI Upper\"]\n",
    "        })\n",
    "\n",
    "    # Add confusion matrix stats\n",
    "    for key, stats in confusion_matrix_ci.items():\n",
    "        classification_metrics_data.append({\n",
    "            \"Metric\": f\"Avg {key}\",\n",
    "            \"Point Estimate\": stats[\"Mean\"],\n",
    "            \"95% CI Lower\": stats[\"CI Lower\"],\n",
    "            \"95% CI Upper\": stats[\"CI Upper\"]\n",
    "        })\n",
    "\n",
    "    # Add AUC metric\n",
    "    classification_metrics_data.append({\n",
    "        \"Metric\": \"AUC\",\n",
    "        \"Point Estimate\": auc_mean,\n",
    "        \"95% CI Lower\": auc_lower,\n",
    "        \"95% CI Upper\": auc_upper\n",
    "    })\n",
    "\n",
    "    classification_metrics_df = pd.DataFrame(classification_metrics_data)\n",
    "\n",
    "    # Save classification metrics\n",
    "    os.makedirs(output_dir_classification, exist_ok=True)\n",
    "    classification_csv_path = os.path.join(output_dir_classification, classification_filename)\n",
    "    classification_metrics_df.to_csv(classification_csv_path, index=False)\n",
    "    print(f\"Classification metrics saved to {classification_csv_path}\")\n",
    "\n",
    "    # Plot ROC curve and save it\n",
    "    def plot_roc_curve(actuals, predictions, output_path):\n",
    "        fpr, tpr, _ = roc_curve(actuals, predictions)\n",
    "        auc = roc_auc_score(actuals, predictions)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        print(f\"ROC curve saved to {output_path}\")\n",
    "\n",
    "    # Save the ROC curve plot\n",
    "    roc_curve_path = os.path.join(output_dir_classification, 'roc_curve.png')\n",
    "    plot_roc_curve(unique_encounters['actual_fever'], unique_encounters['predictions'], roc_curve_path)\n",
    "\n",
    "    return predictions, actuals, merged_data,unique_encounters['actual_fever'],unique_encounters['predictions']\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "deecd4eed52e347fdb227bd6db7ebbadf0a7fc99b7440320ff3532cd8683bd78"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
