{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "from skopt import BayesSearchCV, space\n",
    "from sklearn.metrics import average_precision_score\n",
    "# use feature importance for feature selection\n",
    "from numpy import sort\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "### Parameters\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from autogluon.core.metrics import make_scorer\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from autogluon.common import space\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from sklearn.model_selection import KFold\n",
    "#import ts_fc_functions as ff\n",
    "import import_ipynb\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel, XGBModel, TiDEModel#, LightGBMModel\n",
    "from darts.models import NHiTSModel,LightGBMModel,RNNModel\n",
    "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
    "from darts.metrics import mape, r2_score, mae, rmse\n",
    "import torch\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_training_scale(data,value_to_predict,resample_rate,min_ts_length,imputation,max_train_length,prediction_length,padding,scale_series):\n",
    "\n",
    "    resampled_df = resample_ts(data,resample_rate,min_ts_length,imputation,max_train_length,value_to_predict)\n",
    "\n",
    "    resampled_df, scaler = scale_ts(resampled_df,value_to_predict)\n",
    "    valid_series_dict_full, test_series_dict_full, train_series_dict_full = ts_dictionary(resampled_df,prediction_length,value_to_predict,resample_rate)\n",
    "\n",
    "        # Pad the series\n",
    "    if padding == True:\n",
    "        train_series_dict_full = pad_series_dict(train_series_dict_full, max_train_length, pad_value=0)\n",
    "        valid_series_dict_full = pad_series_dict(valid_series_dict_full, max_train_length, pad_value=0)   \n",
    "\n",
    "    return train_series_dict_full,valid_series_dict_full,resampled_df, scaler\n",
    "\n",
    "def data_processing_training(data,value_to_predict,resample_rate,min_ts_length,imputation,max_train_length,prediction_length,padding):\n",
    "\n",
    "    resampled_df = resample_ts(data,resample_rate,min_ts_length,imputation,max_train_length,value_to_predict)\n",
    "    valid_series_dict_full, test_series_dict_full, train_series_dict_full = ts_dictionary(resampled_df,prediction_length,value_to_predict,resample_rate)\n",
    "    if padding == True:\n",
    "        train_series_dict_full = pad_series_dict(train_series_dict_full, max_train_length, pad_value=0)\n",
    "        valid_series_dict_full = pad_series_dict(valid_series_dict_full, max_train_length, pad_value=0)   \n",
    "\n",
    "    return train_series_dict_full,valid_series_dict_full,resampled_df\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_use_case_3_data(data_use_case_3, start_date, end_date):\n",
    "    df_sorted = data_use_case_3.sort_values(by='performedDateTime').reset_index(drop=True)\n",
    "    df_no_duplicates = df_sorted.drop_duplicates(subset=['performedDateTime', 'subject_reference', 'encounter_reference']).reset_index(drop=True)\n",
    "\n",
    "    # Generate a date range with unique dates\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "    # Duplicate each date to have two entries per day\n",
    "    date_range = date_range.repeat(2)\n",
    "    df_no_duplicates[\"date\"] = pd.to_datetime(df_no_duplicates[\"date\"])\n",
    "\n",
    "\n",
    "    ctscans = pd.DataFrame(date_range, columns=[\"date\"])\n",
    "    ctscans[\"time_of_day\"] = ctscans.groupby(\"date\").cumcount().map({0: \"a_morning\", 1: \"b_evening\"})\n",
    "\n",
    "    count_df = df_no_duplicates.groupby([\"date\", \"time_of_day\"]).size().reset_index(name=\"count_scans\")\n",
    "    ctscans = pd.merge(ctscans, count_df, on=[\"date\", \"time_of_day\"], how=\"left\")\n",
    "    ctscans[\"count_scans\"] = ctscans[\"count_scans\"].fillna(0).astype(int)\n",
    "    ctscans = ctscans.drop(index=0)\n",
    "\n",
    "    # Reset the index if desired, to avoid a missing index number\n",
    "    ctscans = ctscans.reset_index(drop=True)\n",
    "\n",
    "    # Create a new column 'shift_count' initialized to NaN\n",
    "    ctscans[\"shift_count\"] = np.nan\n",
    "\n",
    "    # Iterate over the rows and sum count_scans for each consecutive \"b_evening\" and \"a_morning\" pair\n",
    "    for i in range(len(ctscans) - 1):\n",
    "        if ctscans.loc[i, \"time_of_day\"] == \"b_evening\" and ctscans.loc[i + 1, \"time_of_day\"] == \"a_morning\":\n",
    "            # Sum count_scans for the consecutive \"b_evening\" and \"a_morning\"\n",
    "            shift_sum = ctscans.loc[i, \"count_scans\"] + ctscans.loc[i + 1, \"count_scans\"]\n",
    "            \n",
    "            # Assign the result to the first row of the pair (\"b_evening\" row)\n",
    "            ctscans.loc[i, \"shift_count\"] = shift_sum\n",
    "\n",
    "    # Optionally, forward-fill or set NaN values to 0 in 'shift_count'\n",
    "    ctscans[\"shift_count\"] = ctscans[\"shift_count\"].fillna(0).astype(int)\n",
    "    ctscans = ctscans[ctscans[\"time_of_day\"] != \"a_morning\"].reset_index(drop=True)\n",
    "    ctscans[\"day_of_week\"] = ctscans[\"date\"].dt.day_name()\n",
    "    last_friday = ctscans[ctscans[\"day_of_week\"] == \"Friday\"][\"date\"].max()\n",
    "\n",
    "    # Step 2: Initialize ts_id counter and end_date for the first 4-week period\n",
    "    current_id = 1\n",
    "    end_date = last_friday\n",
    "\n",
    "    # Step 3: Create an empty 'ts_id' column\n",
    "    ctscans[\"ts_id\"] = 0\n",
    "\n",
    "    # Step 4: Assign ts_id to each 4-week (28-day) period, going backwards from the last Friday\n",
    "    while end_date >= ctscans[\"date\"].min():\n",
    "        start_date = end_date - pd.Timedelta(weeks=4)\n",
    "\n",
    "        # Assign the current ts_id to dates within the 4-week range\n",
    "        ctscans.loc[(ctscans[\"date\"] > start_date) & (ctscans[\"date\"] <= end_date), \"ts_id\"] = current_id\n",
    "\n",
    "        # Move to the next 4-week period back in time\n",
    "        end_date = start_date\n",
    "        current_id += 1\n",
    "    ctscans\n",
    "    return ctscans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def bootstrap_metrics_roc(\n",
    "    actual_values,\n",
    "    predicted_values,\n",
    "    group_ids,\n",
    "    n_bootstrap=1000,\n",
    "    ci_percentile=95,\n",
    "    threshold_value=8,\n",
    "    comparison_operator=\"<\"\n",
    "):\n",
    "\n",
    "\n",
    "    # Helper: comparison logic\n",
    "    def apply_threshold(x, operator, threshold):\n",
    "        if operator == \"<\":\n",
    "            return x < threshold\n",
    "        elif operator == \"<=\":\n",
    "            return x <= threshold\n",
    "        elif operator == \">\":\n",
    "            return x > threshold\n",
    "        elif operator == \">=\":\n",
    "            return x >= threshold\n",
    "        elif operator == \"==\":\n",
    "            return x == threshold\n",
    "        else:\n",
    "            raise ValueError(\"Invalid comparison_operator\")\n",
    "\n",
    "    # Combine data\n",
    "    data = pd.DataFrame({\n",
    "        \"actual\": actual_values,\n",
    "        \"predicted\": predicted_values,\n",
    "        \"group_id\": group_ids\n",
    "    })\n",
    "\n",
    "    # Aggregate at group level\n",
    "    grouped = data.groupby(\"group_id\").agg(\n",
    "        actual_condition=(\"actual\", lambda x: apply_threshold(x, comparison_operator, threshold_value).any()),\n",
    "        predicted_condition=(\"predicted\", lambda x: apply_threshold(x, comparison_operator, threshold_value).any()),\n",
    "        predicted_mean=(\"predicted\", \"mean\")\n",
    "    )\n",
    "\n",
    "    # Exact confusion matrix\n",
    "    tp_exact = ((grouped[\"actual_condition\"]) & (grouped[\"predicted_condition\"])).sum()\n",
    "    fn_exact = ((grouped[\"actual_condition\"]) & (~grouped[\"predicted_condition\"])).sum()\n",
    "    fp_exact = ((~grouped[\"actual_condition\"]) & (grouped[\"predicted_condition\"])).sum()\n",
    "    tn_exact = ((~grouped[\"actual_condition\"]) & (~grouped[\"predicted_condition\"])).sum()\n",
    "\n",
    "    # Exact classification metrics\n",
    "    sensitivity_exact = tp_exact / (tp_exact + fn_exact) if (tp_exact + fn_exact) > 0 else 0\n",
    "    specificity_exact = tn_exact / (tn_exact + fp_exact) if (tn_exact + fp_exact) > 0 else 0\n",
    "    ppv_exact = tp_exact / (tp_exact + fp_exact) if (tp_exact + fp_exact) > 0 else 0\n",
    "    npv_exact = tn_exact / (tn_exact + fn_exact) if (tn_exact + fn_exact) > 0 else 0\n",
    "    accuracy_exact = (tp_exact + tn_exact) / (tp_exact + fn_exact + fp_exact + tn_exact) if (tp_exact + fn_exact + fp_exact + tn_exact) > 0 else 0\n",
    "    f1_exact = (2 * tp_exact) / (2 * tp_exact + fp_exact + fn_exact) if (2 * tp_exact + fp_exact + fn_exact) > 0 else 0\n",
    "    mcc_exact = ((tp_exact * tn_exact) - (fp_exact * fn_exact)) / (\n",
    "        np.sqrt((tp_exact + fp_exact) * (tp_exact + fn_exact) * (tn_exact + fp_exact) * (tn_exact + fn_exact))\n",
    "        if (tp_exact + fp_exact) > 0 and (tp_exact + fn_exact) > 0 and (tn_exact + fp_exact) > 0 and (tn_exact + fn_exact) > 0\n",
    "        else 1\n",
    "    )\n",
    "\n",
    "    # Regression metrics \n",
    "    mae_exact = np.mean(np.abs(actual_values - predicted_values))\n",
    "    mse_exact = np.mean((actual_values - predicted_values) ** 2)\n",
    "    rmse_exact = np.sqrt(mse_exact)\n",
    "    mape_exact = np.mean(np.abs((actual_values - predicted_values) / np.where(actual_values == 0, np.nan, actual_values))) * 100\n",
    "    smape_exact = np.mean(2 * np.abs(actual_values - predicted_values) / (np.abs(actual_values) + np.abs(predicted_values))) * 100\n",
    "\n",
    "    #  ROC and AUC (continuous predicted_mean) \n",
    "    try:\n",
    "        auc_exact = roc_auc_score(grouped[\"actual_condition\"], grouped[\"predicted_mean\"])\n",
    "        fpr, tpr, _ = roc_curve(grouped[\"actual_condition\"], grouped[\"predicted_mean\"])\n",
    "    except ValueError:\n",
    "        auc_exact = np.nan\n",
    "        fpr, tpr = [0, 1], [0, 1]\n",
    "\n",
    "    #  Plot ROC \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_exact:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    roc_curve_path = \"roc_curve.png\"\n",
    "    plt.savefig(roc_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Bootstrap containers \n",
    "    auc_bootstrap = []\n",
    "    tp_b, fn_b, fp_b, tn_b = [], [], [], []\n",
    "    sens_b, spec_b, ppv_b, npv_b, acc_b, f1_b, mcc_b = [], [], [], [], [], [], []\n",
    "    mae_b, mse_b, rmse_b, mape_b, smape_b = [], [], [], [], []\n",
    "\n",
    "    #  Bootstrapping \n",
    "    for _ in range(n_bootstrap):\n",
    "        sampled_ids = np.random.choice(grouped.index, size=len(grouped), replace=True)\n",
    "        resampled_data = data[data[\"group_id\"].isin(sampled_ids)]\n",
    "        resampled_grouped = resampled_data.groupby(\"group_id\").agg(\n",
    "            actual_condition=(\"actual\", lambda x: apply_threshold(x, comparison_operator, threshold_value).any()),\n",
    "            predicted_condition=(\"predicted\", lambda x: apply_threshold(x, comparison_operator, threshold_value).any()),\n",
    "            predicted_mean=(\"predicted\", \"mean\")\n",
    "        )\n",
    "\n",
    "        # Confusion matrix\n",
    "        tp = ((resampled_grouped[\"actual_condition\"]) & (resampled_grouped[\"predicted_condition\"])).sum()\n",
    "        fn = ((resampled_grouped[\"actual_condition\"]) & (~resampled_grouped[\"predicted_condition\"])).sum()\n",
    "        fp = ((~resampled_grouped[\"actual_condition\"]) & (resampled_grouped[\"predicted_condition\"])).sum()\n",
    "        tn = ((~resampled_grouped[\"actual_condition\"]) & (~resampled_grouped[\"predicted_condition\"])).sum()\n",
    "\n",
    "        tp_b.append(tp); fn_b.append(fn); fp_b.append(fp); tn_b.append(tn)\n",
    "\n",
    "        # Metrics\n",
    "        sens_b.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        spec_b.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        ppv_b.append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "        npv_b.append(tn / (tn + fn) if (tn + fn) > 0 else 0)\n",
    "        acc_b.append((tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0)\n",
    "        f1_b.append((2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0)\n",
    "        mcc_b.append(((tp * tn) - (fp * fn)) / (\n",
    "            np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "            if (tp + fp) > 0 and (tp + fn) > 0 and (tn + fp) > 0 and (tn + fn) > 0\n",
    "            else 1\n",
    "        ))\n",
    "\n",
    "        try:\n",
    "            auc_b = roc_auc_score(resampled_grouped[\"actual_condition\"], resampled_grouped[\"predicted_mean\"])\n",
    "        except ValueError:\n",
    "            auc_b = np.nan\n",
    "        auc_bootstrap.append(auc_b)\n",
    "\n",
    "        # Regression metrics\n",
    "        a_r, p_r = resampled_data[\"actual\"].values, resampled_data[\"predicted\"].values\n",
    "        mae_b.append(np.mean(np.abs(a_r - p_r)))\n",
    "        mse_b.append(np.mean((a_r - p_r) ** 2))\n",
    "        rmse_b.append(np.sqrt(mse_b[-1]))\n",
    "        mape_b.append(np.mean(np.abs((a_r - p_r) / np.where(a_r == 0, np.nan, a_r))) * 100)\n",
    "        smape_b.append(np.mean(2 * np.abs(a_r - p_r) / (np.abs(a_r) + np.abs(p_r))) * 100)\n",
    "\n",
    "    # --- Helper: CI ---\n",
    "    def ci(v):\n",
    "        return np.nanpercentile(v, (100 - ci_percentile) / 2), np.nanpercentile(v, 100 - (100 - ci_percentile) / 2)\n",
    "\n",
    "    # Classification metrics DF\n",
    "    classification_metrics = pd.DataFrame({\n",
    "        \"Metric\": [\"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\", \"Accuracy\", \"F1\", \"MCC\", \"AUC\", \"TP\", \"FN\", \"FP\", \"TN\"],\n",
    "        \"Point Estimate\": [sensitivity_exact, specificity_exact, ppv_exact, npv_exact, accuracy_exact, f1_exact, mcc_exact, auc_exact, tp_exact, fn_exact, fp_exact, tn_exact],\n",
    "        \"CI Lower\": [ci(sens_b)[0], ci(spec_b)[0], ci(ppv_b)[0], ci(npv_b)[0], ci(acc_b)[0], ci(f1_b)[0], ci(mcc_b)[0], ci(auc_bootstrap)[0], ci(tp_b)[0], ci(fn_b)[0], ci(fp_b)[0], ci(tn_b)[0]],\n",
    "        \"CI Upper\": [ci(sens_b)[1], ci(spec_b)[1], ci(ppv_b)[1], ci(npv_b)[1], ci(acc_b)[1], ci(f1_b)[1], ci(mcc_b)[1], ci(auc_bootstrap)[1], ci(tp_b)[1], ci(fn_b)[1], ci(fp_b)[1], ci(tn_b)[1]],\n",
    "    })\n",
    "\n",
    "    regression_metrics = pd.DataFrame({\n",
    "        \"Metric\": [\"MAE\", \"MSE\", \"RMSE\", \"MAPE\", \"SMAPE\"],\n",
    "        \"Point Estimate\": [mae_exact, mse_exact, rmse_exact, mape_exact, smape_exact],\n",
    "        \"CI Lower\": [ci(mae_b)[0], ci(mse_b)[0], ci(rmse_b)[0], ci(mape_b)[0], ci(smape_b)[0]],\n",
    "        \"CI Upper\": [ci(mae_b)[1], ci(mse_b)[1], ci(rmse_b)[1], ci(mape_b)[1], ci(smape_b)[1]],\n",
    "    })\n",
    "\n",
    "    return classification_metrics, regression_metrics, roc_curve_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timesfm_data_ci_roc_(\n",
    "    model_name,\n",
    "    output_dir_forecasting,\n",
    "    output_dir_classification,\n",
    "    forecasting_filename,\n",
    "    classification_filename,\n",
    "    resampled_df,\n",
    "    tfm,\n",
    "    scaler,\n",
    "    prediction_length,\n",
    "    value_to_predict,\n",
    "    threshold_value=8,\n",
    "    comparison_operator='<',\n",
    "    n_bootstrap=1000\n",
    "):\n",
    "\n",
    "    # Split into train/test\n",
    "    full_data_test = resampled_df.groupby(\"encounter_id\").tail(prediction_length)\n",
    "    full_data_train = resampled_df.drop(full_data_test.index)\n",
    "    full_data_train_ = full_data_train.rename(columns={'recorded_time': 'ds', 'encounter_id': 'unique_id','value':'y'})\n",
    "    full_data_test_ = full_data_test.rename(columns={'recorded_time': 'ds', 'encounter_id': 'unique_id','value':'y'})\n",
    "\n",
    "    # Forecast\n",
    "    forecasts = tfm.forecast_on_df(\n",
    "        inputs=full_data_train_,\n",
    "        freq=\"D\",\n",
    "        value_name=\"y\",\n",
    "        num_jobs=3,\n",
    "    )\n",
    "\n",
    "    # Helper: inverse transform\n",
    "    def rescale_to_original(scaled_df, scaler, columns_to_rescale):\n",
    "        scaled_df = scaled_df.copy()\n",
    "        scaled_df[columns_to_rescale] = scaler.inverse_transform(scaled_df[columns_to_rescale])\n",
    "        return scaled_df\n",
    "\n",
    "    columns_to_rescale_actual = [\"y\"]\n",
    "    columns_to_rescale_predict = [\"timesfm\"]\n",
    "    actuals = rescale_to_original(full_data_test_, scaler, columns_to_rescale_actual)\n",
    "    predictions = rescale_to_original(forecasts, scaler, columns_to_rescale_predict)\n",
    "\n",
    "    # Bootstrap metrics for forecasting\n",
    "    def bootstrap_timellm_metrics_length(forecasts_df, actual_values_df, prediction_length, n_bootstrap, ci_percentile=95):\n",
    "        mae_bootstrap, mse_bootstrap, rmse_bootstrap, smape_bootstrap, mape_bootstrap = [], [], [], [], []\n",
    "\n",
    "        merged_df = pd.merge(forecasts_df.reset_index(), actual_values_df, on=['unique_id', 'ds'])\n",
    "        unique_ids = merged_df['unique_id'].unique()\n",
    "\n",
    "        for _ in range(n_bootstrap):\n",
    "            sampled_ids = np.random.choice(unique_ids, size=len(unique_ids), replace=True)\n",
    "            sampled_data = merged_df[merged_df['unique_id'].isin(sampled_ids)]\n",
    "            if len(sampled_ids) == 0:\n",
    "                continue\n",
    "\n",
    "            total_mae = total_mse = total_smape = total_mape = 0\n",
    "\n",
    "            for unique_id in sampled_ids:\n",
    "                data = sampled_data[sampled_data['unique_id'] == unique_id].sort_values(by='ds')\n",
    "                if data.empty:\n",
    "                    continue\n",
    "\n",
    "                forecast_values = data['timesfm'].values[-prediction_length:]\n",
    "                true_values = data['y'].values[-prediction_length:]\n",
    "                if len(forecast_values) == 0 or len(true_values) == 0:\n",
    "                    continue\n",
    "\n",
    "                abs_err = np.abs(forecast_values - true_values)\n",
    "                mae = abs_err.mean()\n",
    "                mse = (abs_err ** 2).mean()\n",
    "                denominator = (np.abs(true_values) + np.abs(forecast_values)) / 2\n",
    "                smape = (abs_err / denominator).mean() * 100\n",
    "                mape = np.mean(np.abs((true_values - forecast_values) / np.where(true_values == 0, np.nan, true_values))) * 100\n",
    "\n",
    "                total_mae += mae\n",
    "                total_mse += mse\n",
    "                total_smape += smape\n",
    "                total_mape += mape\n",
    "\n",
    "            mae_bootstrap.append(total_mae / len(sampled_ids))\n",
    "            mse_bootstrap.append(total_mse / len(sampled_ids))\n",
    "            rmse_bootstrap.append(np.sqrt(total_mse / len(sampled_ids)))\n",
    "            smape_bootstrap.append(total_smape / len(sampled_ids))\n",
    "            mape_bootstrap.append(total_mape / len(sampled_ids))\n",
    "\n",
    "        metrics = {\n",
    "            \"mae\": (np.mean(mae_bootstrap), np.percentile(mae_bootstrap, 2.5), np.percentile(mae_bootstrap, 97.5)),\n",
    "            \"mse\": (np.mean(mse_bootstrap), np.percentile(mse_bootstrap, 2.5), np.percentile(mse_bootstrap, 97.5)),\n",
    "            \"rmse\": (np.mean(rmse_bootstrap), np.percentile(rmse_bootstrap, 2.5), np.percentile(rmse_bootstrap, 97.5)),\n",
    "            \"smape\": (np.mean(smape_bootstrap), np.percentile(smape_bootstrap, 2.5), np.percentile(smape_bootstrap, 97.5)),\n",
    "            \"mape\": (np.mean(mape_bootstrap), np.percentile(mape_bootstrap, 2.5), np.percentile(mape_bootstrap, 97.5)),\n",
    "        }\n",
    "        return metrics, merged_df\n",
    "\n",
    "    metrics, merged_data = bootstrap_timellm_metrics_length(predictions, actuals, prediction_length, n_bootstrap, 95)\n",
    "\n",
    "    # Save forecasting metrics\n",
    "    forecasting_metrics_df = pd.DataFrame(metrics)\n",
    "    os.makedirs(output_dir_forecasting, exist_ok=True)\n",
    "    forecasting_csv_path = os.path.join(output_dir_forecasting, forecasting_filename)\n",
    "    forecasting_metrics_df.to_csv(forecasting_csv_path, index=False)\n",
    "    print(f\"Forecasting metrics saved to {forecasting_csv_path}\")\n",
    "\n",
    "    merged_data = merged_data.rename(columns={'y': 'actual', 'timesfm': 'predictions'})\n",
    "\n",
    "    # Apply condition threshold \n",
    "    def apply_threshold(x, operator, threshold):\n",
    "        if operator == '<':\n",
    "            return x < threshold\n",
    "        elif operator == '<=':\n",
    "            return x <= threshold\n",
    "        elif operator == '>':\n",
    "            return x > threshold\n",
    "        elif operator == '>=':\n",
    "            return x >= threshold\n",
    "        elif operator == '==':\n",
    "            return x == threshold\n",
    "        else:\n",
    "            raise ValueError(\"Invalid comparison_operator. Choose from '<', '<=', '>', '>=', '=='.\")\n",
    "\n",
    "    merged_data['actual_condition'] = merged_data.groupby('unique_id')['actual'].transform(lambda x: apply_threshold(x, comparison_operator, threshold_value).any())\n",
    "    merged_data['predicted_condition'] = merged_data.groupby('unique_id')['predictions'].transform(lambda x: apply_threshold(x, comparison_operator, threshold_value).any())\n",
    "\n",
    "    merged_data = merged_data.reset_index(drop=True)\n",
    "    unique_encounters = merged_data.drop_duplicates(subset=['unique_id'])\n",
    "\n",
    "    # Confusion matrix\n",
    "    confusion_conditions = [\n",
    "        (unique_encounters['actual_condition'] == True) & (unique_encounters['predicted_condition'] == True),\n",
    "        (unique_encounters['actual_condition'] == True) & (unique_encounters['predicted_condition'] == False),\n",
    "        (unique_encounters['actual_condition'] == False) & (unique_encounters['predicted_condition'] == True),\n",
    "        (unique_encounters['actual_condition'] == False) & (unique_encounters['predicted_condition'] == False),\n",
    "    ]\n",
    "    confusion_choices = ['TP', 'FN', 'FP', 'TN']\n",
    "    unique_encounters['confusion'] = np.select(confusion_conditions, confusion_choices, default='Unknown')\n",
    "\n",
    "    # Bootstrap classification metrics\n",
    "    confusion_matrix_bootstrap = {'TP': [], 'FN': [], 'FP': [], 'TN': []}\n",
    "    metrics_bootstrap = {'sensitivity': [], 'specificity': [], 'ppv': [], 'npv': [], 'accuracy': [], 'f1': [], 'mcc': []}\n",
    "    auc_scores = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        bootstrap_sample = unique_encounters.sample(frac=1, replace=True)\n",
    "        tp = ((bootstrap_sample['actual_condition']) & (bootstrap_sample['predicted_condition'])).sum()\n",
    "        fn = ((bootstrap_sample['actual_condition']) & (~bootstrap_sample['predicted_condition'])).sum()\n",
    "        fp = ((~bootstrap_sample['actual_condition']) & (bootstrap_sample['predicted_condition'])).sum()\n",
    "        tn = ((~bootstrap_sample['actual_condition']) & (~bootstrap_sample['predicted_condition'])).sum()\n",
    "\n",
    "        confusion_matrix_bootstrap['TP'].append(tp)\n",
    "        confusion_matrix_bootstrap['FN'].append(fn)\n",
    "        confusion_matrix_bootstrap['FP'].append(fp)\n",
    "        confusion_matrix_bootstrap['TN'].append(tn)\n",
    "\n",
    "        metrics_bootstrap['sensitivity'].append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['specificity'].append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        metrics_bootstrap['ppv'].append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "        metrics_bootstrap['npv'].append(tn / (tn + fn) if (tn + fn) > 0 else 0)\n",
    "        metrics_bootstrap['accuracy'].append((tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['f1'].append((2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['mcc'].append(((tp * tn) - (fp * fn)) / (\n",
    "            ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "        ) if ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) > 0 else 0)\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(bootstrap_sample['actual_condition'], bootstrap_sample['predictions'])\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    # Compute CI\n",
    "    def ci_dict(values):\n",
    "        return {\n",
    "            \"Mean\": np.nanmean(values),\n",
    "            \"CI Lower\": np.nanpercentile(values, 2.5),\n",
    "            \"CI Upper\": np.nanpercentile(values, 97.5),\n",
    "        }\n",
    "\n",
    "    confusion_matrix_ci = {k: ci_dict(v) for k, v in confusion_matrix_bootstrap.items()}\n",
    "    metrics_ci = {k: ci_dict(v) for k, v in metrics_bootstrap.items()}\n",
    "    auc_ci = ci_dict(auc_scores)\n",
    "\n",
    "    # Prepare classification metrics\n",
    "    classification_metrics_data = [\n",
    "        {\"Metric\": metric.capitalize(), \"Point Estimate\": stats[\"Mean\"], \"95% CI Lower\": stats[\"CI Lower\"], \"95% CI Upper\": stats[\"CI Upper\"]}\n",
    "        for metric, stats in metrics_ci.items()\n",
    "    ]\n",
    "    for key, stats in confusion_matrix_ci.items():\n",
    "        classification_metrics_data.append({\n",
    "            \"Metric\": f\"Avg {key}\",\n",
    "            \"Point Estimate\": stats[\"Mean\"],\n",
    "            \"95% CI Lower\": stats[\"CI Lower\"],\n",
    "            \"95% CI Upper\": stats[\"CI Upper\"]\n",
    "        })\n",
    "    classification_metrics_data.append({\n",
    "        \"Metric\": \"AUC\",\n",
    "        \"Point Estimate\": auc_ci[\"Mean\"],\n",
    "        \"95% CI Lower\": auc_ci[\"CI Lower\"],\n",
    "        \"95% CI Upper\": auc_ci[\"CI Upper\"],\n",
    "    })\n",
    "\n",
    "    classification_metrics_df = pd.DataFrame(classification_metrics_data)\n",
    "    os.makedirs(output_dir_classification, exist_ok=True)\n",
    "    classification_csv_path = os.path.join(output_dir_classification, classification_filename)\n",
    "    classification_metrics_df.to_csv(classification_csv_path, index=False)\n",
    "    print(f\"Classification metrics saved to {classification_csv_path}\")\n",
    "\n",
    "    # Plot ROC\n",
    "    def plot_roc_curve(actuals, predictions, output_path):\n",
    "        if len(np.unique(actuals)) < 2:\n",
    "            print(\"ROC curve not plotted: only one class present.\")\n",
    "            return\n",
    "        fpr, tpr, _ = roc_curve(actuals, predictions)\n",
    "        auc = roc_auc_score(actuals, predictions)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        print(f\"ROC curve saved to {output_path}\")\n",
    "\n",
    "    roc_curve_path = os.path.join(output_dir_classification, 'roc_curve.png')\n",
    "    plot_roc_curve(unique_encounters['actual_condition'], unique_encounters['predictions'], roc_curve_path)\n",
    "\n",
    "    return predictions, actuals, merged_data, unique_encounters['actual_condition'], unique_encounters['predictions']\n",
    "\n",
    "def predict_proba_ci_roc_general(\n",
    "    output_dir_classification,\n",
    "    classification_filename,\n",
    "    output_dir_forecasting,\n",
    "    forecasting_filename,\n",
    "    model,\n",
    "    train_series_dict,\n",
    "    valid_series_dict,\n",
    "    prediction_length,\n",
    "    scaler,\n",
    "    n_bootstrap=1000,\n",
    "    threshold_value=8,\n",
    "    comparison_operator=\"<\"\n",
    "):\n",
    "\n",
    "\n",
    "    # Generate predictions\n",
    "    pred = model.predict(n=prediction_length, series=list(train_series_dict.values()), num_samples=500)\n",
    "\n",
    "    # Collect actuals and predictions \n",
    "    actual_values, predicted_values, keys = [], [], []\n",
    "\n",
    "    for key, ts in valid_series_dict.items():\n",
    "        actual = ts.values()[-prediction_length:].flatten()\n",
    "        predicted = np.median(pred[key].all_values(), axis=1)\n",
    "        actual_values.append(actual)\n",
    "        predicted_values.append(predicted)\n",
    "        keys.append(key)\n",
    "\n",
    "    #  Convert and rescale \n",
    "    actual_values_np = scaler.inverse_transform(np.array(actual_values))\n",
    "    predicted_values_np = scaler.inverse_transform(np.array(predicted_values))\n",
    "\n",
    "    #  Build full long-form data \n",
    "    all_records = []\n",
    "    for i, key in enumerate(keys):\n",
    "        for t in range(prediction_length):\n",
    "            all_records.append({\n",
    "                \"group_id\": key,\n",
    "                \"actual\": actual_values_np[i, t],\n",
    "                \"predicted\": predicted_values_np[i, t]\n",
    "            })\n",
    "    df_all = pd.DataFrame(all_records)\n",
    "\n",
    "    #  Compute metrics \n",
    "    classification_metrics, regression_metrics, roc_path = bootstrap_metrics_roc(\n",
    "        df_all[\"actual\"].values,\n",
    "        df_all[\"predicted\"].values,\n",
    "        df_all[\"group_id\"].values,\n",
    "        n_bootstrap=n_bootstrap,\n",
    "        threshold_value=threshold_value,\n",
    "        comparison_operator=comparison_operator\n",
    "    )\n",
    "\n",
    "    #  Save outputs \n",
    "    os.makedirs(output_dir_classification, exist_ok=True)\n",
    "    classification_metrics.to_csv(os.path.join(output_dir_classification, classification_filename), index=False)\n",
    "\n",
    "    os.makedirs(output_dir_forecasting, exist_ok=True)\n",
    "    regression_metrics.to_csv(os.path.join(output_dir_forecasting, forecasting_filename), index=False)\n",
    "\n",
    "    print(f\"Classification metrics saved to {classification_filename}\")\n",
    "    print(f\"Forecasting metrics saved to {forecasting_filename}\")\n",
    "    print(f\"ROC curve saved to {roc_path}\")\n",
    "\n",
    "    return predicted_values, actual_values, keys, df_all, classification_metrics, regression_metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_use_case_3_data_with_scaling(data_use_case_3,start_date,end_date):\n",
    "    df_sorted = data_use_case_3.sort_values(by='performedDateTime').reset_index(drop=True)\n",
    "    df_no_duplicates = df_sorted.drop_duplicates(subset=['performedDateTime', 'subject_reference', 'encounter_reference']).reset_index(drop=True)\n",
    "\n",
    "    # Generate a date range with unique dates\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "    # Duplicate each date to have two entries per day\n",
    "    date_range = date_range.repeat(2)\n",
    "    df_no_duplicates[\"date\"] = pd.to_datetime(df_no_duplicates[\"date\"])\n",
    "\n",
    "    ctscans = pd.DataFrame(date_range, columns=[\"date\"])\n",
    "    ctscans[\"time_of_day\"] = ctscans.groupby(\"date\").cumcount().map({0: \"a_morning\", 1: \"b_evening\"})\n",
    "\n",
    "    count_df = df_no_duplicates.groupby([\"date\", \"time_of_day\"]).size().reset_index(name=\"count_scans\")\n",
    "    ctscans = pd.merge(ctscans, count_df, on=[\"date\", \"time_of_day\"], how=\"left\")\n",
    "    ctscans[\"count_scans\"] = ctscans[\"count_scans\"].fillna(0).astype(int)\n",
    "    ctscans = ctscans.drop(index=0)\n",
    "\n",
    "    # Reset the index if desired, to avoid a missing index number\n",
    "    ctscans = ctscans.reset_index(drop=True)\n",
    "\n",
    "    # Create a new column 'shift_count' initialized to NaN\n",
    "    ctscans[\"shift_count\"] = np.nan\n",
    "\n",
    "    # Iterate over the rows and sum count_scans for each consecutive \"b_evening\" and \"a_morning\" pair\n",
    "    for i in range(len(ctscans) - 1):\n",
    "        if ctscans.loc[i, \"time_of_day\"] == \"b_evening\" and ctscans.loc[i + 1, \"time_of_day\"] == \"a_morning\":\n",
    "            # Sum count_scans for the consecutive \"b_evening\" and \"a_morning\"\n",
    "            shift_sum = ctscans.loc[i, \"count_scans\"] + ctscans.loc[i + 1, \"count_scans\"]\n",
    "            \n",
    "            # Assign the result to the first row of the pair (\"b_evening\" row)\n",
    "            ctscans.loc[i, \"shift_count\"] = shift_sum\n",
    "\n",
    "    # Optionally, forward-fill or set NaN values to 0 in 'shift_count'\n",
    "    ctscans[\"shift_count\"] = ctscans[\"shift_count\"].fillna(0).astype(int)\n",
    "    ctscans = ctscans[ctscans[\"time_of_day\"] != \"a_morning\"].reset_index(drop=True)\n",
    "    ctscans[\"day_of_week\"] = ctscans[\"date\"].dt.day_name()\n",
    "    last_friday = ctscans[ctscans[\"day_of_week\"] == \"Friday\"][\"date\"].max()\n",
    "\n",
    "    # Step 2: Initialize ts_id counter and end_date for the first 4-week period\n",
    "    current_id = 1\n",
    "    end_date = last_friday\n",
    "\n",
    "    # Step 3: Create an empty 'ts_id' column\n",
    "    ctscans[\"ts_id\"] = 0\n",
    "\n",
    "    # Step 4: Assign ts_id to each 4-week (28-day) period, going backwards from the last Friday\n",
    "    while end_date >= ctscans[\"date\"].min():\n",
    "        start_date = end_date - pd.Timedelta(weeks=4)\n",
    "\n",
    "        # Assign the current ts_id to dates within the 4-week range\n",
    "        ctscans.loc[(ctscans[\"date\"] > start_date) & (ctscans[\"date\"] <= end_date), \"ts_id\"] = current_id\n",
    "\n",
    "        # Move to the next 4-week period back in time\n",
    "        end_date = start_date\n",
    "        current_id += 1\n",
    "\n",
    "    # Min-Max scaling for 'count_scans' and 'shift_count'\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_columns = [\"shift_count\"]\n",
    "\n",
    "    # Apply Min-Max Scaling\n",
    "    ctscans[scaled_columns] = scaler.fit_transform(ctscans[scaled_columns])\n",
    "\n",
    "    return ctscans, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_windows_with_weekday_prediction(data, target_col, lookback, horizon, prediction_days):\n",
    "    \"\"\"\n",
    "    Creates rolling windows for a time series while preserving dates for each window.\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame containing the time series.\n",
    "        target_col (str): Column name of the target variable.\n",
    "        lookback (int): Number of steps to look back.\n",
    "        horizon (int): Number of steps to forecast.\n",
    "        prediction_days (list): List of valid day names for the prediction window (e.g., ['Monday', ..., 'Friday']).\n",
    "    Returns:\n",
    "        X (list): List of dictionaries containing input sequences and their corresponding dates.\n",
    "        y (list): List of dictionaries containing target sequences and their corresponding dates.\n",
    "        dates (list): Start dates of each rolling window.\n",
    "    \"\"\"\n",
    "    X, y, dates = [], [], []\n",
    "\n",
    "    for i in range(len(data) - lookback - horizon + 1):\n",
    "        # Define the input (lookback) window\n",
    "        window = data.iloc[i:i + lookback]\n",
    "        input_values = window[target_col].values\n",
    "        input_dates = window['date'].values\n",
    "        \n",
    "        # Define the prediction (horizon) window\n",
    "        prediction_window = data.iloc[i + lookback:i + lookback + horizon]\n",
    "        target_values = prediction_window[prediction_window['day_of_week'].isin(prediction_days)][target_col].values\n",
    "        target_dates = prediction_window[prediction_window['day_of_week'].isin(prediction_days)]['date'].values\n",
    "\n",
    "        # Ensure the prediction window matches the expected length\n",
    "        if len(target_values) == len(prediction_days):\n",
    "            X.append({\"values\": input_values, \"dates\": input_dates})\n",
    "            y.append({\"values\": target_values, \"dates\": target_dates})\n",
    "            dates.append(data.iloc[i + lookback]['date'])\n",
    "\n",
    "    def convert_to_dataframe_with_id(data, id_name):\n",
    "        \"\"\"\n",
    "        Converts a list of dictionaries with 'values' and 'dates' into a DataFrame, \n",
    "        assigning a unique id to each sequence.\n",
    "        Args:\n",
    "            data (list): List of dictionaries with keys 'values' and 'dates'.\n",
    "            id_name (str): Column name for the unique identifier.\n",
    "        Returns:\n",
    "            DataFrame: Combined DataFrame of all sequences with dates, values, and ids.\n",
    "        \"\"\"\n",
    "        df_list = []\n",
    "        for idx, entry in enumerate(data, start=1):\n",
    "            temp_df = pd.DataFrame({\n",
    "                \"date\": entry[\"dates\"],\n",
    "                \"value\": entry[\"values\"],\n",
    "                id_name: idx  # Assign a unique ID for each sequence\n",
    "            })\n",
    "            df_list.append(temp_df)\n",
    "        return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Example X and y conversion with IDs\n",
    "\n",
    "    X_df = convert_to_dataframe_with_id(X, id_name=\"X_id\")\n",
    "    y_df = convert_to_dataframe_with_id(y, id_name=\"y_id\")\n",
    "    def merge_and_order_dataframes(X_df, y_df, X_id_col=\"X_id\", y_id_col=\"y_id\"):\n",
    "        \"\"\"\n",
    "        Merges X_df and y_df, ordering by id and date, and distinguishing X and y records.\n",
    "        Args:\n",
    "            X_df (DataFrame): DataFrame for X with date, value, and ID.\n",
    "            y_df (DataFrame): DataFrame for y with date, value, and ID.\n",
    "            X_id_col (str): Column name for X IDs.\n",
    "            y_id_col (str): Column name for y IDs.\n",
    "        Returns:\n",
    "            DataFrame: Merged and ordered DataFrame.\n",
    "        \"\"\"\n",
    "        # Add a label column to distinguish X and y\n",
    "        X_df[\"type\"] = \"X\"\n",
    "        y_df[\"type\"] = \"y\"\n",
    "        \n",
    "        # Rename ID columns to align for merging\n",
    "        X_df = X_df.rename(columns={X_id_col: \"id\"})\n",
    "        y_df = y_df.rename(columns={y_id_col: \"id\"})\n",
    "\n",
    "        # Concatenate X and y DataFrames\n",
    "        merged_df = pd.concat([X_df, y_df], ignore_index=True)\n",
    "        \n",
    "        # Order by id and date\n",
    "        merged_df = merged_df.sort_values(by=[\"id\", \"date\"]).reset_index(drop=True)\n",
    "        \n",
    "        return merged_df,X_df,y_df\n",
    "    merged_df,X_df,y_df = merge_and_order_dataframes(X_df, y_df)\n",
    "\n",
    "    return X, y, dates,X_df,y_df,merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_dictionary(full_data, X_df, y_df):\n",
    "    value_to_predict = 'value'\n",
    "    \n",
    "    def create_timeseries_dict(grouped_data):\n",
    "        series_dict = {}\n",
    "        for encounter_id, group in grouped_data:\n",
    "            group['date'] = pd.to_datetime(group['date'])  # Ensure 'date' is datetime\n",
    "            group = group.sort_values(by='date')\n",
    "            group = group.set_index('date')  # Set 'date' as index\n",
    "            ts_series = TimeSeries.from_dataframe(group, value_cols=value_to_predict, freq='D')\n",
    "            series_dict[encounter_id] = ts_series\n",
    "        return series_dict\n",
    "\n",
    "    grouped_train = X_df.groupby('id')\n",
    "    grouped_test = y_df.groupby('id')\n",
    "    grouped_validation = full_data.groupby('id')\n",
    "\n",
    "    train_series_dict = create_timeseries_dict(grouped_train)\n",
    "    test_series_dict = create_timeseries_dict(grouped_test)\n",
    "    valid_series_dict = create_timeseries_dict(grouped_validation)\n",
    "\n",
    "    return valid_series_dict, test_series_dict, train_series_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_validation_split(train_series_dict, valid_series_dict, input_chunk_length, output_chunk_length):\n",
    "    \"\"\"\n",
    "    Adjusts train_series_dict to ensure no overlap with targets in valid_series_dict.\n",
    "    Ensures strict separation to prevent information leakage.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_series_dict: Dictionary of training series (time series without targets).\n",
    "    - valid_series_dict: Dictionary of validation series (includes prediction targets).\n",
    "    - input_chunk_length: Length of the input window for the model.\n",
    "    - output_chunk_length: Length of the prediction horizon.\n",
    "    \n",
    "    Returns:\n",
    "    - adjusted_train_series_dict: Training series dictionary without overlapping target data.\n",
    "    - valid_series_dict: Validation series dictionary remains unchanged.\n",
    "    \"\"\"\n",
    "    # Initialize the adjusted training series dictionary\n",
    "    adjusted_train_series_dict = {}\n",
    "    for key, train_series in train_series_dict.items():\n",
    "        #print('TRAIN:', key)\n",
    "        # Check if the key exists in both dictionaries\n",
    "        if key not in valid_series_dict:\n",
    "            continue\n",
    "\n",
    "        # Extract the corresponding validation series\n",
    "        valid_series = valid_series_dict[key]\n",
    "\n",
    "        # Identify the last index of the training data\n",
    "        train_end_idx = len(train_series) - output_chunk_length  # Leave gap for prediction horizon\n",
    "        #print(\"train_end_idx\", train_end_idx)\n",
    "        # Truncate training series to avoid overlap with validation targets\n",
    "        adjusted_train_series = train_series[:train_end_idx]\n",
    "        adjusted_train_series_dict[key] = adjusted_train_series\n",
    "    #print('adjusted_train_series_dict:', train_series_dict)\n",
    "    return train_series_dict, valid_series_dict\n",
    "\n",
    "    #return train_series_dict, valid_series_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_time_series_split(train_series_dict, valid_series_dict, n_splits):\n",
    "    \"\"\"\n",
    "    Custom time-based splitting function for dictionaries of time series.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_series_dict: Dictionary of training time series.\n",
    "    - valid_series_dict: Dictionary of validation time series.\n",
    "    - n_splits: Number of splits for cross-validation.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of (train_keys, valid_keys) pairs for each fold.\n",
    "    \"\"\"\n",
    "    keys = list(train_series_dict.keys())\n",
    "    #print(keys)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    splits = []\n",
    "    for train_idx, valid_idx in kf.split(keys):\n",
    "        train_keys = [keys[i] for i in train_idx]\n",
    "        valid_keys = [keys[i] for i in valid_idx]\n",
    "        splits.append((train_keys, valid_keys))\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_metrics_auto(actual_values, predicted_values, n_bootstrap=1000, ci_percentile=95):\n",
    "    # Initialize arrays to store metrics across bootstraps\n",
    "    mae_bootstrap = []\n",
    "    mse_bootstrap = []\n",
    "    rmse_bootstrap = []\n",
    "    mape_bootstrap = []\n",
    "    smape_bootstrap = []\n",
    "\n",
    "    n_samples = len(actual_values)\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample with replacement\n",
    "        resample_indices = np.random.choice(np.arange(n_samples), size=n_samples, replace=True)\n",
    "        actual_resample = np.array([actual_values[i] for i in resample_indices])\n",
    "        predicted_resample = np.array([predicted_values[i] for i in resample_indices])\n",
    "\n",
    "        # Calculate metrics for this bootstrap sample\n",
    "        mae = np.mean(np.abs(actual_resample - predicted_resample))\n",
    "        mse = np.mean((actual_resample - predicted_resample) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mape = np.mean(np.abs((actual_resample - predicted_resample) / actual_resample)) * 100\n",
    "        smape = np.mean(np.abs(actual_resample - predicted_resample) / (np.abs(actual_resample) + np.abs(predicted_resample))) * 100\n",
    "\n",
    "        # Append the results\n",
    "        mae_bootstrap.append(mae)\n",
    "        mse_bootstrap.append(mse)\n",
    "        rmse_bootstrap.append(rmse)\n",
    "        mape_bootstrap.append(mape)\n",
    "        smape_bootstrap.append(smape)\n",
    "\n",
    "    # Calculate mean and confidence intervals\n",
    "    metrics = {\n",
    "        \"mae\": (np.mean(mae_bootstrap), np.percentile(mae_bootstrap, (100 - ci_percentile) / 2), np.percentile(mae_bootstrap, 100 - (100 - ci_percentile) / 2)),\n",
    "        \"mse\": (np.mean(mse_bootstrap), np.percentile(mse_bootstrap, (100 - ci_percentile) / 2), np.percentile(mse_bootstrap, 100 - (100 - ci_percentile) / 2)),\n",
    "        \"rmse\": (np.mean(rmse_bootstrap), np.percentile(rmse_bootstrap, (100 - ci_percentile) / 2), np.percentile(rmse_bootstrap, 100 - (100 - ci_percentile) / 2)),\n",
    "        \"mape\": (np.mean(mape_bootstrap), np.percentile(mape_bootstrap, (100 - ci_percentile) / 2), np.percentile(mape_bootstrap, 100 - (100 - ci_percentile) / 2)),\n",
    "        \"smape\": (np.mean(smape_bootstrap), np.percentile(smape_bootstrap, (100 - ci_percentile) / 2), np.percentile(smape_bootstrap, 100 - (100 - ci_percentile) / 2)),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from autogluon.timeseries import TimeSeriesDataFrame\n",
    "\n",
    "\n",
    "def autogluon_data_ci_roc_general(\n",
    "    model_name,\n",
    "    output_dir_forecasting,\n",
    "    output_dir_classification,\n",
    "    forecasting_filename,\n",
    "    classification_filename,\n",
    "    resampled_df,\n",
    "    predictor,\n",
    "    scaler,\n",
    "    prediction_length,\n",
    "    value_to_predict,\n",
    "    threshold_value=8,\n",
    "    comparison_operator=\"<\",\n",
    "    n_bootstrap=1000\n",
    "):\n",
    "\n",
    "    # Prepare data\n",
    "    resampled_df = resampled_df.rename(columns={\"id\": \"encounter_id\", \"date\": \"recorded_time\"})\n",
    "\n",
    "    # Split into train/test\n",
    "    def split_train_test(group):\n",
    "        test_rows = group.nlargest(prediction_length, \"recorded_time\")\n",
    "        train_rows = group.drop(test_rows.index)\n",
    "        return train_rows, test_rows\n",
    "\n",
    "    train_list, test_list = [], []\n",
    "    for _, group in resampled_df.groupby(\"encounter_id\"):\n",
    "        train_rows, test_rows = split_train_test(group)\n",
    "        train_list.append(train_rows)\n",
    "        test_list.append(test_rows)\n",
    "\n",
    "    train_data = pd.concat(train_list).sort_values(by=[\"encounter_id\", \"recorded_time\"]).reset_index(drop=True)\n",
    "    test_data = pd.concat(test_list).sort_values(by=[\"encounter_id\", \"recorded_time\"]).reset_index(drop=True)\n",
    "\n",
    "    train_data_ = train_data[[\"encounter_id\", \"recorded_time\", value_to_predict]].copy()\n",
    "    test_data_ = test_data[[\"encounter_id\", \"recorded_time\", value_to_predict]].copy()\n",
    "    train_data_[\"recorded_time\"] = train_data_[\"recorded_time\"].dt.tz_localize(None)\n",
    "    test_data_[\"recorded_time\"] = test_data_[\"recorded_time\"].dt.tz_localize(None)\n",
    "\n",
    "    train_ts = TimeSeriesDataFrame.from_data_frame(train_data_, id_column=\"encounter_id\", timestamp_column=\"recorded_time\")\n",
    "    test_ts = TimeSeriesDataFrame.from_data_frame(test_data_, id_column=\"encounter_id\", timestamp_column=\"recorded_time\")\n",
    "\n",
    "    # Predict with AutoGluon\n",
    "    predictions = predictor.predict(train_ts, model=model_name)\n",
    "    predictions_ = predictions[[\"mean\", \"0.05\", \"0.95\"]]\n",
    "\n",
    "    # Merge with actuals\n",
    "    merged_data = test_ts.join(predictions_, how=\"inner\")\n",
    "\n",
    "    actuals = merged_data[value_to_predict].values.reshape(-1, 1)\n",
    "    preds = merged_data[\"mean\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Rescale\n",
    "    def rescale_predictions(scaled_values, scaler):\n",
    "        if len(scaled_values.shape) == 1:\n",
    "            scaled_values = scaled_values.reshape(-1, 1)\n",
    "        return scaler.inverse_transform(scaled_values).flatten()\n",
    "\n",
    "    actuals = rescale_predictions(actuals, scaler)\n",
    "    preds = rescale_predictions(preds, scaler)\n",
    "\n",
    "    # --- Forecasting metrics with bootstrapping ---\n",
    "    def bootstrap_metrics(actual, predicted, n_bootstrap=1000, ci=95):\n",
    "        mae_list, mse_list, rmse_list, smape_list, mape_list = [], [], [], [], []\n",
    "        for _ in range(n_bootstrap):\n",
    "            idx = np.random.choice(len(actual), len(actual), replace=True)\n",
    "            a, p = actual[idx], predicted[idx]\n",
    "            abs_err = np.abs(p - a)\n",
    "            mae = abs_err.mean()\n",
    "            mse = np.mean((p - a) ** 2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            denom = (np.abs(a) + np.abs(p)) / 2\n",
    "            smape = np.mean(abs_err / denom) * 100\n",
    "            mape = np.mean(np.abs((a - p) / np.where(a == 0, np.nan, a))) * 100\n",
    "            mae_list.append(mae)\n",
    "            mse_list.append(mse)\n",
    "            rmse_list.append(rmse)\n",
    "            smape_list.append(smape)\n",
    "            mape_list.append(mape)\n",
    "\n",
    "        def ci_vals(v):\n",
    "            return (np.mean(v), np.percentile(v, 2.5), np.percentile(v, 97.5))\n",
    "\n",
    "        return {\n",
    "            \"MAE\": ci_vals(mae_list),\n",
    "            \"MSE\": ci_vals(mse_list),\n",
    "            \"RMSE\": ci_vals(rmse_list),\n",
    "            \"SMAPE\": ci_vals(smape_list),\n",
    "            \"MAPE\": ci_vals(mape_list),\n",
    "        }\n",
    "\n",
    "    metrics_forecasting = bootstrap_metrics(actuals, preds, n_bootstrap=n_bootstrap)\n",
    "\n",
    "    # Save forecasting metrics\n",
    "    forecasting_metrics_df = pd.DataFrame(\n",
    "        [\n",
    "            {\"Metric\": m, \"Point Estimate\": v[0], \"95% CI Lower\": v[1], \"95% CI Upper\": v[2]}\n",
    "            for m, v in metrics_forecasting.items()\n",
    "        ]\n",
    "    )\n",
    "    os.makedirs(output_dir_forecasting, exist_ok=True)\n",
    "    forecasting_path = os.path.join(output_dir_forecasting, forecasting_filename)\n",
    "    forecasting_metrics_df.to_csv(forecasting_path, index=False)\n",
    "    print(f\"Forecasting metrics saved to {forecasting_path}\")\n",
    "\n",
    "    # --- Condition detection ---\n",
    "    merged_data[\"actual\"] = actuals.flatten()\n",
    "    merged_data[\"predictions\"] = preds.flatten()\n",
    "\n",
    "    def apply_threshold(x, operator, threshold):\n",
    "        if operator == \"<\":\n",
    "            return x < threshold\n",
    "        elif operator == \"<=\":\n",
    "            return x <= threshold\n",
    "        elif operator == \">\":\n",
    "            return x > threshold\n",
    "        elif operator == \">=\":\n",
    "            return x >= threshold\n",
    "        elif operator == \"==\":\n",
    "            return x == threshold\n",
    "        else:\n",
    "            raise ValueError(\"Invalid comparison_operator. Choose from '<', '<=', '>', '>=', '=='.\")\n",
    "\n",
    "    merged_data[\"actual_condition\"] = merged_data.groupby(\"encounter_id\")[\"actual\"].transform(\n",
    "        lambda x: apply_threshold(x, comparison_operator, threshold_value).any()\n",
    "    )\n",
    "    merged_data[\"predicted_condition\"] = merged_data.groupby(\"encounter_id\")[\"predictions\"].transform(\n",
    "        lambda x: apply_threshold(x, comparison_operator, threshold_value).any()\n",
    "    )\n",
    "\n",
    "    unique_encounters = merged_data.drop_duplicates(subset=[\"encounter_id\"])\n",
    "\n",
    "    # Confusion matrix setup\n",
    "    confusion_conditions = [\n",
    "        (unique_encounters[\"actual_condition\"]) & (unique_encounters[\"predicted_condition\"]),\n",
    "        (unique_encounters[\"actual_condition\"]) & (~unique_encounters[\"predicted_condition\"]),\n",
    "        (~unique_encounters[\"actual_condition\"]) & (unique_encounters[\"predicted_condition\"]),\n",
    "        (~unique_encounters[\"actual_condition\"]) & (~unique_encounters[\"predicted_condition\"]),\n",
    "    ]\n",
    "    confusion_choices = [\"TP\", \"FN\", \"FP\", \"TN\"]\n",
    "    unique_encounters[\"confusion\"] = np.select(confusion_conditions, confusion_choices, default=\"Unknown\")\n",
    "\n",
    "    # --- Bootstrapping classification metrics ---\n",
    "    confusion_matrix_bootstrap = {\"TP\": [], \"FN\": [], \"FP\": [], \"TN\": []}\n",
    "    metrics_bootstrap = {\"sensitivity\": [], \"specificity\": [], \"ppv\": [], \"npv\": [], \"accuracy\": [], \"f1\": [], \"mcc\": []}\n",
    "    auc_scores = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        bs = unique_encounters.sample(frac=1, replace=True)\n",
    "        tp = ((bs[\"actual_condition\"]) & (bs[\"predicted_condition\"])).sum()\n",
    "        fn = ((bs[\"actual_condition\"]) & (~bs[\"predicted_condition\"])).sum()\n",
    "        fp = ((~bs[\"actual_condition\"]) & (bs[\"predicted_condition\"])).sum()\n",
    "        tn = ((~bs[\"actual_condition\"]) & (~bs[\"predicted_condition\"])).sum()\n",
    "\n",
    "        confusion_matrix_bootstrap[\"TP\"].append(tp)\n",
    "        confusion_matrix_bootstrap[\"FN\"].append(fn)\n",
    "        confusion_matrix_bootstrap[\"FP\"].append(fp)\n",
    "        confusion_matrix_bootstrap[\"TN\"].append(tn)\n",
    "\n",
    "        metrics_bootstrap[\"sensitivity\"].append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        metrics_bootstrap[\"specificity\"].append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        metrics_bootstrap[\"ppv\"].append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "        metrics_bootstrap[\"npv\"].append(tn / (tn + fn) if (tn + fn) > 0 else 0)\n",
    "        metrics_bootstrap[\"accuracy\"].append((tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap[\"f1\"].append((2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap[\"mcc\"].append(((tp * tn) - (fp * fn)) / (\n",
    "            ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "        ) if ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) > 0 else 0)\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(bs[\"actual_condition\"], bs[\"predictions\"])\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    def ci_dict(values):\n",
    "        return {\n",
    "            \"Mean\": np.nanmean(values),\n",
    "            \"CI Lower\": np.nanpercentile(values, 2.5),\n",
    "            \"CI Upper\": np.nanpercentile(values, 97.5),\n",
    "        }\n",
    "\n",
    "    confusion_matrix_ci = {k: ci_dict(v) for k, v in confusion_matrix_bootstrap.items()}\n",
    "    metrics_ci = {k: ci_dict(v) for k, v in metrics_bootstrap.items()}\n",
    "    auc_ci = ci_dict(auc_scores)\n",
    "\n",
    "    classification_metrics_data = [\n",
    "        {\"Metric\": metric.capitalize(), \"Point Estimate\": stats[\"Mean\"], \"95% CI Lower\": stats[\"CI Lower\"], \"95% CI Upper\": stats[\"CI Upper\"]}\n",
    "        for metric, stats in metrics_ci.items()\n",
    "    ]\n",
    "    for key, stats in confusion_matrix_ci.items():\n",
    "        classification_metrics_data.append({\n",
    "            \"Metric\": f\"Avg {key}\",\n",
    "            \"Point Estimate\": stats[\"Mean\"],\n",
    "            \"95% CI Lower\": stats[\"CI Lower\"],\n",
    "            \"95% CI Upper\": stats[\"CI Upper\"]\n",
    "        })\n",
    "    classification_metrics_data.append({\n",
    "        \"Metric\": \"AUC\",\n",
    "        \"Point Estimate\": auc_ci[\"Mean\"],\n",
    "        \"95% CI Lower\": auc_ci[\"CI Lower\"],\n",
    "        \"95% CI Upper\": auc_ci[\"CI Upper\"]\n",
    "    })\n",
    "\n",
    "    classification_metrics_df = pd.DataFrame(classification_metrics_data)\n",
    "    os.makedirs(output_dir_classification, exist_ok=True)\n",
    "    class_path = os.path.join(output_dir_classification, classification_filename)\n",
    "    classification_metrics_df.to_csv(class_path, index=False)\n",
    "    print(f\"Classification metrics saved to {class_path}\")\n",
    "\n",
    "    # --- ROC Plot ---\n",
    "    def plot_roc_curve(actuals, predictions, output_path):\n",
    "        if len(np.unique(actuals)) < 2:\n",
    "            print(\"ROC curve not plotted: only one class present.\")\n",
    "            return\n",
    "        fpr, tpr, _ = roc_curve(actuals, predictions)\n",
    "        auc = roc_auc_score(actuals, predictions)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        print(f\"ROC curve saved to {output_path}\")\n",
    "\n",
    "    roc_path = os.path.join(output_dir_classification, \"roc_curve.png\")\n",
    "    plot_roc_curve(unique_encounters[\"actual_condition\"], unique_encounters[\"predictions\"], roc_path)\n",
    "\n",
    "    return preds, actuals, train_ts, test_ts, merged_data, unique_encounters[\"actual_condition\"], unique_encounters[\"predictions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timellm_data_ci_roc_(\n",
    "    model_name,\n",
    "    output_dir_forecasting,\n",
    "    output_dir_classification,\n",
    "    forecasting_filename,\n",
    "    classification_filename,\n",
    "    resampled_df,\n",
    "    nf,\n",
    "    scaler,\n",
    "    prediction_length,\n",
    "    value_to_predict,\n",
    "    threshold_value=8,\n",
    "    comparison_operator='<',\n",
    "    n_bootstrap=1000\n",
    "):\n",
    "\n",
    "    # Split into train/test\n",
    "    full_data_test = resampled_df.groupby(\"encounter_id\").tail(prediction_length)\n",
    "    full_data_train = resampled_df.drop(full_data_test.index)\n",
    "    full_data_train_ = full_data_train.rename(columns={'recorded_time': 'ds', 'encounter_id': 'unique_id','value':'y'})\n",
    "    full_data_test_ = full_data_test.rename(columns={'recorded_time': 'ds', 'encounter_id': 'unique_id','value':'y'})\n",
    "\n",
    "    # Forecast with TimeLLM model\n",
    "    forecasts = nf.predict(full_data_train_)\n",
    "\n",
    "    # Helper: inverse transform\n",
    "    def rescale_to_original(scaled_df, scaler, columns_to_rescale):\n",
    "        scaled_df = scaled_df.copy()\n",
    "        scaled_df[columns_to_rescale] = scaler.inverse_transform(scaled_df[columns_to_rescale])\n",
    "        return scaled_df\n",
    "\n",
    "    columns_to_rescale_actual = [\"y\"]\n",
    "    columns_to_rescale_predict = [\"TimeLLM\"]\n",
    "    actuals = rescale_to_original(full_data_test_, scaler, columns_to_rescale_actual)\n",
    "    predictions = rescale_to_original(forecasts, scaler, columns_to_rescale_predict)\n",
    "\n",
    "    # --- Bootstrap metrics for forecasting ---\n",
    "    def bootstrap_timellm_metrics_length(forecasts_df, actual_values_df, prediction_length, n_bootstrap, ci_percentile=95):\n",
    "        mae_bootstrap, mse_bootstrap, rmse_bootstrap, smape_bootstrap, mape_bootstrap = [], [], [], [], []\n",
    "\n",
    "        merged_df = pd.merge(forecasts_df.reset_index(), actual_values_df, on=['unique_id', 'ds'])\n",
    "        unique_ids = merged_df['unique_id'].unique()\n",
    "\n",
    "        for _ in range(n_bootstrap):\n",
    "            sampled_ids = np.random.choice(unique_ids, size=len(unique_ids), replace=True)\n",
    "            sampled_data = merged_df[merged_df['unique_id'].isin(sampled_ids)]\n",
    "            if len(sampled_ids) == 0:\n",
    "                continue\n",
    "\n",
    "            total_mae = total_mse = total_smape = total_mape = 0\n",
    "\n",
    "            for unique_id in sampled_ids:\n",
    "                data = sampled_data[sampled_data['unique_id'] == unique_id].sort_values(by='ds')\n",
    "                if data.empty:\n",
    "                    continue\n",
    "\n",
    "                forecast_values = data['TimeLLM'].values[-prediction_length:]\n",
    "                true_values = data['y'].values[-prediction_length:]\n",
    "                if len(forecast_values) == 0 or len(true_values) == 0:\n",
    "                    continue\n",
    "\n",
    "                abs_err = np.abs(forecast_values - true_values)\n",
    "                mae = abs_err.mean()\n",
    "                mse = (abs_err ** 2).mean()\n",
    "                denominator = (np.abs(true_values) + np.abs(forecast_values)) / 2\n",
    "                smape = (abs_err / denominator).mean() * 100\n",
    "                mape = np.mean(np.abs((true_values - forecast_values) / np.where(true_values == 0, np.nan, true_values))) * 100\n",
    "\n",
    "                total_mae += mae\n",
    "                total_mse += mse\n",
    "                total_smape += smape\n",
    "                total_mape += mape\n",
    "\n",
    "            mae_bootstrap.append(total_mae / len(sampled_ids))\n",
    "            mse_bootstrap.append(total_mse / len(sampled_ids))\n",
    "            rmse_bootstrap.append(np.sqrt(total_mse / len(sampled_ids)))\n",
    "            smape_bootstrap.append(total_smape / len(sampled_ids))\n",
    "            mape_bootstrap.append(total_mape / len(sampled_ids))\n",
    "\n",
    "        metrics = {\n",
    "            \"mae\": (np.mean(mae_bootstrap), np.percentile(mae_bootstrap, 2.5), np.percentile(mae_bootstrap, 97.5)),\n",
    "            \"mse\": (np.mean(mse_bootstrap), np.percentile(mse_bootstrap, 2.5), np.percentile(mse_bootstrap, 97.5)),\n",
    "            \"rmse\": (np.mean(rmse_bootstrap), np.percentile(rmse_bootstrap, 2.5), np.percentile(rmse_bootstrap, 97.5)),\n",
    "            \"smape\": (np.mean(smape_bootstrap), np.percentile(smape_bootstrap, 2.5), np.percentile(smape_bootstrap, 97.5)),\n",
    "            \"mape\": (np.mean(mape_bootstrap), np.percentile(mape_bootstrap, 2.5), np.percentile(mape_bootstrap, 97.5)),\n",
    "        }\n",
    "        return metrics, merged_df\n",
    "\n",
    "    metrics, merged_data = bootstrap_timellm_metrics_length(predictions, actuals, prediction_length, n_bootstrap, 95)\n",
    "\n",
    "    # Save forecasting metrics\n",
    "    forecasting_metrics_df = pd.DataFrame(metrics)\n",
    "    os.makedirs(output_dir_forecasting, exist_ok=True)\n",
    "    forecasting_csv_path = os.path.join(output_dir_forecasting, forecasting_filename)\n",
    "    forecasting_metrics_df.to_csv(forecasting_csv_path, index=False)\n",
    "    print(f\"Forecasting metrics saved to {forecasting_csv_path}\")\n",
    "\n",
    "    merged_data = merged_data.rename(columns={'y': 'actual', 'TimeLLM': 'predictions'})\n",
    "\n",
    "    # --- Apply condition threshold ---\n",
    "    def apply_threshold(x, operator, threshold):\n",
    "        if operator == '<':\n",
    "            return x < threshold\n",
    "        elif operator == '<=':\n",
    "            return x <= threshold\n",
    "        elif operator == '>':\n",
    "            return x > threshold\n",
    "        elif operator == '>=':\n",
    "            return x >= threshold\n",
    "        elif operator == '==':\n",
    "            return x == threshold\n",
    "        else:\n",
    "            raise ValueError(\"Invalid comparison_operator. Choose from '<', '<=', '>', '>=', '=='.\")\n",
    "\n",
    "    merged_data['actual_condition'] = merged_data.groupby('unique_id')['actual'].transform(lambda x: apply_threshold(x, comparison_operator, threshold_value).any())\n",
    "    merged_data['predicted_condition'] = merged_data.groupby('unique_id')['predictions'].transform(lambda x: apply_threshold(x, comparison_operator, threshold_value).any())\n",
    "\n",
    "    merged_data = merged_data.reset_index(drop=True)\n",
    "    unique_encounters = merged_data.drop_duplicates(subset=['unique_id'])\n",
    "\n",
    "    # Confusion matrix\n",
    "    confusion_conditions = [\n",
    "        (unique_encounters['actual_condition']) & (unique_encounters['predicted_condition']),\n",
    "        (unique_encounters['actual_condition']) & (~unique_encounters['predicted_condition']),\n",
    "        (~unique_encounters['actual_condition']) & (unique_encounters['predicted_condition']),\n",
    "        (~unique_encounters['actual_condition']) & (~unique_encounters['predicted_condition']),\n",
    "    ]\n",
    "    confusion_choices = ['TP', 'FN', 'FP', 'TN']\n",
    "    unique_encounters['confusion'] = np.select(confusion_conditions, confusion_choices, default='Unknown')\n",
    "\n",
    "    # Bootstrap classification metrics\n",
    "    confusion_matrix_bootstrap = {'TP': [], 'FN': [], 'FP': [], 'TN': []}\n",
    "    metrics_bootstrap = {'sensitivity': [], 'specificity': [], 'ppv': [], 'npv': [], 'accuracy': [], 'f1': [], 'mcc': []}\n",
    "    auc_scores = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        bootstrap_sample = unique_encounters.sample(frac=1, replace=True)\n",
    "        tp = ((bootstrap_sample['actual_condition']) & (bootstrap_sample['predicted_condition'])).sum()\n",
    "        fn = ((bootstrap_sample['actual_condition']) & (~bootstrap_sample['predicted_condition'])).sum()\n",
    "        fp = ((~bootstrap_sample['actual_condition']) & (bootstrap_sample['predicted_condition'])).sum()\n",
    "        tn = ((~bootstrap_sample['actual_condition']) & (~bootstrap_sample['predicted_condition'])).sum()\n",
    "\n",
    "        confusion_matrix_bootstrap['TP'].append(tp)\n",
    "        confusion_matrix_bootstrap['FN'].append(fn)\n",
    "        confusion_matrix_bootstrap['FP'].append(fp)\n",
    "        confusion_matrix_bootstrap['TN'].append(tn)\n",
    "\n",
    "        metrics_bootstrap['sensitivity'].append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['specificity'].append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        metrics_bootstrap['ppv'].append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "        metrics_bootstrap['npv'].append(tn / (tn + fn) if (tn + fn) > 0 else 0)\n",
    "        metrics_bootstrap['accuracy'].append((tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['f1'].append((2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0)\n",
    "        metrics_bootstrap['mcc'].append(((tp * tn) - (fp * fn)) / (\n",
    "            ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "        ) if ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) > 0 else 0)\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(bootstrap_sample['actual_condition'], bootstrap_sample['predictions'])\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    # Compute CI\n",
    "    def ci_dict(values):\n",
    "        return {\n",
    "            \"Mean\": np.nanmean(values),\n",
    "            \"CI Lower\": np.nanpercentile(values, 2.5),\n",
    "            \"CI Upper\": np.nanpercentile(values, 97.5),\n",
    "        }\n",
    "\n",
    "    confusion_matrix_ci = {k: ci_dict(v) for k, v in confusion_matrix_bootstrap.items()}\n",
    "    metrics_ci = {k: ci_dict(v) for k, v in metrics_bootstrap.items()}\n",
    "    auc_ci = ci_dict(auc_scores)\n",
    "\n",
    "    # Prepare classification metrics\n",
    "    classification_metrics_data = [\n",
    "        {\"Metric\": metric.capitalize(), \"Point Estimate\": stats[\"Mean\"], \"95% CI Lower\": stats[\"CI Lower\"], \"95% CI Upper\": stats[\"CI Upper\"]}\n",
    "        for metric, stats in metrics_ci.items()\n",
    "    ]\n",
    "    for key, stats in confusion_matrix_ci.items():\n",
    "        classification_metrics_data.append({\n",
    "            \"Metric\": f\"Avg {key}\",\n",
    "            \"Point Estimate\": stats[\"Mean\"],\n",
    "            \"95% CI Lower\": stats[\"CI Lower\"],\n",
    "            \"95% CI Upper\": stats[\"CI Upper\"]\n",
    "        })\n",
    "    classification_metrics_data.append({\n",
    "        \"Metric\": \"AUC\",\n",
    "        \"Point Estimate\": auc_ci[\"Mean\"],\n",
    "        \"95% CI Lower\": auc_ci[\"CI Lower\"],\n",
    "        \"95% CI Upper\": auc_ci[\"CI Upper\"],\n",
    "    })\n",
    "\n",
    "    classification_metrics_df = pd.DataFrame(classification_metrics_data)\n",
    "    os.makedirs(output_dir_classification, exist_ok=True)\n",
    "    classification_csv_path = os.path.join(output_dir_classification, classification_filename)\n",
    "    classification_metrics_df.to_csv(classification_csv_path, index=False)\n",
    "    print(f\"Classification metrics saved to {classification_csv_path}\")\n",
    "\n",
    "    # Plot ROC\n",
    "    def plot_roc_curve(actuals, predictions, output_path):\n",
    "        if len(np.unique(actuals)) < 2:\n",
    "            print(\"ROC curve not plotted: only one class present.\")\n",
    "            return\n",
    "        fpr, tpr, _ = roc_curve(actuals, predictions)\n",
    "        auc = roc_auc_score(actuals, predictions)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        print(f\"ROC curve saved to {output_path}\")\n",
    "\n",
    "    roc_curve_path = os.path.join(output_dir_classification, 'roc_curve.png')\n",
    "    plot_roc_curve(unique_encounters['actual_condition'], unique_encounters['predictions'], roc_curve_path)\n",
    "\n",
    "    return predictions, actuals, merged_data, unique_encounters['actual_condition'], unique_encounters['predictions']\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "deecd4eed52e347fdb227bd6db7ebbadf0a7fc99b7440320ff3532cd8683bd78"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
