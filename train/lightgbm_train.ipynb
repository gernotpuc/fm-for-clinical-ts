{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from darts import TimeSeries\n",
    "from darts.models import LightGBMModel\n",
    "from darts.metrics import mae, rmse, mape, smape\n",
    "\n",
    "# Force CPU if no GPU available\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" \n",
    "prediction_length = prediction_length\n",
    "\n",
    "\n",
    "def get_train_test_split(series_list, prediction_length):\n",
    "    train_series = [s[:-prediction_length] for s in series_list]\n",
    "    test_series = [s[-prediction_length:] for s in series_list]\n",
    "    return train_series, test_series\n",
    "\n",
    "\n",
    "def bootstrap_metrics(actual, predicted, n_bootstraps=1000, alpha=0.05):\n",
    " \n",
    "    actual = np.asarray(actual)\n",
    "    predicted = np.asarray(predicted)\n",
    "    n = len(actual)\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    def compute_metrics(a, p):\n",
    "        return {\n",
    "            \"mae\": mae(TimeSeries.from_values(a), TimeSeries.from_values(p)),\n",
    "            \"rmse\": rmse(TimeSeries.from_values(a), TimeSeries.from_values(p)),\n",
    "            \"mse\": np.mean((a - p) ** 2),\n",
    "            \"mape\": mape(TimeSeries.from_values(a), TimeSeries.from_values(p)),\n",
    "            \"smape\": smape(TimeSeries.from_values(a), TimeSeries.from_values(p)),\n",
    "        }\n",
    "\n",
    "    base = compute_metrics(actual, predicted)\n",
    "    boot = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        a_s, p_s = actual[idx], predicted[idx]\n",
    "        boot.append(list(compute_metrics(a_s, p_s).values()))\n",
    "    boot = np.array(boot)\n",
    "    lower = np.percentile(boot, 100 * (alpha / 2), axis=0)\n",
    "    upper = np.percentile(boot, 100 * (1 - alpha / 2), axis=0)\n",
    "    keys = list(base.keys())\n",
    "    return {**{k: base[k] for k in keys},\n",
    "            **{f\"{k}_ci_lower\": l for k, l in zip(keys, lower)},\n",
    "            **{f\"{k}_ci_upper\": u for k, u in zip(keys, upper)}}\n",
    "\n",
    "\n",
    "def inner_objective(trial, train_series_list):\n",
    "    # Search space\n",
    "    lags = trial.suggest_int(\"lags\", 3, 42)\n",
    "    min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 5, 50)\n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 8, 64)\n",
    "    feature_fraction = trial.suggest_float(\"feature_fraction\", 0.5, 1.0)\n",
    "    bagging_fraction = trial.suggest_float(\"bagging_fraction\", 0.5, 1.0)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True)\n",
    "    lambda_l1 = trial.suggest_float(\"lambda_l1\", 0.0, 1.0)\n",
    "    lambda_l2 = trial.suggest_float(\"lambda_l2\", 0.0, 1.0)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "\n",
    "    kf_inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    inner_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf_inner.split(train_series_list):\n",
    "        fold_train = [train_series_list[i] for i in train_idx]\n",
    "        fold_val = [train_series_list[i] for i in val_idx]\n",
    "\n",
    "        min_req = lags + prediction_length\n",
    "        if any(len(s) <= min_req for s in fold_train + fold_val):\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        model = LightGBMModel(\n",
    "            lags=lags,\n",
    "            lags_past_covariates=None,\n",
    "            output_chunk_length=prediction_length,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            num_leaves=num_leaves,\n",
    "            feature_fraction=feature_fraction,\n",
    "            bagging_fraction=bagging_fraction,\n",
    "            max_depth=max_depth,\n",
    "            lambda_l1=lambda_l1,\n",
    "            lambda_l2=lambda_l2,\n",
    "            min_data_in_leaf=min_data_in_leaf,\n",
    "        )\n",
    "\n",
    "        model.fit(fold_train, verbose=False)\n",
    "        preds = model.predict(n=prediction_length, series=fold_val)\n",
    "        actual = np.concatenate([s.values().flatten() for s in fold_val])\n",
    "        pred = np.concatenate([p.values().flatten() for p in preds])\n",
    "        score = mae(TimeSeries.from_values(actual), TimeSeries.from_values(pred))\n",
    "        inner_scores.append(score)\n",
    "\n",
    "    return np.mean(inner_scores)\n",
    "\n",
    "\n",
    "def nested_cross_validation(full_series_dict, n_outer_folds=5, n_trials=20):\n",
    "    all_series_list = list(full_series_dict.values())\n",
    "    train_input_series, test_target_series = get_train_test_split(all_series_list, prediction_length)\n",
    "    kf_outer = KFold(n_splits=n_outer_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    aggregated_actuals, aggregated_predictions = [], []\n",
    "    outer_results = []\n",
    "\n",
    "    print(f\"Starting Nested {n_outer_folds}-Fold Cross-Validation...\")\n",
    "\n",
    "    for fold_num, (train_idx, test_idx) in enumerate(kf_outer.split(train_input_series)):\n",
    "        print(f\"\\n--- Outer Fold {fold_num+1}/{n_outer_folds} ---\")\n",
    "\n",
    "        outer_train = [train_input_series[i] for i in train_idx]\n",
    "        outer_test = [train_input_series[i] for i in test_idx]\n",
    "        outer_targets = [test_target_series[i] for i in test_idx]\n",
    "\n",
    "        # Scale based on outer training data\n",
    "        all_train_vals = np.concatenate([s.values().flatten() for s in outer_train])\n",
    "        scaler = MinMaxScaler((0, 1))\n",
    "        scaler.fit(all_train_vals.reshape(-1, 1))\n",
    "        outer_train_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in outer_train]\n",
    "        outer_test_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in outer_test]\n",
    "        outer_targets_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in outer_targets]\n",
    "\n",
    "        # Inner optimization\n",
    "        study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        wrapped_objective = lambda trial: inner_objective(trial, outer_train_scaled)\n",
    "        study.optimize(wrapped_objective, n_trials=n_trials, show_progress_bar=False)\n",
    "\n",
    "        if len(study.trials) == 0 or study.best_trial is None:\n",
    "            print(f\"[Warning] No valid trials in Fold {fold_num+1}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        best_params = study.best_trial.params\n",
    "        print(f\"Best hyperparameters (Fold {fold_num+1}): {best_params}\")\n",
    "\n",
    "        # Train final fold model\n",
    "        model = LightGBMModel(\n",
    "            lags=best_params[\"lags\"],\n",
    "            output_chunk_length=prediction_length,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_estimators=best_params[\"n_estimators\"],\n",
    "            learning_rate=best_params[\"learning_rate\"],\n",
    "            num_leaves=best_params[\"num_leaves\"],\n",
    "            feature_fraction=best_params[\"feature_fraction\"],\n",
    "            bagging_fraction=best_params[\"bagging_fraction\"],\n",
    "            max_depth=best_params[\"max_depth\"],\n",
    "            lambda_l1=best_params[\"lambda_l1\"],\n",
    "            lambda_l2=best_params[\"lambda_l2\"],\n",
    "            min_data_in_leaf=best_params[\"min_data_in_leaf\"],\n",
    "        )\n",
    "\n",
    "        model.fit(series=outer_train_scaled, verbose=False)\n",
    "        preds_scaled = model.predict(n=prediction_length, series=outer_test_scaled)\n",
    "\n",
    "        actuals_fold, preds_fold = [], []\n",
    "        for p, t in zip(preds_scaled, outer_targets_scaled):\n",
    "            y_pred = scaler.inverse_transform(p.values().reshape(-1, 1))[0, 0]\n",
    "            y_true = scaler.inverse_transform(t.values().reshape(-1, 1))[0, 0]\n",
    "            preds_fold.append(y_pred)\n",
    "            actuals_fold.append(y_true)\n",
    "\n",
    "        metrics = bootstrap_metrics(np.array(actuals_fold), np.array(preds_fold))\n",
    "        fold_mae = metrics[\"mae\"]\n",
    "        print(f\"Outer Fold {fold_num+1} MAE: {fold_mae:.4f}\")\n",
    "        outer_results.append({\"fold\": fold_num + 1, \"mae\": fold_mae, \"params\": best_params})\n",
    "\n",
    "        aggregated_actuals.extend(actuals_fold)\n",
    "        aggregated_predictions.extend(preds_fold)\n",
    "\n",
    "    final_metrics = bootstrap_metrics(np.array(aggregated_actuals), np.array(aggregated_predictions))\n",
    "    best_overall = min(outer_results, key=lambda x: x[\"mae\"])\n",
    "    best_params = best_overall[\"params\"]\n",
    "\n",
    "    print(\"Best Overall Parameters (Lowest Outer-Fold MAE):\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(f\"Best outer fold: {best_overall['fold']} (MAE={best_overall['mae']:.4f})\")\n",
    "\n",
    "    return final_metrics, best_params\n",
    "\n",
    "def train_final_and_evaluate_external(internal_series_dict, external_series_dict,\n",
    "                                      best_params, prediction_length=1):\n",
    "    print(\"Retraining final LightGBM model using best overall parameters\")\n",
    "    internal_series = list(internal_series_dict.values())\n",
    "    external_series = list(external_series_dict.values())\n",
    "\n",
    "    # Fit global scaler on internal data only\n",
    "    all_vals = np.concatenate([s.values().flatten() for s in internal_series])\n",
    "    scaler = MinMaxScaler((0, 1))\n",
    "    scaler.fit(all_vals.reshape(-1, 1))\n",
    "    internal_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in internal_series]\n",
    "    external_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in external_series]\n",
    "\n",
    "    model = LightGBMModel(\n",
    "        lags=best_params[\"lags\"],\n",
    "        output_chunk_length=prediction_length,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        num_leaves=best_params[\"num_leaves\"],\n",
    "        feature_fraction=best_params[\"feature_fraction\"],\n",
    "        bagging_fraction=best_params[\"bagging_fraction\"],\n",
    "        max_depth=best_params[\"max_depth\"],\n",
    "        lambda_l1=best_params[\"lambda_l1\"],\n",
    "        lambda_l2=best_params[\"lambda_l2\"],\n",
    "        min_data_in_leaf=best_params[\"min_data_in_leaf\"],\n",
    "    )\n",
    "\n",
    "    model.fit(series=internal_scaled, verbose=True)\n",
    "\n",
    "    ext_input, ext_target = get_train_test_split(external_scaled, prediction_length)\n",
    "    preds_scaled = model.predict(n=prediction_length, series=ext_input)\n",
    "\n",
    "    preds, actuals = [], []\n",
    "    for p, t in zip(preds_scaled, ext_target):\n",
    "        y_pred = scaler.inverse_transform(p.values().reshape(-1, 1))[0, 0]\n",
    "        y_true = scaler.inverse_transform(t.values().reshape(-1, 1))[0, 0]\n",
    "        preds.append(y_pred)\n",
    "        actuals.append(y_true)\n",
    "\n",
    "    metrics = bootstrap_metrics(np.array(actuals), np.array(preds))\n",
    "    print(\"Final External Evaluation:\")\n",
    "    print(f\"MAE:   {metrics['mae']:.4f} (95% CI: [{metrics['mae_ci_lower']:.4f}, {metrics['mae_ci_upper']:.4f}])\")\n",
    "    print(f\"RMSE:  {metrics['rmse']:.4f} (95% CI: [{metrics['rmse_ci_lower']:.4f}, {metrics['rmse_ci_upper']:.4f}])\")\n",
    "    print(f\"MSE:   {metrics['mse']:.4f} (95% CI: [{metrics['mse_ci_lower']:.4f}, {metrics['mse_ci_upper']:.4f}])\")\n",
    "    print(f\"MAPE:  {metrics['mape']:.2f}% (95% CI: [{metrics['mape_ci_lower']:.2f}%, {metrics['mape_ci_upper']:.2f}%])\")\n",
    "    print(f\"sMAPE: {metrics['smape']:.2f}% (95% CI: [{metrics['smape_ci_lower']:.2f}%, {metrics['smape_ci_upper']:.2f}%])\")\n",
    "    return model, metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Generating dummy internal and external datasets for test. Replace with your data.\")\n",
    "    valid_series_dict_full = {f\"Encounter/{i}\": TimeSeries.from_values(np.random.rand(20, 1) * 50) for i in range(30)}\n",
    "    external_series_dict = {f\"HospId/{i}\": TimeSeries.from_values(np.random.rand(20, 1) * 50) for i in range(10)}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 1: Nested CV to find best parameters\n",
    "    nested_metrics, best_params = nested_cross_validation(\n",
    "        full_series_dict=valid_series_dict_full,\n",
    "        n_outer_folds=10,\n",
    "        n_trials=50\n",
    "    )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 2: Retrain final model and evaluate on external data\n",
    "    final_model, external_metrics = train_final_and_evaluate_external(\n",
    "        internal_series_dict=valid_series_dict_full,\n",
    "        external_series_dict=external_series_dict,\n",
    "        best_params=best_params,\n",
    "        prediction_length=prediction_length\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "deecd4eed52e347fdb227bd6db7ebbadf0a7fc99b7440320ff3532cd8683bd78"
  },
  "kernelspec": {
   "display_name": "Python 3.10.18 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
