{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel\n",
    "from darts.metrics import mae, rmse\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from darts.metrics import mae, rmse, mse, mape, smape\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "prediction_length = prediction_length\n",
    "\n",
    "def bootstrap_metrics(actual, predicted, n_bootstraps=1000, alpha=0.05, random_state=42):\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    actual = np.asarray(actual).flatten()\n",
    "    predicted = np.asarray(predicted).flatten()\n",
    "    n = len(actual)\n",
    "\n",
    "    actual_ts = TimeSeries.from_values(actual)\n",
    "    pred_ts = TimeSeries.from_values(predicted)\n",
    "\n",
    "    point_mae = mae(actual_ts, pred_ts)\n",
    "    point_rmse = rmse(actual_ts, pred_ts)\n",
    "    point_mse = mse(actual_ts, pred_ts)\n",
    "    point_mape = mape(actual_ts, pred_ts)\n",
    "    point_smape = smape(actual_ts, pred_ts)\n",
    "\n",
    "    metrics_boot = {\n",
    "        \"mae\": np.empty(n_bootstraps),\n",
    "        \"rmse\": np.empty(n_bootstraps),\n",
    "        \"mse\": np.empty(n_bootstraps),\n",
    "        \"mape\": np.empty(n_bootstraps),\n",
    "        \"smape\": np.empty(n_bootstraps),\n",
    "    }\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        y_true = actual[idx]\n",
    "        y_pred = predicted[idx]\n",
    "\n",
    "        a_ts = TimeSeries.from_values(y_true)\n",
    "        p_ts = TimeSeries.from_values(y_pred)\n",
    "\n",
    "        metrics_boot[\"mae\"][i] = mae(a_ts, p_ts)\n",
    "        metrics_boot[\"rmse\"][i] = rmse(a_ts, p_ts)\n",
    "        metrics_boot[\"mse\"][i] = mse(a_ts, p_ts)\n",
    "        metrics_boot[\"mape\"][i] = mape(a_ts, p_ts)\n",
    "        metrics_boot[\"smape\"][i] = smape(a_ts, p_ts)\n",
    "\n",
    "    lower_q, upper_q = 100 * (alpha / 2), 100 * (1 - alpha / 2)\n",
    "    result = {}\n",
    "    for name, arr in metrics_boot.items():\n",
    "        low, high = np.percentile(arr, [lower_q, upper_q])\n",
    "        result[name] = eval(f\"point_{name}\")\n",
    "        result[f\"{name}_ci_lower\"] = low\n",
    "        result[f\"{name}_ci_upper\"] = high\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_train_test_split(series_list, prediction_length):\n",
    "    # Splits each series into training and test part\n",
    "    train_series = [s[:-prediction_length] for s in series_list]\n",
    "    test_series = [s[-prediction_length:] for s in series_list]\n",
    "    return train_series, test_series\n",
    "\n",
    "\n",
    "def inner_objective(trial, train_series_list):\n",
    "    # using inner 3-fold CV for HOP\n",
    "    input_chunk_length = trial.suggest_int(\"input_chunk_length\", 3, 9)\n",
    "    layer_widths_value = trial.suggest_int(\"layer_widths_value\", 128, 512)\n",
    "    num_blocks = trial.suggest_int(\"num_blocks\", 1, 6)\n",
    "    num_stacks = trial.suggest_int(\"num_stacks\", 1, 6)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    kf_inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    inner_cv_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf_inner.split(train_series_list):\n",
    "        fold_train_series = [train_series_list[i] for i in train_idx]\n",
    "        fold_val_series = [train_series_list[i] for i in val_idx]\n",
    "\n",
    "        # Scale per fold\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        train_concat = np.concatenate([s.values() for s in fold_train_series])\n",
    "        scaler.fit(train_concat)\n",
    "        fold_train_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in fold_train_series]\n",
    "        fold_val_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in fold_val_series]\n",
    "\n",
    "        \n",
    "        model = NBEATSModel(\n",
    "            input_chunk_length=input_chunk_length,\n",
    "            output_chunk_length=prediction_length,\n",
    "            layer_widths=layer_widths_value,\n",
    "            num_blocks=num_blocks,\n",
    "            num_stacks=num_stacks,\n",
    "            dropout=dropout,\n",
    "            random_state=42,\n",
    "            batch_size=batch_size,\n",
    "            optimizer_kwargs={\"lr\": learning_rate},\n",
    "            likelihood=None,\n",
    "            pl_trainer_kwargs={\n",
    "                \"accelerator\": \"cpu\",\n",
    "                \"devices\": 1,\n",
    "                \"callbacks\": [EarlyStopping(monitor=\"train_loss\", patience=10, mode=\"min\")]\n",
    "            },\n",
    "        )\n",
    "\n",
    "        model.fit(series=fold_train_scaled, epochs=50, verbose=False)\n",
    "\n",
    "        # Forecast and inverse-transform\n",
    "        predictions_scaled = model.predict(n=prediction_length, series=fold_val_scaled)\n",
    "\n",
    "        actual_values = np.concatenate([s.values().flatten() for s in fold_val_series])\n",
    "        pred_values = np.concatenate([p.all_values().flatten() for p in predictions_scaled])\n",
    "\n",
    "        actual_rescaled = scaler.inverse_transform(actual_values.reshape(-1, 1)).flatten()\n",
    "        predicted_rescaled = scaler.inverse_transform(pred_values.reshape(-1, 1)).flatten()\n",
    "\n",
    "        fold_mae = mae(TimeSeries.from_values(actual_rescaled), TimeSeries.from_values(predicted_rescaled))\n",
    "        inner_cv_scores.append(fold_mae)\n",
    "\n",
    "    return np.mean(inner_cv_scores)\n",
    "\n",
    "\n",
    "def nested_cross_validation(full_series_dict, n_outer_folds=10, n_trials=50, max_epochs=500):\n",
    "    all_series_list = list(full_series_dict.values())\n",
    "    train_input_series, test_target_series = get_train_test_split(all_series_list, prediction_length)\n",
    "\n",
    "    kf_outer = KFold(n_splits=n_outer_folds, shuffle=True, random_state=42)\n",
    "    aggregated_actuals = []\n",
    "    aggregated_predictions = []\n",
    "\n",
    "    print(f\"Starting Nested {n_outer_folds}-Fold Cross-Validation...\")\n",
    "\n",
    "    for fold_num, (train_index, test_index) in enumerate(kf_outer.split(train_input_series)):\n",
    "        print(f\"\\n--- Outer Fold {fold_num+1}/{n_outer_folds} ---\")\n",
    "\n",
    "        # Define outer splits\n",
    "        outer_train_series = [train_input_series[i] for i in train_index]\n",
    "        outer_test_series = [train_input_series[i] for i in test_index]\n",
    "        outer_test_targets = [test_target_series[i] for i in test_index]\n",
    "\n",
    "        # Fit global scaler on outer-train\n",
    "        all_train_values = np.concatenate([s.values().flatten() for s in outer_train_series])\n",
    "        global_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        global_scaler.fit(all_train_values.reshape(-1, 1))\n",
    "\n",
    "        # Transform\n",
    "        outer_train_scaled = [TimeSeries.from_values(global_scaler.transform(s.values())) for s in outer_train_series]\n",
    "        outer_test_scaled = [TimeSeries.from_values(global_scaler.transform(s.values())) for s in outer_test_series]\n",
    "        outer_test_targets_scaled = [TimeSeries.from_values(global_scaler.transform(s.values())) for s in outer_test_targets]\n",
    "\n",
    "        # Inner optimization\n",
    "        study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        wrapped_objective = lambda trial: inner_objective(trial, outer_train_scaled)\n",
    "        study.optimize(wrapped_objective, n_trials=n_trials, show_progress_bar=False)\n",
    "        best_params = study.best_trial.params\n",
    "        print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "        # Train\n",
    "        best_model = NBEATSModel(\n",
    "            input_chunk_length=best_params[\"input_chunk_length\"],\n",
    "            output_chunk_length=prediction_length,\n",
    "            random_state=42,\n",
    "            dropout=best_params[\"dropout\"],\n",
    "            num_blocks=best_params[\"num_blocks\"],\n",
    "            num_stacks=best_params[\"num_stacks\"],\n",
    "            batch_size=best_params[\"batch_size\"],\n",
    "            layer_widths=best_params[\"layer_widths_value\"],\n",
    "            optimizer_kwargs={\"lr\": best_params[\"learning_rate\"]},\n",
    "            likelihood=None,\n",
    "            pl_trainer_kwargs={\n",
    "                \"accelerator\": \"cpu\",\n",
    "                \"devices\": 1,\n",
    "                \"callbacks\": [EarlyStopping(monitor=\"train_loss\", patience=20, mode=\"min\")]\n",
    "            },\n",
    "        )\n",
    "\n",
    "        best_model.fit(series=outer_train_scaled, epochs=max_epochs, verbose=False)\n",
    "\n",
    "        # Forecast\n",
    "        predictions_scaled = best_model.predict(n=prediction_length, series=outer_test_scaled)\n",
    "\n",
    "        # Inverse-transform predictions\n",
    "        actuals_fold, preds_fold = [], []\n",
    "        for pred_ts, true_ts in zip(predictions_scaled, outer_test_targets_scaled):\n",
    "            y_pred_scaled = pred_ts.values().flatten()[0]\n",
    "            y_true_scaled = true_ts.values().flatten()[0]\n",
    "            y_pred = global_scaler.inverse_transform([[y_pred_scaled]])[0, 0]\n",
    "            y_true = global_scaler.inverse_transform([[y_true_scaled]])[0, 0]\n",
    "            preds_fold.append(y_pred)\n",
    "            actuals_fold.append(y_true)\n",
    "\n",
    "        print(\"Example rescaled pairs (first 5):\")\n",
    "        for a, p in list(zip(actuals_fold, preds_fold))[:5]:\n",
    "            print(f\"True={a:.2f}, Pred={p:.2f}, Diff={abs(a-p):.2f}\")\n",
    "\n",
    "        aggregated_actuals.extend(actuals_fold)\n",
    "        aggregated_predictions.extend(preds_fold)\n",
    "\n",
    "    print(\"\\nNested CV Complete. Running Bootstrapping\")\n",
    "\n",
    "    final_metrics = bootstrap_metrics(\n",
    "        np.array(aggregated_actuals),\n",
    "        np.array(aggregated_predictions),\n",
    "        n_bootstraps=1000\n",
    "    )\n",
    "\n",
    "    return final_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'valid_series_dict_full' not in locals() and 'valid_series_dict_full' not in globals():\n",
    "        print(\"Using dummy data for demonstration\")\n",
    "        valid_series_dict_full = {\n",
    "            f\"Encounter/{i}\": TimeSeries.from_values(np.random.rand(15, 1))\n",
    "            for i in range(100)\n",
    "        }\n",
    "\n",
    "    results = nested_cross_validation(\n",
    "        full_series_dict=valid_series_dict_full,\n",
    "        n_outer_folds=10,\n",
    "        n_trials=50, \n",
    "        max_epochs=100\n",
    "    )\n",
    "\n",
    "    print(f\"Aggregated Test-Fold Results\")\n",
    "    print(f\"MAE:   {results['mae']:.3f}  (95% CI: [{results['mae_ci_lower']:.3f}, {results['mae_ci_upper']:.3f}])\")\n",
    "    print(f\"RMSE:  {results['rmse']:.3f} (95% CI: [{results['rmse_ci_lower']:.3f}, {results['rmse_ci_upper']:.3f}])\")\n",
    "    print(f\"MSE:   {results['mse']:.3f} (95% CI: [{results['mse_ci_lower']:.3f}, {results['mse_ci_upper']:.3f}])\")\n",
    "    print(f\"MAPE:  {results['mape']:.2f}% (95% CI: [{results['mape_ci_lower']:.2f}%, {results['mape_ci_upper']:.2f}%])\")\n",
    "    print(f\"sMAPE: {results['smape']:.2f}% (95% CI: [{results['smape_ci_lower']:.2f}%, {results['smape_ci_upper']:.2f}%])\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "deecd4eed52e347fdb227bd6db7ebbadf0a7fc99b7440320ff3532cd8683bd78"
  },
  "kernelspec": {
   "display_name": "Python 3.10.18 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
