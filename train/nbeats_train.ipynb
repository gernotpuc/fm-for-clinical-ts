{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel\n",
    "from darts.metrics import mae, rmse, mape, smape\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# deterministic CPU execution if GPU is not available\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "prediction_length = prediction_length\n",
    "\n",
    "\n",
    "def get_train_test_split(series_list, prediction_length):\n",
    "    train_series = [s[:-prediction_length] for s in series_list]\n",
    "    test_series = [s[-prediction_length:] for s in series_list]\n",
    "    return train_series, test_series\n",
    "\n",
    "\n",
    "def bootstrap_metrics(actual, predicted, n_bootstraps=1000, alpha=0.05):\n",
    "\n",
    "    actual = np.asarray(actual)\n",
    "    predicted = np.asarray(predicted)\n",
    "    n = len(actual)\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    def compute_metrics(a, p):\n",
    "        return {\n",
    "            \"mae\": mae(TimeSeries.from_values(a), TimeSeries.from_values(p)),\n",
    "            \"rmse\": rmse(TimeSeries.from_values(a), TimeSeries.from_values(p)),\n",
    "            \"mse\": np.mean((a - p) ** 2),\n",
    "            \"mape\": mape(TimeSeries.from_values(a), TimeSeries.from_values(p)),\n",
    "            \"smape\": smape(TimeSeries.from_values(a), TimeSeries.from_values(p)),\n",
    "        }\n",
    "\n",
    "    base = compute_metrics(actual, predicted)\n",
    "    boot = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        a_s, p_s = actual[idx], predicted[idx]\n",
    "        boot.append(list(compute_metrics(a_s, p_s).values()))\n",
    "    boot = np.array(boot)\n",
    "    lower = np.percentile(boot, 100 * (alpha / 2), axis=0)\n",
    "    upper = np.percentile(boot, 100 * (1 - alpha / 2), axis=0)\n",
    "    keys = list(base.keys())\n",
    "    return {**{k: base[k] for k in keys},\n",
    "            **{f\"{k}_ci_lower\": l for k, l in zip(keys, lower)},\n",
    "            **{f\"{k}_ci_upper\": u for k, u in zip(keys, upper)}}\n",
    "\n",
    "\n",
    "def inner_objective(trial, train_series_list):\n",
    "    # search space\n",
    "    input_chunk_length = trial.suggest_int(\"input_chunk_length\", 6, 9)\n",
    "    layer_widths_value = trial.suggest_int(\"layer_widths_value\", 128, 512)\n",
    "    num_blocks = trial.suggest_int(\"num_blocks\", 1, 6)\n",
    "    num_stacks = trial.suggest_int(\"num_stacks\", 1, 6)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    kf_inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    inner_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf_inner.split(train_series_list):\n",
    "        fold_train = [train_series_list[i] for i in train_idx]\n",
    "        fold_val = [train_series_list[i] for i in val_idx]\n",
    "        min_req = input_chunk_length + prediction_length\n",
    "        if any(len(s) <= min_req for s in fold_train + fold_val):\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        model = NBEATSModel(\n",
    "            input_chunk_length=input_chunk_length,\n",
    "            output_chunk_length=prediction_length,\n",
    "            \n",
    "            layer_widths=[layer_widths_value] * num_stacks,\n",
    "            num_blocks=num_blocks,\n",
    "            num_stacks=num_stacks,\n",
    "            dropout=dropout,\n",
    "            batch_size=batch_size,\n",
    "            optimizer_kwargs={\"lr\": learning_rate},\n",
    "            likelihood=None,\n",
    "            random_state=42,\n",
    "            pl_trainer_kwargs={\n",
    "                \"accelerator\": \"cpu\",\n",
    "                \"devices\": 1,\n",
    "                \"callbacks\": [EarlyStopping(monitor=\"train_loss\", patience=10, mode=\"min\")]\n",
    "            },\n",
    "        )\n",
    "\n",
    "        model.fit(fold_train, epochs=50, verbose=False)\n",
    "        preds = model.predict(n=prediction_length, series=fold_val)\n",
    "        actual = np.concatenate([s.values().flatten() for s in fold_val])\n",
    "        pred = np.concatenate([p.values().flatten() for p in preds])\n",
    "        score = mae(TimeSeries.from_values(actual), TimeSeries.from_values(pred))\n",
    "        inner_scores.append(score)\n",
    "\n",
    "    return np.mean(inner_scores)\n",
    "\n",
    "def nested_cross_validation(full_series_dict, n_outer_folds=10, n_trials=50, max_epochs=500):\n",
    "    all_series_list = list(full_series_dict.values())\n",
    "    train_input_series, test_target_series = get_train_test_split(all_series_list, prediction_length)\n",
    "    kf_outer = KFold(n_splits=n_outer_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    aggregated_actuals, aggregated_predictions = [], []\n",
    "    outer_results = []\n",
    "\n",
    "    print(f\"Starting Nested {n_outer_folds}-Fold Cross-Validation...\")\n",
    "\n",
    "    for fold_num, (train_idx, test_idx) in enumerate(kf_outer.split(train_input_series)):\n",
    "        print(f\"\\n--- Outer Fold {fold_num+1}/{n_outer_folds} ---\")\n",
    "\n",
    "        outer_train = [train_input_series[i] for i in train_idx]\n",
    "        outer_test = [train_input_series[i] for i in test_idx]\n",
    "        outer_targets = [test_target_series[i] for i in test_idx]\n",
    "\n",
    "        # global scaler on outer-train\n",
    "        all_train_vals = np.concatenate([s.values().flatten() for s in outer_train])\n",
    "        scaler = MinMaxScaler((0, 1))\n",
    "        scaler.fit(all_train_vals.reshape(-1, 1))\n",
    "        outer_train_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in outer_train]\n",
    "        outer_test_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in outer_test]\n",
    "        outer_targets_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in outer_targets]\n",
    "\n",
    "        # inner optimization\n",
    "        study = optuna.create_study(direction=\"minimize\",\n",
    "                                    sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        wrapped_objective = lambda trial: inner_objective(trial, outer_train_scaled)\n",
    "        study.optimize(wrapped_objective, n_trials=n_trials, show_progress_bar=False)\n",
    "        best_params = study.best_trial.params\n",
    "        print(f\"Best hyperparameters (Fold {fold_num+1}): {best_params}\")\n",
    "\n",
    "        # train best model\n",
    "        model = NBEATSModel(\n",
    "            input_chunk_length=best_params[\"input_chunk_length\"],\n",
    "            output_chunk_length=prediction_length,\n",
    "            random_state=42,\n",
    "            dropout=best_params[\"dropout\"],\n",
    "            num_blocks=best_params[\"num_blocks\"],\n",
    "            num_stacks=best_params[\"num_stacks\"],\n",
    "            layer_widths=[best_params[\"layer_widths_value\"]] * best_params[\"num_stacks\"],\n",
    "            batch_size=best_params[\"batch_size\"],\n",
    "            optimizer_kwargs={\"lr\": best_params[\"learning_rate\"]},\n",
    "            likelihood=None,\n",
    "            pl_trainer_kwargs={\n",
    "                \"accelerator\": \"cpu\",\n",
    "                \"devices\": 1,\n",
    "                \"callbacks\": [EarlyStopping(monitor=\"train_loss\", patience=20, mode=\"min\")]\n",
    "            },\n",
    "        )\n",
    "\n",
    "        model.fit(series=outer_train_scaled, epochs=max_epochs, verbose=False)\n",
    "        preds_scaled = model.predict(n=prediction_length, series=outer_test_scaled)\n",
    "\n",
    "        # inverse-transform predictions & actuals\n",
    "        actuals_fold, preds_fold = [], []\n",
    "        for p, t in zip(preds_scaled, outer_targets_scaled):\n",
    "            y_pred = scaler.inverse_transform(p.values().reshape(-1, 1))[0, 0]\n",
    "            y_true = scaler.inverse_transform(t.values().reshape(-1, 1))[0, 0]\n",
    "            preds_fold.append(y_pred)\n",
    "            actuals_fold.append(y_true)\n",
    "\n",
    "        metrics = bootstrap_metrics(np.array(actuals_fold), np.array(preds_fold))\n",
    "        fold_mae = metrics[\"mae\"]\n",
    "        print(f\"Outer Fold {fold_num+1} MAE: {fold_mae:.4f}\")\n",
    "        outer_results.append({\"fold\": fold_num + 1, \"mae\": fold_mae, \"params\": best_params})\n",
    "\n",
    "        aggregated_actuals.extend(actuals_fold)\n",
    "        aggregated_predictions.extend(preds_fold)\n",
    "\n",
    "    final_metrics = bootstrap_metrics(np.array(aggregated_actuals), np.array(aggregated_predictions))\n",
    "    best_overall = min(outer_results, key=lambda x: x[\"mae\"])\n",
    "    best_params = best_overall[\"params\"]\n",
    "\n",
    "    print(\"Best Overall Parameters (Lowest Outer-Fold MAE):\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(f\"Best outer fold: {best_overall['fold']} (MAE={best_overall['mae']:.4f})\")\n",
    "\n",
    "    return final_metrics, best_params\n",
    "\n",
    "\n",
    "def train_final_and_evaluate_external(internal_series_dict, external_series_dict,\n",
    "                                      best_params, prediction_length=1, max_epochs=500):\n",
    "    print(\"Retraining final model using best overall parameters\")\n",
    "    internal_series = list(internal_series_dict.values())\n",
    "    external_series = list(external_series_dict.values())\n",
    "\n",
    "    # fit global scaler on internal data only\n",
    "    all_vals = np.concatenate([s.values().flatten() for s in internal_series])\n",
    "    scaler = MinMaxScaler((0, 1))\n",
    "    scaler.fit(all_vals.reshape(-1, 1))\n",
    "    internal_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in internal_series]\n",
    "    external_scaled = [TimeSeries.from_values(scaler.transform(s.values())) for s in external_series]\n",
    "\n",
    "    model = NBEATSModel(\n",
    "        input_chunk_length=best_params[\"input_chunk_length\"],\n",
    "        output_chunk_length=prediction_length,\n",
    "        random_state=42,\n",
    "        dropout=best_params[\"dropout\"],\n",
    "        num_blocks=best_params[\"num_blocks\"],\n",
    "        num_stacks=best_params[\"num_stacks\"],\n",
    "        layer_widths=[best_params[\"layer_widths_value\"]] * best_params[\"num_stacks\"],\n",
    "        batch_size=best_params[\"batch_size\"],\n",
    "        optimizer_kwargs={\"lr\": best_params[\"learning_rate\"]},\n",
    "        likelihood=None,\n",
    "        pl_trainer_kwargs={\n",
    "            \"accelerator\": \"cpu\",\n",
    "            \"devices\": 1,\n",
    "            \"callbacks\": [EarlyStopping(monitor=\"train_loss\", patience=20, mode=\"min\")]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    model.fit(series=internal_scaled, epochs=max_epochs, verbose=True)\n",
    "\n",
    "    ext_input, ext_target = get_train_test_split(external_scaled, prediction_length)\n",
    "    preds_scaled = model.predict(n=prediction_length, series=ext_input)\n",
    "\n",
    "    preds, actuals = [], []\n",
    "    for p, t in zip(preds_scaled, ext_target):\n",
    "        y_pred = scaler.inverse_transform(p.values().reshape(-1, 1))[0, 0]\n",
    "        y_true = scaler.inverse_transform(t.values().reshape(-1, 1))[0, 0]\n",
    "        preds.append(y_pred)\n",
    "        actuals.append(y_true)\n",
    "\n",
    "    metrics = bootstrap_metrics(np.array(actuals), np.array(preds))\n",
    "    print(\"Final External Evaluation:\")\n",
    "    print(f\"MAE:   {metrics['mae']:.4f} (95% CI: [{metrics['mae_ci_lower']:.4f}, {metrics['mae_ci_upper']:.4f}])\")\n",
    "    print(f\"RMSE:  {metrics['rmse']:.4f} (95% CI: [{metrics['rmse_ci_lower']:.4f}, {metrics['rmse_ci_upper']:.4f}])\")\n",
    "    print(f\"MSE:   {metrics['mse']:.4f} (95% CI: [{metrics['mse_ci_lower']:.4f}, {metrics['mse_ci_upper']:.4f}])\")\n",
    "    print(f\"MAPE:  {metrics['mape']:.2f}% (95% CI: [{metrics['mape_ci_lower']:.2f}%, {metrics['mape_ci_upper']:.2f}%])\")\n",
    "    print(f\"sMAPE: {metrics['smape']:.2f}% (95% CI: [{metrics['smape_ci_lower']:.2f}%, {metrics['smape_ci_upper']:.2f}%])\")\n",
    "    return model, metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Generating dummy internal and external datasets for testing. Please replace with actual data\")\n",
    "    valid_series_dict_full = {f\"Encounter/{i}\": TimeSeries.from_values(np.random.rand(15, 1) * 50) for i in range(30)}\n",
    "    external_series_dict = {f\"Hospid/{i}\": TimeSeries.from_values(np.random.rand(15, 1) * 50) for i in range(10)}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 1: Nested CV to find best parameters\n",
    "    nested_metrics, best_params = nested_cross_validation(\n",
    "        full_series_dict=valid_series_dict_full,\n",
    "        n_outer_folds=10,\n",
    "        n_trials=50,\n",
    "        max_epochs=100\n",
    "    )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Step 2: Retrain final model using best params and evaluate on external data\n",
    "    final_model, external_metrics = train_final_and_evaluate_external(\n",
    "        internal_series_dict=valid_series_dict_full,\n",
    "        external_series_dict=external_series_dict,\n",
    "        best_params=best_params,\n",
    "        prediction_length=prediction_length,\n",
    "        max_epochs=100\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "deecd4eed52e347fdb227bd6db7ebbadf0a7fc99b7440320ff3532cd8683bd78"
  },
  "kernelspec": {
   "display_name": "Python 3.10.18 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
