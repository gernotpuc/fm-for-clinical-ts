{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108f2b2-a129-4e4a-afcd-07c3b867374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from functions_uc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb941122-bc6b-45bf-8665-6bf332ec3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesfm_backend = \"gpu\"  # @param\n",
    "\n",
    "tfm = timesfm.TimesFm(\n",
    "      hparams=timesfm.TimesFmHparams(\n",
    "          backend=timesfm_backend,\n",
    "          horizon_len=prediction_length,\n",
    "          num_layers=50,\n",
    "          use_positional_embedding=False,\n",
    "          context_len=32,\n",
    "      ),\n",
    "      checkpoint=timesfm.TimesFmCheckpoint(\n",
    "          huggingface_repo_id=\"google/timesfm-2.0-500m-jax\"),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6110e6-135a-4a6e-8f25-0789cdb4720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DICT = {\n",
    "    \"data_ume\": {\n",
    "        \"boundaries\": [8164,10398,13654],\n",
    "        \"data_path\": \"./data/retrain_timesffm.csv\",\n",
    "        \"freq\": frequency,\n",
    "    },\n",
    "\n",
    "}\n",
    "dataset = \"data_ume\"\n",
    "data_path = DATA_DICT[dataset][\"data_path\"]\n",
    "freq = DATA_DICT[dataset][\"freq\"]\n",
    "int_freq = timesfm.freq_map(freq)\n",
    "boundaries = DATA_DICT[dataset][\"boundaries\"]\n",
    "\n",
    "data_df = pd.read_csv(open(data_path, \"r\"))\n",
    "\n",
    "ts_cols = [col for col in data_df.columns if col not in [\"ds\", \"unique_id\"]]\n",
    "\n",
    "num_cov_cols = None\n",
    "cat_cov_cols = None\n",
    "\n",
    "context_len = context_len\n",
    "pred_len = prediction_length\n",
    "\n",
    "num_ts = len(ts_cols)\n",
    "batch_size = 6\n",
    "\n",
    "dtl = data_loader.TimeSeriesdata(\n",
    "      data_path=data_path,\n",
    "      datetime_col=\"ds\",\n",
    "      num_cov_cols=num_cov_cols,\n",
    "      cat_cov_cols=cat_cov_cols,\n",
    "      ts_cols=np.array(ts_cols),\n",
    "      train_range=[0, boundaries[0]],\n",
    "      val_range=[boundaries[0], boundaries[1]],\n",
    "      test_range=[boundaries[1], boundaries[2]],\n",
    "      hist_len=context_len,\n",
    "      pred_len=pred_len,\n",
    "      batch_size=num_ts,\n",
    "      freq=freq,\n",
    "      normalize=True,\n",
    "      epoch_len=None,\n",
    "      holiday=False,\n",
    "      permute=True,\n",
    "  )\n",
    "train_batches = dtl.tf_dataset(mode=\"train\", shift=1).batch(batch_size)\n",
    "val_batches = dtl.tf_dataset(mode=\"val\", shift=pred_len)\n",
    "test_batches = dtl.tf_dataset(mode=\"test\", shift=pred_len)\n",
    "\n",
    "# PAX shortcuts\n",
    "NestedMap = py_utils.NestedMap\n",
    "WeightInit = base_layer.WeightInit\n",
    "WeightHParams = base_layer.WeightHParams\n",
    "InstantiableParams = py_utils.InstantiableParams\n",
    "JTensor = pytypes.JTensor\n",
    "NpTensor = pytypes.NpTensor\n",
    "WeightedScalars = pytypes.WeightedScalars\n",
    "instantiate = base_hyperparams.instantiate\n",
    "LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]\n",
    "AuxLossStruct = base_layer.AuxLossStruct\n",
    "\n",
    "AUX_LOSS = base_layer.AUX_LOSS\n",
    "template_field = base_layer.template_field\n",
    "\n",
    "# Standard prng key names\n",
    "PARAMS = base_layer.PARAMS\n",
    "RANDOM = base_layer.RANDOM\n",
    "\n",
    "key = jax.random.PRNGKey(seed=1234)\n",
    "\n",
    "model = pax_fiddle.Config(\n",
    "    patched_decoder.PatchedDecoderFinetuneModel,\n",
    "    name='patched_decoder_finetune',\n",
    "    core_layer_tpl=tfm.model_p,\n",
    ")\n",
    "\n",
    "@pax_fiddle.auto_config\n",
    "def build_learner_finetune(learning_rate=1e-3, clip_threshold=50) -> learners.Learner:\n",
    "    return pax_fiddle.Config(\n",
    "        learners.Learner,\n",
    "        name='learner',\n",
    "        loss_name='avg_qloss',\n",
    "        optimizer=optimizers.Adam(\n",
    "            epsilon=1e-7,\n",
    "            clip_threshold=clip_threshold, \n",
    "            learning_rate=learning_rate,   \n",
    "            lr_schedule=pax_fiddle.Config(\n",
    "                schedules.Cosine,\n",
    "                initial_value=learning_rate,\n",
    "                final_value=learning_rate * 0.1,\n",
    "                total_steps=40000,\n",
    "            ),\n",
    "            ema_decay=0.9999,\n",
    "        ),\n",
    "        bprop_variable_exclusion=['.*/stacked_transformer_layer/.*'],  # linear probing\n",
    "    )\n",
    "\n",
    "task_p = tasks_lib.SingleTask(\n",
    "    name='ts-learn',\n",
    "    model=model,\n",
    "    train=tasks_lib.SingleTask.Train(\n",
    "        learner=build_learner_finetune(\n",
    "            learning_rate=1e-3,\n",
    "            clip_threshold=50.0,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "task_p.model.ici_mesh_shape = [1, 1, 1]\n",
    "task_p.model.mesh_axis_names = ['replica', 'data', 'mdl']\n",
    "\n",
    "DEVICES = np.array(jax.devices()).reshape([1, 1, 1])\n",
    "MESH = jax.sharding.Mesh(DEVICES, ['replica', 'data', 'mdl'])\n",
    "\n",
    "num_devices = jax.local_device_count()\n",
    "print(f'num_devices: {num_devices}')\n",
    "print(f'device kind: {jax.local_devices()[0].device_kind}')\n",
    "\n",
    "jax_task = task_p\n",
    "key, init_key = jax.random.split(key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_train_batch(batch):\n",
    "    past_ts = batch[0].reshape(batch_size * num_ts, -1)\n",
    "    actual_ts = batch[3].reshape(batch_size * num_ts, -1)\n",
    "    return NestedMap(input_ts=past_ts, actual_ts=actual_ts)\n",
    "\n",
    "\n",
    "def process_eval_batch(batch):\n",
    "    past_ts = batch[0]\n",
    "    actual_ts = batch[3]\n",
    "    return NestedMap(input_ts=past_ts, actual_ts=actual_ts)\n",
    "\n",
    "\n",
    "first_train_batch = next(train_batches.as_numpy_iterator())\n",
    "tbatch = process_train_batch(first_train_batch)\n",
    "\n",
    "jax_model_states, _ = trainer_lib.initialize_model_state(\n",
    "    jax_task,\n",
    "    init_key,\n",
    "    tbatch,\n",
    "    checkpoint_type=checkpoint_types.CheckpointType.GDA,\n",
    ")\n",
    "jax_model_states.mdl_vars['params']['core_layer'] = tfm._train_state.mdl_vars['params']\n",
    "jax_vars = jax_model_states.mdl_vars\n",
    "gc.collect()\n",
    "\n",
    "jax_task = task_p\n",
    "\n",
    "\n",
    "def train_step(states, prng_key, inputs):\n",
    "  return trainer_lib.train_step_single_learner(\n",
    "      jax_task, states, prng_key, inputs\n",
    "  )\n",
    "\n",
    "\n",
    "def eval_step(states, prng_key, inputs):\n",
    "  states = states.to_eval_state()\n",
    "  return trainer_lib.eval_step_single_learner(\n",
    "      jax_task, states, prng_key, inputs\n",
    "  )\n",
    "\n",
    "key, train_key, eval_key = jax.random.split(key, 3)\n",
    "train_prng_seed = jax.random.split(train_key, num=jax.local_device_count())\n",
    "eval_prng_seed = jax.random.split(eval_key, num=jax.local_device_count())\n",
    "\n",
    "p_train_step = jax.pmap(train_step, axis_name='batch')\n",
    "p_eval_step = jax.pmap(eval_step, axis_name='batch')\n",
    "\n",
    "replicated_jax_states = trainer_lib.replicate_model_state(jax_model_states)\n",
    "replicated_jax_vars = replicated_jax_states.mdl_vars\n",
    "\n",
    "best_eval_loss = 1e7\n",
    "step_count = 0\n",
    "patience = 0\n",
    "NUM_EPOCHS = 10\n",
    "PATIENCE = 5\n",
    "TRAIN_STEPS_PER_EVAL = 1000\n",
    "CHECKPOINT_DIR='./timesfm_finetune'\n",
    "\n",
    "def reshape_batch_for_pmap(batch, num_devices):\n",
    "  def _reshape(input_tensor):\n",
    "    bsize = input_tensor.shape[0]\n",
    "    residual_shape = list(input_tensor.shape[1:])\n",
    "    nbsize = bsize // num_devices\n",
    "    return jnp.reshape(input_tensor, [num_devices, nbsize] + residual_shape)\n",
    "\n",
    "  return jax.tree.map(_reshape, batch)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"__________________Epoch: {epoch}__________________\", flush=True)\n",
    "    train_its = train_batches.as_numpy_iterator()\n",
    "    if patience >= PATIENCE:\n",
    "        print(\"Early stopping.\", flush=True)\n",
    "        break\n",
    "    for batch in tqdm(train_its):\n",
    "        train_losses = []\n",
    "        if patience >= PATIENCE:\n",
    "            print(\"Early stopping.\", flush=True)\n",
    "            break\n",
    "        tbatch = process_train_batch(batch)\n",
    "        tbatch = reshape_batch_for_pmap(tbatch, num_devices)\n",
    "        replicated_jax_states, step_fun_out = p_train_step(\n",
    "            replicated_jax_states, train_prng_seed, tbatch\n",
    "        )\n",
    "        train_losses.append(step_fun_out.loss[0])\n",
    "        if step_count % TRAIN_STEPS_PER_EVAL == 0:\n",
    "            print(\n",
    "                f\"Train loss at step {step_count}: {np.mean(train_losses)}\",\n",
    "                flush=True,\n",
    "            )\n",
    "            train_losses = []\n",
    "            print(\"Starting eval.\", flush=True)\n",
    "            val_its = val_batches.as_numpy_iterator()\n",
    "            eval_losses = []\n",
    "            for ev_batch in tqdm(val_its):\n",
    "                ebatch = process_eval_batch(ev_batch)\n",
    "                ebatch = reshape_batch_for_pmap(ebatch, num_devices)\n",
    "                _, step_fun_out = p_eval_step(\n",
    "                    replicated_jax_states, eval_prng_seed, ebatch\n",
    "                )\n",
    "                eval_losses.append(step_fun_out.loss[0])\n",
    "            mean_loss = np.mean(eval_losses)\n",
    "            print(f\"Eval loss at step {step_count}: {mean_loss}\", flush=True)\n",
    "            if mean_loss < best_eval_loss or np.isnan(mean_loss):\n",
    "                best_eval_loss = mean_loss\n",
    "                print(\"Saving checkpoint.\")\n",
    "                jax_state_for_saving = py_utils.maybe_unreplicate_for_fully_replicated(\n",
    "                    replicated_jax_states\n",
    "                )\n",
    "                checkpoints.save_checkpoint(\n",
    "                    jax_state_for_saving, CHECKPOINT_DIR, overwrite=True\n",
    "                )\n",
    "                patience = 0\n",
    "                del jax_state_for_saving\n",
    "                gc.collect()\n",
    "            else:\n",
    "                patience += 1\n",
    "                print(f\"patience: {patience}\")\n",
    "        step_count += 1\n",
    "\n",
    "train_state = checkpoints.restore_checkpoint(jax_model_states, CHECKPOINT_DIR)\n",
    "print(train_state.step)\n",
    "tfm._train_state.mdl_vars['params'] = train_state.mdl_vars['params']['core_layer']\n",
    "tfm.jit_decode()\n",
    "\n",
    "mae_losses = []\n",
    "for batch in tqdm(test_batches.as_numpy_iterator()):\n",
    "    past = batch[0]\n",
    "    actuals = batch[3]\n",
    "    _, forecasts = tfm.forecast(list(past), [0] * past.shape[0])\n",
    "    forecasts = forecasts[:, 0 : actuals.shape[1], 5]\n",
    "    mae_losses.append(np.abs(forecasts - actuals).mean())\n",
    "\n",
    "print(f\"MAE: {np.mean(mae_losses)}\")\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters to tune\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    clip_threshold = trial.suggest_uniform(\"clip_threshold\", 1.0, 100.0)\n",
    "\n",
    "    print(f\"\\n🔍 Trial {trial.number}: learning_rate={learning_rate:.5e}, clip_threshold={clip_threshold:.2f}\")\n",
    "\n",
    "    # Swap in the new learner with trial parameters\n",
    "    task_p.train.learner = build_learner_finetune(\n",
    "        learning_rate=learning_rate,\n",
    "        clip_threshold=clip_threshold,\n",
    "    )\n",
    "\n",
    "    # Re-initialize model state for this trial\n",
    "    key = jax.random.PRNGKey(seed=trial.number)\n",
    "    key, init_key = jax.random.split(key)\n",
    "    tbatch = process_train_batch(next(train_batches.as_numpy_iterator()))\n",
    "\n",
    "    jax_model_states, _ = trainer_lib.initialize_model_state(\n",
    "        task_p,\n",
    "        init_key,\n",
    "        tbatch,\n",
    "        checkpoint_type=checkpoint_types.CheckpointType.GDA,\n",
    "    )\n",
    "    jax_model_states.mdl_vars['params']['core_layer'] = tfm._train_state.mdl_vars['params']\n",
    "    replicated_jax_states = trainer_lib.replicate_model_state(jax_model_states)\n",
    "\n",
    "    # Train for a few steps\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    step_count = 0\n",
    "    NUM_STEPS = 1000\n",
    "    train_prng_seed = jax.random.split(init_key, num=jax.local_device_count())\n",
    "    eval_prng_seed = jax.random.split(init_key, num=jax.local_device_count())\n",
    "\n",
    "    train_iterator = train_batches.as_numpy_iterator()\n",
    "    for batch in train_iterator:\n",
    "        if step_count >= NUM_STEPS:\n",
    "            break\n",
    "        tbatch = process_train_batch(batch)\n",
    "        tbatch = reshape_batch_for_pmap(tbatch, num_devices)\n",
    "        replicated_jax_states, step_fun_out = p_train_step(\n",
    "            replicated_jax_states, train_prng_seed, tbatch\n",
    "        )\n",
    "        train_losses.append(step_fun_out.loss[0])\n",
    "        step_count += 1\n",
    "\n",
    "    # Evaluate once\n",
    "    val_iterator = val_batches.as_numpy_iterator()\n",
    "    for val_batch in val_iterator:\n",
    "        ebatch = process_eval_batch(val_batch)\n",
    "        ebatch = reshape_batch_for_pmap(ebatch, num_devices)\n",
    "        _, step_fun_out = p_eval_step(replicated_jax_states, eval_prng_seed, ebatch)\n",
    "        eval_losses.append(step_fun_out.loss[0])\n",
    "\n",
    "    # Return mean eval loss as trial objective\n",
    "    mean_eval_loss = float(np.mean(eval_losses))\n",
    "    print(f\"Trial {trial.number} Eval Loss (MAE): {mean_eval_loss:.6f}\")\n",
    "\n",
    "    gc.collect()  # Optional: free memory\n",
    "    return mean_eval_loss\n",
    "\n",
    "\n",
    "def run_optuna_timesfm(n_trials=10):\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Trial Summary:\")\n",
    "    print(study.best_trial)\n",
    "\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for k, v in study.best_trial.params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    return study\n",
    "\n",
    "train_timesffm = run_optuna_timesfm(n_trials=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
